---
title: "ë² ì´ì¦ˆ ì •ë¦¬"
weight: 1
math: true
---

# ë² ì´ì¦ˆ ì •ë¦¬ (Bayes' Theorem)

## ê°œìš”

ì¡°ê±´ë¶€ í™•ë¥ ì„ ì—­ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê³µì‹ìœ¼ë¡œ, ê´€ì¸¡ ë°ì´í„°ë¡œë¶€í„° ì›ì¸ì˜ í™•ë¥ ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

## ì •ì˜

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

- **P(A|B)**: ì‚¬í›„ í™•ë¥  (Posterior) - Bê°€ ì£¼ì–´ì¡Œì„ ë•Œ Aì˜ í™•ë¥ 
- **P(B|A)**: ê°€ëŠ¥ë„ (Likelihood) - Aê°€ ì°¸ì¼ ë•Œ Bê°€ ê´€ì¸¡ë  í™•ë¥ 
- **P(A)**: ì‚¬ì „ í™•ë¥  (Prior) - Aì— ëŒ€í•œ ì‚¬ì „ ë¯¿ìŒ
- **P(B)**: ì¦ê±° (Evidence) - Bê°€ ê´€ì¸¡ë  ì „ì²´ í™•ë¥ 

### ì‹œê°ì  ì´í•´

![ë² ì´ì¦ˆ ì •ë¦¬](/images/probability/ko/bayes-theorem.svg)

## ì§ê´€ì  ì´í•´

### ğŸ¥ ì˜ë£Œ ì§„ë‹¨ ì˜ˆì‹œ

> ì§ˆë³‘ ê²€ì‚¬ê°€ ì–‘ì„±ìœ¼ë¡œ ë‚˜ì™”ë‹¤. ì •ë§ ì§ˆë³‘ì´ ìˆì„ í™•ë¥ ì€?

**ì£¼ì–´ì§„ ì •ë³´**:
- ìœ ë³‘ë¥ : 1% (P(ì§ˆë³‘) = 0.01)
- ë¯¼ê°ë„: 95% (P(ì–‘ì„±|ì§ˆë³‘) = 0.95)
- íŠ¹ì´ë„: 90% (P(ìŒì„±|ì •ìƒ) = 0.90)

**ê³„ì‚°**:
$$
P(\text{ì§ˆë³‘}|\text{ì–‘ì„±}) = \frac{0.95 \times 0.01}{0.95 \times 0.01 + 0.10 \times 0.99} \approx 0.088
$$

> ğŸ˜® ì–‘ì„±ì´ì–´ë„ ì‹¤ì œ ì§ˆë³‘ì¼ í™•ë¥ ì€ ì•½ 8.8%! (ì§ˆë³‘ì´ í¬ê·€í•´ì„œ)

## ë”¥ëŸ¬ë‹ì—ì„œì˜ í™œìš©

### 1. ë¶„ë¥˜ ë¬¸ì œì˜ í•´ì„

ì‹ ê²½ë§ì˜ Softmax ì¶œë ¥ì€ ì‚¬í›„ í™•ë¥ ë¡œ í•´ì„:

$$
P(y=k|x) = \frac{e^{z_k}}{\sum_j e^{z_j}}
$$

### 2. Bayesian Neural Networks

ê°€ì¤‘ì¹˜ë¥¼ í™•ë¥ ë¶„í¬ë¡œ ëª¨ë¸ë§:

$$
P(W|D) = \frac{P(D|W) \cdot P(W)}{P(D)}
$$

```python
import torch
import torch.nn as nn

class BayesianLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        # ê°€ì¤‘ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ í•™ìŠµ
        self.weight_mu = nn.Parameter(torch.randn(out_features, in_features))
        self.weight_logvar = nn.Parameter(torch.zeros(out_features, in_features))

    def forward(self, x):
        # ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§ (Reparameterization trick)
        std = torch.exp(0.5 * self.weight_logvar)
        eps = torch.randn_like(std)
        weight = self.weight_mu + std * eps
        return x @ weight.T
```

### 3. ë¶ˆí™•ì‹¤ì„± ì¶”ì •

ì—¬ëŸ¬ ë²ˆ ì˜ˆì¸¡í•˜ì—¬ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •:

```python
model.train()  # Dropout í™œì„±í™”
predictions = []
for _ in range(100):
    pred = model(x)
    predictions.append(pred)

mean = torch.stack(predictions).mean(0)  # ì˜ˆì¸¡ê°’
std = torch.stack(predictions).std(0)    # ë¶ˆí™•ì‹¤ì„±
```

## Maximum A Posteriori (MAP)

ì‚¬í›„ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ì°¾ê¸°:

$$
\theta_{MAP} = \arg\max_\theta P(\theta|D) = \arg\max_\theta P(D|\theta) P(\theta)
$$

ë¡œê·¸ë¥¼ ì·¨í•˜ë©´:
$$
\theta_{MAP} = \arg\max_\theta [\log P(D|\theta) + \log P(\theta)]
$$

- ì²« ë²ˆì§¸ í•­: ë°ì´í„° ê°€ëŠ¥ë„ â†’ Loss
- ë‘ ë²ˆì§¸ í•­: ì‚¬ì „ ë¶„í¬ â†’ ì •ê·œí™” (Weight Decay)

## ê´€ë ¨ ì½˜í…ì¸ 

- [í™•ë¥ ë¶„í¬](/ko/docs/math/probability/distribution)
- [Cross-Entropy](/ko/docs/math/training/loss/cross-entropy) - ê°€ëŠ¥ë„ ê¸°ë°˜ ì†ì‹¤
- [Weight Decay](/ko/docs/math/training/regularization/weight-decay) - ì‚¬ì „ ë¶„í¬ì˜ íš¨ê³¼
