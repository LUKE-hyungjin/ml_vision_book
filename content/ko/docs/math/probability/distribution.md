---
title: "í™•ë¥ ë¶„í¬"
weight: 2
math: true
---

# í™•ë¥ ë¶„í¬ (Probability Distributions)

## ê°œìš”

> ğŸ’¡ í™•ë¥ ë¶„í¬ëŠ” **"ê° ê°’ì´ ë‚˜ì˜¬ í™•ë¥ "**ì„ í•¨ìˆ˜ë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤.

### í•œëˆˆì— ë³´ëŠ” ì£¼ìš” ë¶„í¬

![í™•ë¥ ë¶„í¬](/images/probability/distributions.svg)

---

## ì´ì‚° ë¶„í¬

### ë² ë¥´ëˆ„ì´ ë¶„í¬ (Bernoulli)

ë™ì „ ë˜ì§€ê¸°ì²˜ëŸ¼ ì„±ê³µ/ì‹¤íŒ¨ ë‘ ê°€ì§€ ê²°ê³¼:

$$
P(X=1) = p, \quad P(X=0) = 1-p
$$

**ë”¥ëŸ¬ë‹ ì ìš©**: Dropout

```python
import torch

p = 0.5  # ë“œë¡­ í™•ë¥ 
mask = torch.bernoulli(torch.full((10,), 1-p))  # 0 ë˜ëŠ” 1
x = x * mask / (1 - p)  # Inverted dropout
```

### ì¹´í…Œê³ ë¦¬ ë¶„í¬ (Categorical)

Kê°œì˜ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ ì„ íƒ:

$$
P(X=k) = p_k, \quad \sum_{k=1}^K p_k = 1
$$

**ë”¥ëŸ¬ë‹ ì ìš©**: Softmax ì¶œë ¥

```python
logits = model(x)  # (batch, num_classes)
probs = torch.softmax(logits, dim=-1)  # ì¹´í…Œê³ ë¦¬ ë¶„í¬
```

## ì—°ì† ë¶„í¬

### ê°€ìš°ì‹œì•ˆ ë¶„í¬ (Gaussian / Normal)

$$
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

**ë”¥ëŸ¬ë‹ ì ìš©**: VAEì˜ ì ì¬ ê³µê°„, ë…¸ì´ì¦ˆ ëª¨ë¸ë§

```python
mu = encoder_mu(x)
log_var = encoder_logvar(x)

# Reparameterization trick
std = torch.exp(0.5 * log_var)
eps = torch.randn_like(std)  # N(0, 1)ì—ì„œ ìƒ˜í”Œë§
z = mu + std * eps  # N(mu, sigma^2)ì—ì„œ ìƒ˜í”Œë§
```

### ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ

$$
p(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
$$

## Softmaxì™€ í™•ë¥ 

SoftmaxëŠ” ì„ì˜ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¥¼ í™•ë¥ ë¶„í¬ë¡œ ë³€í™˜:

$$
\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

ì†ì„±:
- ëª¨ë“  ì¶œë ¥ > 0
- í•© = 1
- ê°€ì¥ í° ê°’ì´ ê°€ì¥ ë†’ì€ í™•ë¥ 

```python
def softmax(z):
    exp_z = torch.exp(z - z.max(dim=-1, keepdim=True).values)  # ìˆ˜ì¹˜ ì•ˆì •ì„±
    return exp_z / exp_z.sum(dim=-1, keepdim=True)

z = torch.tensor([2.0, 1.0, 0.1])
print(softmax(z))  # [0.659, 0.242, 0.099]
```

## KL Divergence

ë‘ í™•ë¥ ë¶„í¬ ê°„ì˜ ê±°ë¦¬:

$$
D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

ì†ì„±:
- D_KL â‰¥ 0
- D_KL = 0 âŸº P = Q
- ë¹„ëŒ€ì¹­: D_KL(P||Q) â‰  D_KL(Q||P)

**ë”¥ëŸ¬ë‹ ì ìš©**: VAE Loss

```python
def kl_divergence(mu, log_var):
    # N(mu, sigma^2)ì™€ N(0, 1) ì‚¬ì´ì˜ KL
    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
```

## ê´€ë ¨ ì½˜í…ì¸ 

- [ë² ì´ì¦ˆ ì •ë¦¬](/ko/docs/math/probability/bayes)
- [ìƒ˜í”Œë§](/ko/docs/math/probability/sampling)
- [Cross-Entropy](/ko/docs/math/training/loss/cross-entropy)
- [VAE](/ko/docs/architecture/generative/vae)
