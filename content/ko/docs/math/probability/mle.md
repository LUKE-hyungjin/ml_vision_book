---
title: "ìµœëŒ€ ìš°ë„ ì¶”ì •"
weight: 12
math: true
---

# ìµœëŒ€ ìš°ë„ ì¶”ì • (Maximum Likelihood Estimation)

## ê°œìš”

> ğŸ’¡ **MLE**: "ë°ì´í„°ê°€ ê°€ì¥ ì˜ ë‚˜ì˜¬ ê²ƒ ê°™ì€" íŒŒë¼ë¯¸í„° ì°¾ê¸°

**ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ = MLE**ì…ë‹ˆë‹¤!

### ì‹œê°ì  ì´í•´

![ìµœëŒ€ ìš°ë„ ì¶”ì •](/images/probability/ko/mle.svg)

---

## ìš°ë„ í•¨ìˆ˜ (Likelihood Function)

### ì •ì˜

íŒŒë¼ë¯¸í„° $\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë°ì´í„° $D$ê°€ ê´€ì¸¡ë  í™•ë¥ :

$$
L(\theta) = P(D | \theta) = \prod_{i=1}^{n} P(x_i | \theta)
$$

(i.i.d ê°€ì • ì‹œ)

### í™•ë¥  vs ìš°ë„

| | í™•ë¥  | ìš°ë„ |
|---|---|---|
| ê³ ì • | íŒŒë¼ë¯¸í„° $\theta$ | ë°ì´í„° $D$ |
| ë³€ìˆ˜ | ë°ì´í„° $D$ | íŒŒë¼ë¯¸í„° $\theta$ |
| ì˜ë¯¸ | $\theta$ì¼ ë•Œ Dê°€ ë‚˜ì˜¬ í™•ë¥  | Dê°€ ê´€ì¸¡ëì„ ë•Œ $\theta$ì˜ ê·¸ëŸ´ë“¯í•¨ |

---

## ìµœëŒ€ ìš°ë„ ì¶”ì •

### ì •ì˜

$$
\hat{\theta}_{MLE} = \arg\max_\theta L(\theta) = \arg\max_\theta P(D | \theta)
$$

### Log-Likelihood

ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ ë¡œê·¸ ì·¨í•¨:

$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log P(x_i | \theta)
$$

ë¡œê·¸ëŠ” ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ ìµœëŒ€í™” ê²°ê³¼ ë™ì¼.

### ìŒì˜ ë¡œê·¸ ìš°ë„ (NLL: Negative Log-Likelihood)

ìµœëŒ€í™”ë¥¼ ìµœì†Œí™”ë¡œ:

$$
\hat{\theta}_{MLE} = \arg\min_\theta \left[ -\sum_{i=1}^{n} \log P(x_i | \theta) \right]
$$

---

## ì˜ˆì‹œ: ë² ë¥´ëˆ„ì´ ë¶„í¬

ë™ì „ì„ 10ë²ˆ ë˜ì ¸ì„œ ì•ë©´ì´ 7ë²ˆ ë‚˜ì˜´. $p$ì˜ MLEëŠ”?

### ìš°ë„ í•¨ìˆ˜

$$
L(p) = p^7 (1-p)^3
$$

### Log-Likelihood

$$
\ell(p) = 7 \log p + 3 \log(1-p)
$$

### ë¯¸ë¶„í•˜ì—¬ ìµœëŒ€í™”

$$
\frac{d\ell}{dp} = \frac{7}{p} - \frac{3}{1-p} = 0
$$

$$
\hat{p}_{MLE} = \frac{7}{10} = 0.7
$$

ì§ê´€ê³¼ ì¼ì¹˜: ê´€ì¸¡ëœ ë¹„ìœ¨ = MLE ì¶”ì •ì¹˜

---

## ì˜ˆì‹œ: ì •ê·œ ë¶„í¬

ë°ì´í„° $\{x_1, ..., x_n\}$ì´ $\mathcal{N}(\mu, \sigma^2)$ì—ì„œ ì™”ì„ ë•Œ:

### Log-Likelihood

$$
\ell(\mu, \sigma^2) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i - \mu)^2
$$

### MLE í•´

$$
\hat{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^{n} x_i = \bar{x}
$$

$$
\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2
$$

í‘œë³¸ í‰ê· ê³¼ (í¸í–¥ëœ) í‘œë³¸ ë¶„ì‚°ì´ MLE.

---

## ë”¥ëŸ¬ë‹ì—ì„œì˜ MLE

### ë¶„ë¥˜: Cross-Entropy = NLL

ëª¨ë¸ì´ $P(y|x; \theta)$ë¥¼ ì¶œë ¥í•  ë•Œ:

$$
\text{NLL} = -\sum_{i=1}^{n} \log P(y_i | x_i; \theta)
$$

one-hot ë ˆì´ë¸” $y$ì™€ softmax ì¶œë ¥ $\hat{y}$ì— ëŒ€í•´:

$$
\text{Cross-Entropy} = -\sum_c y_c \log \hat{y}_c = -\log \hat{y}_{true}
$$

**Cross-Entropy ìµœì†Œí™” = MLE!**

### íšŒê·€: MSEì™€ MLE

ì¶œë ¥ì´ $\mathcal{N}(f_\theta(x), \sigma^2)$ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ë©´:

$$
\text{NLL} = \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - f_\theta(x_i))^2 + \text{const}
$$

**MSE ìµœì†Œí™” = ê°€ìš°ì‹œì•ˆ ê°€ì • í•˜ì—ì„œì˜ MLE!**

### ìš”ì•½

| ì†ì‹¤ í•¨ìˆ˜ | MLE ê´€ì  |
|-----------|----------|
| Cross-Entropy | ì¹´í…Œê³ ë¦¬ì»¬ ë¶„í¬ì˜ NLL |
| MSE | ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ NLL |
| MAE | ë¼í”Œë¼ìŠ¤ ë¶„í¬ì˜ NLL |

---

## MLEì˜ ì„±ì§ˆ

### 1. ì¼ì¹˜ì„± (Consistency)

$$
\hat{\theta}_{MLE} \xrightarrow{p} \theta_{true} \quad \text{as } n \rightarrow \infty
$$

ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì§„ì§œ íŒŒë¼ë¯¸í„°ë¡œ ìˆ˜ë ´.

### 2. ì ê·¼ì  ì •ê·œì„± (Asymptotic Normality)

$$
\sqrt{n}(\hat{\theta}_{MLE} - \theta_{true}) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})
$$

$I(\theta)$: Fisher Information

### 3. ì ê·¼ì  íš¨ìœ¨ì„± (Asymptotic Efficiency)

ì ê·¼ì ìœ¼ë¡œ CramÃ©r-Rao í•˜í•œì— ë„ë‹¬ (ë¶„ì‚°ì´ ê°€ì¥ ì‘ìŒ).

---

## MLE vs MAP

### MAP (Maximum A Posteriori)

ì‚¬ì „ ë¶„í¬ë¥¼ í¬í•¨:

$$
\hat{\theta}_{MAP} = \arg\max_\theta P(\theta | D) = \arg\max_\theta P(D | \theta) P(\theta)
$$

### Log í˜•íƒœ

$$
\hat{\theta}_{MAP} = \arg\max_\theta \left[ \log P(D | \theta) + \log P(\theta) \right]
$$

### ì •ê·œí™”ë¡œì˜ í•´ì„

$$
\hat{\theta}_{MAP} = \arg\min_\theta \left[ \text{NLL} - \log P(\theta) \right]
$$

- $P(\theta) = \mathcal{N}(0, \sigma^2)$ â†’ L2 ì •ê·œí™” (Weight Decay)
- $P(\theta) = \text{Laplace}(0, b)$ â†’ L1 ì •ê·œí™”

**MAP = MLE + ì •ê·œí™”!**

---

## êµ¬í˜„

```python
import numpy as np
import torch
import torch.nn as nn

# ì˜ˆì‹œ 1: ë² ë¥´ëˆ„ì´ MLE
data = np.array([1, 1, 1, 0, 1, 1, 0, 1, 0, 1])  # 7 ì„±ê³µ, 3 ì‹¤íŒ¨
p_mle = data.mean()
print(f"ë² ë¥´ëˆ„ì´ p MLE: {p_mle}")  # 0.7

# ì˜ˆì‹œ 2: ì •ê·œ ë¶„í¬ MLE
data = np.random.normal(loc=5, scale=2, size=1000)
mu_mle = data.mean()
sigma_mle = data.std()  # MLEëŠ” nìœ¼ë¡œ ë‚˜ëˆ”
print(f"ì •ê·œ ë¶„í¬ Î¼ MLE: {mu_mle:.3f}")
print(f"ì •ê·œ ë¶„í¬ Ïƒ MLE: {sigma_mle:.3f}")

# ì˜ˆì‹œ 3: ì‹ ê²½ë§ í•™ìŠµ = MLE
model = nn.Sequential(
    nn.Linear(10, 32),
    nn.ReLU(),
    nn.Linear(32, 5),  # 5 í´ë˜ìŠ¤
)

# Cross-Entropy Loss = NLL
criterion = nn.CrossEntropyLoss()  # ë‚´ë¶€ì ìœ¼ë¡œ softmax + NLL

# ê°€ìƒì˜ ë°ì´í„°
x = torch.randn(32, 10)
y = torch.randint(0, 5, (32,))

# Forward
logits = model(x)
loss = criterion(logits, y)  # NLL â†’ ì´ê±¸ ìµœì†Œí™” = MLE

print(f"NLL Loss: {loss.item():.4f}")

# L2 ì •ê·œí™” = MAP with Gaussian prior
optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.01)
# weight_decay = L2 regularization = Gaussian prior
```

---

## í•µì‹¬ ì •ë¦¬

| ê°œë… | í•µì‹¬ |
|------|------|
| ìš°ë„ | $L(\theta) = P(D \| \theta)$ |
| MLE | $\arg\max_\theta P(D \| \theta)$ |
| NLL | $-\log P(D \| \theta)$ |
| CE Loss | ì¹´í…Œê³ ë¦¬ì»¬ ë¶„í¬ì˜ NLL |
| MSE Loss | ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ NLL |
| MAP | MLE + Prior = MLE + ì •ê·œí™” |

---

## ê´€ë ¨ ì½˜í…ì¸ 

- [ë² ì´ì¦ˆ ì •ë¦¬](/ko/docs/math/probability/bayes) - MAPì˜ ê¸°ë°˜
- [Cross-Entropy Loss](/ko/docs/math/training/loss/cross-entropy) - MLEì˜ êµ¬í˜„
- [Weight Decay](/ko/docs/math/training/regularization/weight-decay) - MAP ê´€ì 
- [í™•ë¥ ë¶„í¬](/ko/docs/math/probability/distribution) - ìš°ë„ í•¨ìˆ˜ì˜ í˜•íƒœ
