---
title: "ìƒ˜í”Œë§"
weight: 3
math: true
---

# ìƒ˜í”Œë§ (Sampling)

## ê°œìš”

> ğŸ’¡ **ìƒ˜í”Œë§**: í™•ë¥ ë¶„í¬ì—ì„œ ê°’ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•

ìƒì„± ëª¨ë¸ê³¼ í™•ë¥ ì  í•™ìŠµì˜ í•µì‹¬ì…ë‹ˆë‹¤.

### Temperature Scaling ì‹œê°í™”

![Temperature Scaling](/images/probability/ko/sampling-temperature.svg)

## ê¸°ë³¸ ìƒ˜í”Œë§

### ê· ë“± ë¶„í¬ ìƒ˜í”Œë§

```python
import torch

# [0, 1) ê· ë“± ë¶„í¬
samples = torch.rand(1000)

# [a, b) ê· ë“± ë¶„í¬
a, b = 2, 5
samples = a + (b - a) * torch.rand(1000)
```

### ê°€ìš°ì‹œì•ˆ ìƒ˜í”Œë§

```python
# N(0, 1) í‘œì¤€ì •ê·œë¶„í¬
samples = torch.randn(1000)

# N(mu, sigma^2)
mu, sigma = 5, 2
samples = mu + sigma * torch.randn(1000)
```

## Reparameterization Trick

í™•ë¥ ì  ë…¸ë“œë¥¼ í†µí•œ ì—­ì „íŒŒë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê¸°ë²•:

**ë¬¸ì œ**: z ~ N(Î¼, ÏƒÂ²) ì—ì„œ ìƒ˜í”Œë§í•˜ë©´ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥

**í•´ê²°**: z = Î¼ + Ïƒ * Îµ, Îµ ~ N(0, 1)

```python
class VAE(nn.Module):
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_logvar(h)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)  # ë¯¸ë¶„ê³¼ ë¬´ê´€
        return mu + std * eps        # ë¯¸ë¶„ ê°€ëŠ¥

    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var
```

## ì¹´í…Œê³ ë¦¬ ìƒ˜í”Œë§

### Argmax (ê²°ì •ì )

```python
logits = model(x)
prediction = logits.argmax(dim=-1)  # í•­ìƒ ê°™ì€ ê²°ê³¼
```

### í™•ë¥ ì  ìƒ˜í”Œë§

```python
probs = torch.softmax(logits, dim=-1)
samples = torch.multinomial(probs, num_samples=1)
```

### Gumbel-Softmax (ë¯¸ë¶„ ê°€ëŠ¥ ìƒ˜í”Œë§)

```python
def gumbel_softmax(logits, tau=1.0, hard=False):
    gumbels = -torch.log(-torch.log(torch.rand_like(logits)))
    y_soft = torch.softmax((logits + gumbels) / tau, dim=-1)

    if hard:
        # Forward: one-hot, Backward: soft
        index = y_soft.argmax(dim=-1, keepdim=True)
        y_hard = torch.zeros_like(logits).scatter_(-1, index, 1.0)
        return y_hard - y_soft.detach() + y_soft

    return y_soft
```

## Temperature Scaling

ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ:

$$
p_i = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}
$$

- T â†’ 0: Argmax (ê²°ì •ì )
- T = 1: ì›ë˜ ë¶„í¬
- T â†’ âˆ: ê· ë“± ë¶„í¬ (ëœë¤)

```python
def sample_with_temperature(logits, temperature=1.0):
    scaled_logits = logits / temperature
    probs = torch.softmax(scaled_logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

## Top-k, Top-p ìƒ˜í”Œë§

### Top-k

ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤:

```python
def top_k_sampling(logits, k=50):
    top_k_logits, top_k_indices = logits.topk(k)
    probs = torch.softmax(top_k_logits, dim=-1)
    sample_idx = torch.multinomial(probs, num_samples=1)
    return top_k_indices.gather(-1, sample_idx)
```

### Top-p (Nucleus)

ëˆ„ì  í™•ë¥  pê¹Œì§€ì˜ í† í°ë§Œ ê³ ë ¤:

```python
def top_p_sampling(logits, p=0.9):
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)

    # ëˆ„ì  í™•ë¥  p ì´ˆê³¼ í† í° ì œê±°
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False

    sorted_logits[sorted_indices_to_remove] = float('-inf')
    probs = torch.softmax(sorted_logits, dim=-1)
    return sorted_indices.gather(-1, torch.multinomial(probs, num_samples=1))
```

## ê´€ë ¨ ì½˜í…ì¸ 

- [í™•ë¥ ë¶„í¬](/ko/docs/math/probability/distribution)
- [VAE](/ko/docs/architecture/generative/vae) - Reparameterization í™œìš©
- [Diffusion](/ko/docs/math/diffusion) - ë°˜ë³µì  ìƒ˜í”Œë§
- [VLM](/ko/docs/architecture/multimodal/vlm) - í…ìŠ¤íŠ¸ ìƒì„± ì‹œ ìƒ˜í”Œë§
