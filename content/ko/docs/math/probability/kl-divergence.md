---
title: "KL ë°œì‚°"
weight: 11
math: true
---

# KL ë°œì‚° (Kullback-Leibler Divergence)

## ê°œìš”

> ğŸ’¡ **KL Divergence**: ë‘ í™•ë¥ ë¶„í¬ê°€ **ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€** ì¸¡ì •

VAE, ì§€ì‹ ì¦ë¥˜, ì •ì±… ìµœì í™” ë“± ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ìˆ˜í•™ì…ë‹ˆë‹¤.

### ì‹œê°ì  ì´í•´

![KL Divergence](/images/probability/kl-divergence.svg)

---

## ì •ì˜

$$
D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} = \mathbb{E}_P \left[ \log \frac{P(X)}{Q(X)} \right]
$$

ì—°ì†ì¸ ê²½ìš°:
$$
D_{KL}(P \| Q) = \int p(x) \log \frac{p(x)}{q(x)} \, dx
$$

### ë‹¤ë¥¸ í‘œí˜„

$$
D_{KL}(P \| Q) = H(P, Q) - H(P)
$$

Cross-Entropyì—ì„œ ì—”íŠ¸ë¡œí”¼ë¥¼ ë¹¼ë©´ KL Divergence.

---

## ì§ê´€ì  ì´í•´

### ì •ë³´ ì´ë¡ ì  í•´ì„

- Pë¥¼ "ì§„ì§œ" ë¶„í¬ë¼ê³  í•  ë•Œ
- Që¡œ Pë¥¼ ì¸ì½”ë”©í•˜ë©´ ì–¼ë§ˆë‚˜ ì¶”ê°€ ë¹„íŠ¸ê°€ í•„ìš”í•œê°€

```
P = ì‹¤ì œ ë°ì´í„° ë¶„í¬
Q = ëª¨ë¸ì´ í•™ìŠµí•œ ë¶„í¬

D_KL(P || Q) = Pë¡œ ìƒ˜í”Œë§í•  ë•Œ Qì˜ ë†€ë¼ì›€ - Pì˜ ë†€ë¼ì›€
             = "Qê°€ Pë¥¼ ì˜ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” ì •ë„"
```

### ì˜ˆì‹œ

```
P: [0.5, 0.5]      (ê³µì •í•œ ë™ì „)
Q: [0.9, 0.1]      (í¸í–¥ëœ ë™ì „)

D_KL(P || Q) = 0.5 Ã— log(0.5/0.9) + 0.5 Ã— log(0.5/0.1)
             = 0.5 Ã— (-0.85) + 0.5 Ã— (2.32)
             â‰ˆ 0.74 bits
```

---

## KL ë°œì‚°ì˜ ì„±ì§ˆ

### 1. ë¹„ìŒìˆ˜ì„± (Gibbs' Inequality)

$$
D_{KL}(P \| Q) \geq 0
$$

ë“±í˜¸ëŠ” $P = Q$ì¼ ë•Œë§Œ ì„±ë¦½.

### 2. ë¹„ëŒ€ì¹­ì„± âš ï¸

$$
D_{KL}(P \| Q) \neq D_{KL}(Q \| P)
$$

**ì¤‘ìš”**: KL ë°œì‚°ì€ ê±°ë¦¬(metric)ê°€ ì•„ë‹™ë‹ˆë‹¤!

### Forward vs Reverse KL

| | Forward KL: $D_{KL}(P \| Q)$ | Reverse KL: $D_{KL}(Q \| P)$ |
|---|---|---|
| ìµœì†Œí™” ëŒ€ìƒ | Q (ëª¨ë¸) | Q (ëª¨ë¸) |
| Pê°€ ë†’ì€ë° Qê°€ ë‚®ìœ¼ë©´ | í° í˜ë„í‹° (0ìœ¼ë¡œ ë‚˜ëˆ”) | ì‘ì€ í˜ë„í‹° |
| íŠ¹ì„± | Mode-covering | Mode-seeking |
| ê²°ê³¼ | ëª¨ë“  ëª¨ë“œ ì»¤ë²„, íë¦¿ | í•˜ë‚˜ì˜ ëª¨ë“œ ì§‘ì¤‘, ì„ ëª… |

```
P (ì‹¤ì œ: ë‘ ë´‰ìš°ë¦¬)       Forward KL ê²°ê³¼        Reverse KL ê²°ê³¼

   â•­â”€â•®   â•­â”€â•®               â•­â”€â”€â”€â”€â”€â•®               â•­â”€â•®
   â”‚ â”‚   â”‚ â”‚               â”‚     â”‚               â”‚ â”‚
â”€â”€â”€â”´â”€â”´â”€â”€â”€â”´â”€â”´â”€â”€â”€         â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€         â”€â”€â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€
    â†‘       â†‘               íë¦¿í•˜ì§€ë§Œ             í•˜ë‚˜ë§Œ ì„ íƒ
  mode1  mode2            ë‘˜ ë‹¤ ì»¤ë²„
```

---

## ë”¥ëŸ¬ë‹ì—ì„œì˜ í™œìš©

### 1. VAE (Variational Autoencoder)

$$
\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))
$$

- $q(z|x)$: ì¸ì½”ë” (ê·¼ì‚¬ ì‚¬í›„ ë¶„í¬)
- $p(z)$: ì‚¬ì „ ë¶„í¬ (í‘œì¤€ ì •ê·œ)
- KL í•­: ì ì¬ ê³µê°„ì„ ì •ê·œí™”

### 2. ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)

$$
\mathcal{L}_{KD} = T^2 \cdot D_{KL}(P_{teacher} \| P_{student})
$$

Teacherì˜ soft labelì„ Studentê°€ ë”°ë¼í•˜ê²Œ.

### 3. PPO (ê°•í™”í•™ìŠµ ì •ì±… ìµœì í™”)

$$
\text{clip}\left( \frac{\pi_{new}}{\pi_{old}}, 1-\epsilon, 1+\epsilon \right)
$$

ìƒˆ ì •ì±…ì´ êµ¬ ì •ì±…ì—ì„œ ë„ˆë¬´ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡.

### 4. ì •ê·œí™”

ë ˆì´ë¸” ìŠ¤ë¬´ë”©, Focal Loss ë“±ë„ ë¶„í¬ ê°„ ê±°ë¦¬ ê°œë….

---

## ê°€ìš°ì‹œì•ˆ KL ë°œì‚°

ë‘ ì •ê·œ ë¶„í¬ ì‚¬ì´ì˜ KL Divergence (í•´ì„ì  í•´):

$$
D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
$$

### VAEì—ì„œ ìì£¼ ì“°ëŠ” í˜•íƒœ

$q(z|x) = \mathcal{N}(\mu, \sigma^2)$, $p(z) = \mathcal{N}(0, 1)$ì¼ ë•Œ:

$$
D_{KL}(q \| p) = -\frac{1}{2} \sum_{j=1}^{J} \left( 1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2 \right)
$$

---

## êµ¬í˜„

```python
import numpy as np
import torch
import torch.nn.functional as F

def kl_divergence_discrete(p, q, eps=1e-10):
    """ì´ì‚° ë¶„í¬ì˜ KL Divergence"""
    p = np.array(p) + eps
    q = np.array(q) + eps
    return np.sum(p * np.log(p / q))

def kl_divergence_gaussian(mu1, sigma1, mu2, sigma2):
    """ë‘ 1D ê°€ìš°ì‹œì•ˆ ì‚¬ì´ì˜ KL Divergence"""
    return (np.log(sigma2/sigma1) +
            (sigma1**2 + (mu1-mu2)**2) / (2*sigma2**2) - 0.5)

def kl_divergence_vae(mu, logvar):
    """VAEì—ì„œ ì‚¬ìš©í•˜ëŠ” KL Divergence (vs í‘œì¤€ ì •ê·œ)"""
    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

# ì˜ˆì‹œ 1: ì´ì‚° ë¶„í¬
p = [0.5, 0.5]
q = [0.9, 0.1]
print(f"D_KL(P || Q) = {kl_divergence_discrete(p, q):.4f}")
print(f"D_KL(Q || P) = {kl_divergence_discrete(q, p):.4f}")  # ë‹¤ë¥¸ ê°’!

# ì˜ˆì‹œ 2: ê°€ìš°ì‹œì•ˆ
kl_gauss = kl_divergence_gaussian(mu1=1, sigma1=1, mu2=0, sigma2=1)
print(f"D_KL(N(1,1) || N(0,1)) = {kl_gauss:.4f}")

# ì˜ˆì‹œ 3: VAE KL Loss
mu = torch.randn(32, 64)      # ë°°ì¹˜ 32, ì ì¬ ì°¨ì› 64
logvar = torch.randn(32, 64)  # log(sigma^2)
kl_loss = kl_divergence_vae(mu, logvar)
print(f"VAE KL Loss: {kl_loss.item():.4f}")

# ì˜ˆì‹œ 4: PyTorchì˜ KL Divergence
p_logits = torch.tensor([[1.0, 2.0, 3.0]])
q_logits = torch.tensor([[3.0, 2.0, 1.0]])

p_probs = F.softmax(p_logits, dim=-1)
q_probs = F.softmax(q_logits, dim=-1)

# F.kl_divëŠ” log_probsë¥¼ ë°›ìŒ
kl_pt = F.kl_div(q_probs.log(), p_probs, reduction='sum')
print(f"PyTorch KL: {kl_pt.item():.4f}")
```

---

## ë‹¤ë¥¸ ë°œì‚°/ê±°ë¦¬ì™€ ë¹„êµ

| ì¸¡ë„ | ìˆ˜ì‹ | ëŒ€ì¹­ | íŠ¹ì§• |
|------|------|------|------|
| KL Divergence | $\sum p \log(p/q)$ | âŒ | ì •ë³´ ì´ë¡ ì  ì˜ë¯¸ |
| Jensen-Shannon | $\frac{1}{2}D_{KL}(P\|M) + \frac{1}{2}D_{KL}(Q\|M)$ | âœ… | ëŒ€ì¹­í™”ëœ KL |
| Wasserstein | $\inf_\gamma \mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|]$ | âœ… | ê¸°í•˜í•™ì  ê±°ë¦¬ |
| Total Variation | $\frac{1}{2}\sum\|p-q\|$ | âœ… | L1 ê±°ë¦¬ |

---

## í•µì‹¬ ì •ë¦¬

| ê°œë… | í•µì‹¬ |
|------|------|
| ì •ì˜ | $D_{KL}(P\|Q) = \sum p \log(p/q)$ |
| ì˜ë¯¸ | Pë¥¼ Që¡œ í‘œí˜„í•  ë•Œ í•„ìš”í•œ ì¶”ê°€ ì •ë³´ |
| ë¹„ëŒ€ì¹­ | $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$ |
| ë¹„ìŒìˆ˜ | $D_{KL}(P\|Q) \geq 0$ |
| Cross-Entropy ê´€ê³„ | $H(P,Q) = H(P) + D_{KL}(P\|Q)$ |

---

## ê´€ë ¨ ì½˜í…ì¸ 

- [ì—”íŠ¸ë¡œí”¼](/docs/math/probability/entropy) - KL Divergenceì˜ ê¸°ë°˜
- [Cross-Entropy Loss](/docs/math/training/loss/cross-entropy) - ì†ì‹¤ í•¨ìˆ˜ì™€ì˜ ê´€ê³„
- [í™•ë¥ ë¶„í¬](/docs/math/probability/distribution) - ë¶„í¬ ê¸°ì´ˆ
