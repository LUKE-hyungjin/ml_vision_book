---
title: "ì—”íŠ¸ë¡œí”¼"
weight: 10
math: true
---

# ì—”íŠ¸ë¡œí”¼ (Entropy)

## ê°œìš”

> ğŸ’¡ **ì—”íŠ¸ë¡œí”¼**: í™•ë¥ ë¶„í¬ì˜ **ë¶ˆí™•ì‹¤ì„±**ì„ ì¸¡ì •í•˜ëŠ” ê°’

- ê²°ê³¼ê°€ ë»”í•˜ë©´ â†’ ì—”íŠ¸ë¡œí”¼ ë‚®ìŒ
- ê²°ê³¼ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ â†’ ì—”íŠ¸ë¡œí”¼ ë†’ìŒ

Cross-Entropy Lossì˜ í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤!

### ì‹œê°ì  ì´í•´

![ì—”íŠ¸ë¡œí”¼ì™€ Cross-Entropy](/images/probability/ko/entropy-crossentropy.svg)

---

## ì •ë³´ëŸ‰ (Information)

### ì§ê´€

- í™•ë¥ ì´ ë‚®ì€ ì‚¬ê±´ì´ ì¼ì–´ë‚˜ë©´ â†’ ë†€ë¼ì›€ì´ í¼ â†’ ì •ë³´ëŸ‰ì´ ë§ìŒ
- í™•ë¥ ì´ ë†’ì€ ì‚¬ê±´ì´ ì¼ì–´ë‚˜ë©´ â†’ ë‹¹ì—°í•¨ â†’ ì •ë³´ëŸ‰ì´ ì ìŒ

### ì •ì˜

$$
I(x) = -\log P(x) = \log \frac{1}{P(x)}
$$

**ì˜ˆì‹œ**:
- ê³µì •í•œ ë™ì „ì˜ ì•ë©´: $I = -\log_2(0.5) = 1$ bit
- ì£¼ì‚¬ìœ„ 1ì´ ë‚˜ì˜´: $I = -\log_2(1/6) \approx 2.58$ bits

---

## ì—”íŠ¸ë¡œí”¼ (Entropy)

### ì •ì˜

ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’:

$$
H(X) = -\sum_x P(x) \log P(x) = \mathbb{E}[-\log P(X)]
$$

ì—°ì†ì¸ ê²½ìš° (ë¯¸ë¶„ ì—”íŠ¸ë¡œí”¼):
$$
H(X) = -\int f(x) \log f(x) \, dx
$$

### ì§ê´€ì  ì´í•´

```
í™•ì •ì  (ì—”íŠ¸ë¡œí”¼ ë‚®ìŒ)     ê· ë“± (ì—”íŠ¸ë¡œí”¼ ë†’ìŒ)

P(x)                      P(x)
  â”‚                         â”‚
1 â”œâ”€â”€â”                    â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚  â”‚                      â”‚
  â””â”€â”€â”´â”€â”€â”€â”€â”€â†’ x              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     ê²°ê³¼ê°€ ë»”í•¨               ë­ê°€ ë‚˜ì˜¬ì§€ ëª¨ë¦„
     H â‰ˆ 0                   H = log(n)
```

### ì—”íŠ¸ë¡œí”¼ì˜ ì„±ì§ˆ

1. **ë¹„ìŒìˆ˜**: $H(X) \geq 0$
2. **ê· ë“±ë¶„í¬ì—ì„œ ìµœëŒ€**: $H \leq \log n$ (nê°œ ê²°ê³¼)
3. **ê²°ì •ì ì´ë©´ 0**: $P(x) = 1$ì´ë©´ $H = 0$

---

## ì£¼ìš” ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼

| ë¶„í¬ | ì—”íŠ¸ë¡œí”¼ |
|------|----------|
| ë² ë¥´ëˆ„ì´$(p)$ | $-p\log p - (1-p)\log(1-p)$ |
| ê· ë“± (nê°œ) | $\log n$ |
| ì •ê·œ$(\mu, \sigma^2)$ | $\frac{1}{2}\log(2\pi e \sigma^2)$ |

### ë² ë¥´ëˆ„ì´ ì—”íŠ¸ë¡œí”¼ ê·¸ë˜í”„

```
H(p)
  â”‚      â•­â”€â”€â”€â•®
1 â”œâ”€â”€â”€â”€â”€â•±     â•²â”€â”€â”€â”€â”€
  â”‚    â•±       â•²
  â”‚   â•±         â•²
  â”‚  â•±           â•²
0 â”œâ”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â†’ p
  0      0.5      1

p=0.5ì¼ ë•Œ ìµœëŒ€ (ê°€ì¥ ë¶ˆí™•ì‹¤)
p=0 ë˜ëŠ” 1ì¼ ë•Œ ìµœì†Œ (í™•ì •ì )
```

---

## Cross-Entropy

### ì •ì˜

ë¶„í¬ $p$ë¥¼ ë¶„í¬ $q$ë¡œ ì¸ì½”ë”©í•  ë•Œ í•„ìš”í•œ í‰ê·  ë¹„íŠ¸ ìˆ˜:

$$
H(p, q) = -\sum_x p(x) \log q(x) = \mathbb{E}_p[-\log q(X)]
$$

### ë”¥ëŸ¬ë‹ì—ì„œì˜ Cross-Entropy Loss

- $p$: ì‹¤ì œ ë ˆì´ë¸” ë¶„í¬ (one-hot)
- $q$: ëª¨ë¸ ì˜ˆì¸¡ (softmax ì¶œë ¥)

**ì´ì§„ ë¶„ë¥˜**:
$$
\mathcal{L} = -[y \log \hat{y} + (1-y) \log(1-\hat{y})]
$$

**ë‹¤ì¤‘ í´ë˜ìŠ¤**:
$$
\mathcal{L} = -\sum_c y_c \log \hat{y}_c
$$

one-hotì´ë©´:
$$
\mathcal{L} = -\log \hat{y}_{true}
$$

### ì™œ Cross-Entropyì¸ê°€?

```
Cross-Entropy = Entropy + KL Divergence

H(p, q) = H(p) + D_KL(p || q)
         â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         ìƒìˆ˜       ìµœì†Œí™” ëŒ€ìƒ
         (ë ˆì´ë¸”)   (pì™€ që¥¼ ê°™ê²Œ)
```

H(p)ëŠ” ê³ ì •ì´ë¯€ë¡œ, Cross-Entropy ìµœì†Œí™” = KL Divergence ìµœì†Œí™”

---

## êµ¬í˜„

```python
import numpy as np

def entropy(p):
    """ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë°‘ì´ 2ì¸ ë¡œê·¸)"""
    # 0 * log(0) = 0 ì²˜ë¦¬
    p = np.array(p)
    p = p[p > 0]
    return -np.sum(p * np.log2(p))

def cross_entropy(p, q, eps=1e-15):
    """Cross-Entropy ê³„ì‚°"""
    p = np.array(p)
    q = np.clip(q, eps, 1 - eps)  # log(0) ë°©ì§€
    return -np.sum(p * np.log(q))

# ì˜ˆì‹œ 1: ê· ë“± ë¶„í¬
uniform = [0.25, 0.25, 0.25, 0.25]
print(f"ê· ë“± ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {entropy(uniform):.3f} bits")  # 2.0

# ì˜ˆì‹œ 2: í¸í–¥ëœ ë¶„í¬
biased = [0.9, 0.05, 0.03, 0.02]
print(f"í¸í–¥ëœ ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {entropy(biased):.3f} bits")  # 0.61

# ì˜ˆì‹œ 3: ê²°ì •ì  ë¶„í¬
deterministic = [1.0, 0.0, 0.0, 0.0]
print(f"ê²°ì •ì  ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {entropy(deterministic):.3f} bits")  # 0.0

# Cross-Entropy Loss (ë‹¤ì¤‘ í´ë˜ìŠ¤)
y_true = [0, 0, 1, 0]  # ì •ë‹µ: í´ë˜ìŠ¤ 2
y_pred = [0.1, 0.2, 0.6, 0.1]  # ëª¨ë¸ ì˜ˆì¸¡
loss = cross_entropy(y_true, y_pred)
print(f"Cross-Entropy Loss: {loss:.3f}")

# PyTorchì—ì„œ
import torch
import torch.nn.functional as F

logits = torch.tensor([[1.0, 2.0, 3.0, 1.0]])  # ë¡œì§“
target = torch.tensor([2])  # ì •ë‹µ ì¸ë±ìŠ¤

loss_pt = F.cross_entropy(logits, target)
print(f"PyTorch CE Loss: {loss_pt.item():.3f}")
```

---

## ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼

### ì •ì˜

$$
H(Y|X) = \mathbb{E}_X[H(Y|X=x)] = -\sum_{x,y} P(x,y) \log P(y|x)
$$

### ì˜ë¯¸

Xë¥¼ ì•Œ ë•Œ Yì˜ ë‚¨ì€ ë¶ˆí™•ì‹¤ì„±.

### ì„±ì§ˆ

$$
H(Y|X) \leq H(Y)
$$

ì •ë³´ë¥¼ ë” ì•Œë©´ ë¶ˆí™•ì‹¤ì„±ì€ ì¤„ì–´ë“¦ (ë˜ëŠ” ê°™ìŒ).

---

## ìƒí˜¸ ì •ë³´ëŸ‰ (Mutual Information)

### ì •ì˜

$$
I(X; Y) = H(Y) - H(Y|X) = H(X) - H(X|Y)
$$

### ì˜ë¯¸

Xì™€ Yê°€ ê³µìœ í•˜ëŠ” ì •ë³´ëŸ‰.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         H(X,Y)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      H(X)           â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”         â”‚    â”‚
â”‚  â”‚   â”‚ I(X;Y)â”‚   H(Y)  â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë”¥ëŸ¬ë‹ì—ì„œì˜ í™œìš©

- íŠ¹ì„± ì„ íƒ
- í‘œí˜„ í•™ìŠµ (InfoNCE, Contrastive Learning)
- ì •ë³´ ë³‘ëª© (Information Bottleneck)

---

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ìˆ˜ì‹ | ì˜ë¯¸ |
|------|------|------|
| ì •ë³´ëŸ‰ | $I(x) = -\log P(x)$ | ì‚¬ê±´ì˜ ë†€ë¼ì›€ |
| ì—”íŠ¸ë¡œí”¼ | $H(X) = -\sum p(x)\log p(x)$ | í‰ê·  ë¶ˆí™•ì‹¤ì„± |
| Cross-Entropy | $H(p,q) = -\sum p(x)\log q(x)$ | ë¶„í¬ ê°„ ì°¨ì´ |

---

## ê´€ë ¨ ì½˜í…ì¸ 

- [KL ë°œì‚°](/ko/docs/math/probability/kl-divergence) - ë¶„í¬ ê°„ ê±°ë¦¬
- [Cross-Entropy Loss](/ko/docs/math/training/loss/cross-entropy) - ì†ì‹¤ í•¨ìˆ˜ë¡œì˜ ì ìš©
- [í™•ë¥ ë¶„í¬](/ko/docs/math/probability/distribution) - ë¶„í¬ë³„ ì—”íŠ¸ë¡œí”¼
