---
title: "Positional Encoding"
weight: 4
math: true
---

# Positional Encoding

{{% hint info %}}
**Prerequisites**: [Self-Attention](/en/docs/components/attention/self-attention)
{{% /hint %}}

## One-line Summary
> **Attention is order-blind, so position information must be explicitly injected.**

## Why is this needed?

### The Problem: Attention Ignores Order

At its core, Self-Attention is a **set operation**. Changing the input order produces identical outputs (for each position).

{{< figure src="/images/components/attention/ko/positional-encoding-sinusoidal.jpeg" caption="Sinusoidal Positional Encoding Pattern — Lower dimensions oscillate faster" >}}

```
Feed Self-Attention "dog bites man":
  → Computes relationships between all words

Feed Self-Attention "man bites dog":
  → Same words, computes same relationships
  → Identical result! (no order information)
```

**Problem**: "Dog bites man" and "man bites dog" have completely different meanings, but Attention alone can't distinguish them.

### Why Don't CNNs Have This Problem?

CNN kernels **slide in order**. What's computed at position 1 vs. position 3 comes from different locations, so position information is naturally included.

Attention sees all positions at once, so **position information must be separately injected**.

### The Solution: Add Position Information

The idea is simple. Add a **unique pattern** to each position:

```
input = word embedding + position encoding

Position 0: [0.2, -0.5, 0.8] + [sin(0), cos(0), sin(0)] = [0.2, 0.5, 0.8]
Position 1: [0.1, 0.3, -0.2] + [sin(1), cos(1), sin(1)] = [0.94, 0.84, 0.64]
Position 2: [0.7, 0.1, 0.4] + [sin(2), cos(2), sin(2)] = [1.61, -0.32, 1.31]

→ Same word at different positions gets different values
→ Attention can now distinguish order
```

---

## Method 1: Sinusoidal Positional Encoding

Proposed in the original Transformer paper ("Attention Is All You Need," 2017).

### Formula

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

**Symbol meanings:**
- $pos$: Position index (0, 1, 2, ...)
- $i$: Dimension index (0, 1, 2, ...)
- $d$: Total embedding dimension
- $10000$: Constant controlling frequency

### Intuition

**Clock analogy**: Think of hour, minute, and second hands.

```
Hour hand:   Rotates slowly (1 revolution per 12 hours) → Large time units
Minute hand: Medium speed (1 revolution per hour)       → Medium time units
Second hand: Rotates fast (1 revolution per minute)     → Small time units

→ Combining hours, minutes, seconds uniquely represents any time!
```

Sinusoidal PE works the same way:
- **Low dimensions (small i)**: Fast oscillation → Distinguishes nearby positions
- **High dimensions (large i)**: Slow oscillation → Distinguishes distant positions
- Combining all dimensions → **Each position has a unique pattern**

### Advantages

| Advantage | Description |
|-----------|-------------|
| **No learning required** | Generated by math formula → 0 parameters |
| **Arbitrary length** | Can handle sequences longer than training data |
| **Relative position** | $PE(pos+k)$ is a linear transform of $PE(pos)$ → Enables relative position learning |

### Implementation

```python
import torch
import math

def sinusoidal_positional_encoding(max_len, d_model):
    """
    Sinusoidal Positional Encoding

    max_len: Maximum sequence length (e.g., 1000)
    d_model: Embedding dimension (e.g., 512)
    """
    pe = torch.zeros(max_len, d_model)

    # Position indices: [0, 1, 2, ..., max_len-1]
    position = torch.arange(0, max_len).unsqueeze(1).float()

    # Frequency: Higher dimensions oscillate more slowly
    div_term = torch.exp(
        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
    )

    # Even dimensions: sin, Odd dimensions: cos
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)

    return pe  # (max_len, d_model)

# Usage
pe = sinusoidal_positional_encoding(1000, 512)
x = x + pe[:seq_len]  # Add position info to input embeddings
```

---

## Method 2: Learnable Positional Encoding

Used in BERT and ViT. The position encoding is **learned**.

### Idea

"Instead of a math formula, let the model learn the optimal position representation from data."

### Implementation

```python
import torch.nn as nn

class LearnablePositionalEncoding(nn.Module):
    def __init__(self, max_len, d_model):
        super().__init__()
        # Learnable parameter (nn.Parameter)
        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))

    def forward(self, x):
        # x: (B, N, d_model)
        return x + self.pe[:, :x.size(1)]

# Usage
pe = LearnablePositionalEncoding(max_len=197, d_model=768)
x = pe(x)  # Add position info
```

### Pros and Cons

| | Pros | Cons |
|---|------|------|
| **Learnable** | Optimized for data | Only handles lengths seen during training |
| **Sinusoidal** | Handles any length | Cannot adapt to data |

---

## Method 3: 2D Positional Encoding (ViT)

Images are 2D grids, not 1D sequences. ViT splits images into patches and applies positional encoding to each patch.

```
224×224 image → split into 16×16 patches → 14×14 = 196 patches
+ 1 [CLS] token = 197 positions
```

### Implementation

```python
class ViTPositionalEncoding(nn.Module):
    def __init__(self, num_patches, d_model):
        super().__init__()
        # num_patches = (H // patch_size) * (W // patch_size)
        # +1: position for [CLS] token
        self.pe = nn.Parameter(torch.randn(1, num_patches + 1, d_model))

    def forward(self, x):
        # x: (B, 1+num_patches, d_model) — [CLS] + patches
        return x + self.pe

# ViT-Base: 14×14 = 196 patches, 768 dim
vit_pe = ViTPositionalEncoding(num_patches=196, d_model=768)
```

When visualized, ViT's Positional Encoding reveals a 2D grid pattern where neighboring patches have similar values.

---

## Method 4: Rotary Positional Encoding (RoPE)

The standard in modern LLMs (LLaMA, Qwen, Gemma, etc.).

### Core Idea

Previous methods: **Add** position info to input (additive)
RoPE: **Rotate** Q and K based on position (rotary)

```
Previous: Q + PE(pos), K + PE(pos)
RoPE:     Rotate(Q, pos), Rotate(K, pos)
```

### Advantages

| Advantage | Description |
|-----------|-------------|
| **Natural relative position encoding** | Dot product of two positions automatically depends on relative distance |
| **Arbitrary length extension** | Can handle sequences longer than training data |
| **Computationally efficient** | Only simple rotation operations needed |

### Implementation

```python
def rotate_half(x):
    """Swap and negate front/back halves of vector"""
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat([-x2, x1], dim=-1)

def apply_rope(q, k, cos, sin):
    """
    Apply RoPE: Rotate Q and K based on position

    cos, sin: pre-computed rotation angles per position
    """
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

# Usage (LLaMA style)
# cos, sin = precompute_freqs(seq_len, head_dim)
# q, k = apply_rope(q, k, cos, sin)
```

### Intuition

Consider rotating a 2D vector by angle $\theta$:

$$
\begin{pmatrix} x' \\ y' \end{pmatrix} = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}
$$

RoPE extends this to high dimensions. Applying different rotation angles at each position causes the dot product of two positions to automatically depend only on their **relative distance**.

---

## Method 5: ALiBi (Attention with Linear Biases)

Used in BLOOM-style models. Instead of adding positional vectors to inputs, ALiBi adds a **distance-based bias** directly to attention logits.

### Formula

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} - m_h \cdot |i - j|\right) V
$$

**Symbol meanings:**
- $Q, K, V$: Query, Key, Value matrices
- $d_k$: Key dimension (for scaling)
- $i, j$: Query/Key position indices
- $|i-j|$: Absolute token distance
- $m_h$: Slope for head $h$ (different heads use different distance penalties)

- **Practical note (decoder causal setup)**: With a causal mask, valid pairs satisfy $j \le i$, so using $(i-j)$ instead of $|i-j|$ gives the same effect on valid positions. Different codebases use different sign/direction conventions, so verify it together with the causal mask.

### Intuition

Heads with larger $m_h$ focus more on nearby tokens; heads with smaller $m_h$ can look farther away.
This naturally creates multi-scale context windows without learning extra positional parameters.

### Per-head slope construction (common practical pattern)

ALiBi uses different slopes $m_h$ for different heads, so some heads specialize in short-range context while others keep longer-range context.

```python
import torch

def build_alibi_slopes(n_heads: int) -> torch.Tensor:
    """ALiBi slope per head (larger slope = stronger local bias).

    Returns shape: (n_heads,)
    """
    # Common power-of-two-style schedule used in many ALiBi implementations
    # Produces monotonically decreasing slopes even when n_heads is not a power of two
    start = 2 ** (-8.0 / n_heads)
    return torch.tensor([start ** (i + 1) for i in range(n_heads)], dtype=torch.float32)

# Example: 8 heads
slopes = build_alibi_slopes(8)
print(slopes)  # tensor([0.9170, 0.8409, ..., 0.5000])
```

> Exact slope formulas vary by codebase, but the key principle is the same: **monotonically decreasing per-head slopes**.

### Minimal implementation sketch

```python
# scores: (batch, heads, q_len, k_len)
# distance: |i-j| matrix, shape (q_len, k_len)
# slopes: per-head slope, shape (heads, 1, 1)

scores = (q @ k.transpose(-2, -1)) / math.sqrt(d_k)
scores = scores - slopes * distance  # ALiBi bias
scores = scores + causal_mask         # keep causal masking when needed
attn = torch.softmax(scores, dim=-1)
out = attn @ v
```

---

## Comparison

| Method | Learning | Length Extrapolation | Relative Position | Key Models |
|--------|----------|---------------------|-------------------|------------|
| **Sinusoidal** | No | Yes | Indirect | Original Transformer |
| **Learnable** | Yes | No | No | BERT, ViT |
| **RoPE** | No | Yes | Yes (natural) | LLaMA, Qwen, Gemma |
| **ALiBi** | No | Yes | Yes (bias) | BLOOM |

### Which One to Use?

- **Vision (ViT)**: Learnable PE (image sizes tend to be fixed)
- **LLM (text)**: RoPE (better for variable-length text)
- **Learning/education**: Sinusoidal (easiest to understand)

---

## Summary

| Question | Answer |
|----------|--------|
| Why is it needed? | Attention is a set operation and doesn't know order |
| How is it solved? | Add a unique pattern to each position |
| Sinusoidal? | Fixed sin/cos pattern — no learning, any length |
| Learnable? | Optimized through training — used in ViT |
| RoPE? | Rotates Q/K by position — modern LLM standard |

## Practical Checklist

1. **Padding handling**: Even with positional encoding added, attention masks must still block padding positions.
2. **Length extrapolation**: If long contexts are common, RoPE/ALiBi is usually more stable than Learnable PE.
3. **Resolution change (ViT)**: When input resolution changes, apply position interpolation to reduce accuracy drops.
4. **Debug order**: For unstable training, check `mask direction → PE shape → max length overflow` first.
5. **KV-cache offset (inference)**: When using RoPE with KV cache, continue position indices from the current sequence length. A wrong offset can quickly break long-context coherence.
6. **RoPE base consistency (train/serve)**: Keep `rope_theta` (for example, 10000 or 500000) identical between training and inference. If the base differs, the same position index maps to a different rotation angle, often causing visible quality degradation.

## Related Content

- [Self-Attention](/en/docs/components/attention/self-attention) — Why Positional Encoding is needed
- [Cross-Attention](/en/docs/components/attention/cross-attention) — Connecting different sequences
- [ViT](/en/docs/architecture/transformer/vit) — Uses 2D Positional Encoding
