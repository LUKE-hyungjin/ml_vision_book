<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Detection on Vision Engineer 지식 가이드</title>
    <link>http://localhost:1313/docs/architecture/detection/</link>
    <description>Recent content in Detection on Vision Engineer 지식 가이드</description>
    <generator>Hugo</generator>
    <language>ko-KR</language>
    <atom:link href="http://localhost:1313/docs/architecture/detection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Faster R-CNN</title>
      <link>http://localhost:1313/docs/architecture/detection/faster-rcnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/architecture/detection/faster-rcnn/</guid>
      <description>&lt;h1 id=&#34;faster-r-cnn&#34;&gt;Faster R-CNN&lt;a class=&#34;anchor&#34; href=&#34;#faster-r-cnn&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;개요&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;논문&lt;/strong&gt;: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;저자&lt;/strong&gt;: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;핵심 기여&lt;/strong&gt;: Region Proposal Network(RPN)으로 end-to-end 학습 가능한 detector&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;r-cnn-계보&#34;&gt;R-CNN 계보&lt;a class=&#34;anchor&#34; href=&#34;#r-cnn-%ea%b3%84%eb%b3%b4&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;R-CNN (2014)&#xA;    ↓ ROI Pooling 도입&#xA;Fast R-CNN (2015)&#xA;    ↓ RPN 도입&#xA;Faster R-CNN (2015)&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;모델&lt;/th&gt;&#xA;          &lt;th&gt;Region Proposal&lt;/th&gt;&#xA;          &lt;th&gt;속도&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;R-CNN&lt;/td&gt;&#xA;          &lt;td&gt;Selective Search&lt;/td&gt;&#xA;          &lt;td&gt;~47s/image&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Fast R-CNN&lt;/td&gt;&#xA;          &lt;td&gt;Selective Search&lt;/td&gt;&#xA;          &lt;td&gt;~2s/image&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Faster R-CNN&lt;/td&gt;&#xA;          &lt;td&gt;RPN (학습됨)&lt;/td&gt;&#xA;          &lt;td&gt;~0.2s/image&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;구조&#34;&gt;구조&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b5%ac%ec%a1%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;전체-아키텍처&#34;&gt;전체 아키텍처&lt;a class=&#34;anchor&#34; href=&#34;#%ec%a0%84%ec%b2%b4-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Input Image&#xA;    ↓&#xA;Backbone (ResNet/VGG) → Feature Map&#xA;    ↓&#xA;┌───────────────────────────────────┐&#xA;│     Region Proposal Network       │&#xA;│  Feature Map → Anchors → RoIs     │&#xA;└───────────────────────────────────┘&#xA;    ↓&#xA;┌───────────────────────────────────┐&#xA;│        RoI Pooling/Align          │&#xA;│  다양한 크기 RoI → 고정 크기 특징   │&#xA;└───────────────────────────────────┘&#xA;    ↓&#xA;┌───────────────────────────────────┐&#xA;│    Detection Head (R-CNN Head)    │&#xA;│  Classification + Box Regression  │&#xA;└───────────────────────────────────┘&#xA;    ↓&#xA;Output: [class, x, y, w, h]&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;핵심-컴포넌트&#34;&gt;핵심 컴포넌트&lt;a class=&#34;anchor&#34; href=&#34;#%ed%95%b5%ec%8b%ac-%ec%bb%b4%ed%8f%ac%eb%84%8c%ed%8a%b8&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-backbone&#34;&gt;1. Backbone&lt;a class=&#34;anchor&#34; href=&#34;#1-backbone&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;이미지에서 특징 맵 추출:&lt;/p&gt;</description>
    </item>
    <item>
      <title>YOLO</title>
      <link>http://localhost:1313/docs/architecture/detection/yolo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/architecture/detection/yolo/</guid>
      <description>&lt;h1 id=&#34;yolo-you-only-look-once&#34;&gt;YOLO (You Only Look Once)&lt;a class=&#34;anchor&#34; href=&#34;#yolo-you-only-look-once&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;개요&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;논문&lt;/strong&gt;: You Only Look Once: Unified, Real-Time Object Detection (2016)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;저자&lt;/strong&gt;: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;핵심 기여&lt;/strong&gt;: Detection을 단일 회귀 문제로 재정의, 실시간 처리 가능&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;핵심-아이디어&#34;&gt;핵심 아이디어&lt;a class=&#34;anchor&#34; href=&#34;#%ed%95%b5%ec%8b%ac-%ec%95%84%ec%9d%b4%eb%94%94%ec%96%b4&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;blockquote class=&#39;book-hint &#39;&gt;&#xA;&lt;p&gt;&amp;ldquo;Detection을 한 번의 forward pass로 해결&amp;rdquo;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;p&gt;Two-stage detector와 달리, 이미지를 그리드로 나누고 각 그리드에서 직접 박스와 클래스를 예측합니다.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;yolo-버전-변화&#34;&gt;YOLO 버전 변화&lt;a class=&#34;anchor&#34; href=&#34;#yolo-%eb%b2%84%ec%a0%84-%eb%b3%80%ed%99%94&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;버전&lt;/th&gt;&#xA;          &lt;th&gt;연도&lt;/th&gt;&#xA;          &lt;th&gt;핵심 개선&lt;/th&gt;&#xA;          &lt;th&gt;mAP (COCO)&lt;/th&gt;&#xA;          &lt;th&gt;FPS&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv1&lt;/td&gt;&#xA;          &lt;td&gt;2016&lt;/td&gt;&#xA;          &lt;td&gt;첫 one-stage detector&lt;/td&gt;&#xA;          &lt;td&gt;63.4 (VOC)&lt;/td&gt;&#xA;          &lt;td&gt;45&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv2&lt;/td&gt;&#xA;          &lt;td&gt;2016&lt;/td&gt;&#xA;          &lt;td&gt;Batch Norm, Anchor&lt;/td&gt;&#xA;          &lt;td&gt;78.6 (VOC)&lt;/td&gt;&#xA;          &lt;td&gt;67&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv3&lt;/td&gt;&#xA;          &lt;td&gt;2018&lt;/td&gt;&#xA;          &lt;td&gt;Multi-scale, FPN&lt;/td&gt;&#xA;          &lt;td&gt;33.0&lt;/td&gt;&#xA;          &lt;td&gt;30&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv4&lt;/td&gt;&#xA;          &lt;td&gt;2020&lt;/td&gt;&#xA;          &lt;td&gt;CSPNet, Mish&lt;/td&gt;&#xA;          &lt;td&gt;43.5&lt;/td&gt;&#xA;          &lt;td&gt;65&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv5&lt;/td&gt;&#xA;          &lt;td&gt;2020&lt;/td&gt;&#xA;          &lt;td&gt;PyTorch, 편의성&lt;/td&gt;&#xA;          &lt;td&gt;50.7&lt;/td&gt;&#xA;          &lt;td&gt;140&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;YOLOv8&lt;/td&gt;&#xA;          &lt;td&gt;2023&lt;/td&gt;&#xA;          &lt;td&gt;Anchor-free, SOTA&lt;/td&gt;&#xA;          &lt;td&gt;53.9&lt;/td&gt;&#xA;          &lt;td&gt;100+&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;yolov1-구조&#34;&gt;YOLOv1 구조&lt;a class=&#34;anchor&#34; href=&#34;#yolov1-%ea%b5%ac%ec%a1%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;동작-원리&#34;&gt;동작 원리&lt;a class=&#34;anchor&#34; href=&#34;#%eb%8f%99%ec%9e%91-%ec%9b%90%eb%a6%ac&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1. 이미지를 S×S 그리드로 분할 (S=7)&#xA;2. 각 그리드 셀이 B개의 bounding box 예측 (B=2)&#xA;3. 각 박스: (x, y, w, h, confidence)&#xA;4. 각 그리드 셀이 C개 클래스 확률 예측 (C=20)&#xA;&#xA;Output: S × S × (B×5 + C) = 7 × 7 × 30&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;네트워크-구조&#34;&gt;네트워크 구조&lt;a class=&#34;anchor&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac-%ea%b5%ac%ec%a1%b0&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Input (448×448×3)&#xA;    ↓&#xA;24 Conv Layers (GoogLeNet inspired)&#xA;    ↓&#xA;2 FC Layers&#xA;    ↓&#xA;Output (7×7×30)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;confidence-score&#34;&gt;Confidence Score&lt;a class=&#34;anchor&#34; href=&#34;#confidence-score&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;$$\text{Confidence} = P(\text{Object}) \times \text{IoU}_{pred}^{truth}$$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
