<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="ViT (Vision Transformer)# 개요# 논문: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) 저자: Alexey Dosovitskiy et al. (Google) 핵심 기여: 이미지를 패치로 나누어 순수 Transformer로 처리 핵심 아이디어# “이미지를 단어처럼 취급하자”
이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.
구조# 전체 아키텍처# Input Image (224×224×3) ↓ Split into patches (14×14 = 196 patches of 16×16) ↓ Linear projection (flatten &#43; linear → D dim) ↓ &#43; Positional Embedding &#43; [CLS] token ↓ ┌─────────────────────────────┐ │ Transformer Encoder │ │ [Multi-Head Attn &#43; FFN]×L │ └─────────────────────────────┘ ↓ [CLS] token output ↓ MLP Head → Classification핵심 컴포넌트# 컴포넌트 설명 Patch Embedding 16×16×3 → D 차원 벡터 [CLS] Token 전체 이미지 표현 학습 Position Embedding 학습 가능한 위치 인코딩 Transformer Encoder L개 레이어 Patch Embedding# # 이미지를 패치로 분할 후 선형 변환 # (B, 3, 224, 224) → (B, 196, 768) class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super().__init__() self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): # (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim) x = self.proj(x) x = x.flatten(2).transpose(1, 2) return x 모델 변형# 모델 Layers Hidden Heads Params ViT-Base 12 768 12 86M ViT-Large 24 1024 16 307M ViT-Huge 32 1280 16 632M 학습# 사전학습의 중요성# ViT는 CNN과 달리 inductive bias가 적어서 대규모 데이터가 필수:
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/architecture/transformer/vit/">
  <meta property="og:site_name" content="Vision Engineer 지식 가이드">
  <meta property="og:title" content="ViT">
  <meta property="og:description" content="ViT (Vision Transformer)# 개요# 논문: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) 저자: Alexey Dosovitskiy et al. (Google) 핵심 기여: 이미지를 패치로 나누어 순수 Transformer로 처리 핵심 아이디어# “이미지를 단어처럼 취급하자”
이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.
구조# 전체 아키텍처# Input Image (224×224×3) ↓ Split into patches (14×14 = 196 patches of 16×16) ↓ Linear projection (flatten &#43; linear → D dim) ↓ &#43; Positional Embedding &#43; [CLS] token ↓ ┌─────────────────────────────┐ │ Transformer Encoder │ │ [Multi-Head Attn &#43; FFN]×L │ └─────────────────────────────┘ ↓ [CLS] token output ↓ MLP Head → Classification핵심 컴포넌트# 컴포넌트 설명 Patch Embedding 16×16×3 → D 차원 벡터 [CLS] Token 전체 이미지 표현 학습 Position Embedding 학습 가능한 위치 인코딩 Transformer Encoder L개 레이어 Patch Embedding# # 이미지를 패치로 분할 후 선형 변환 # (B, 3, 224, 224) → (B, 196, 768) class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super().__init__() self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): # (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim) x = self.proj(x) x = x.flatten(2).transpose(1, 2) return x 모델 변형# 모델 Layers Hidden Heads Params ViT-Base 12 768 12 86M ViT-Large 24 1024 16 307M ViT-Huge 32 1280 16 632M 학습# 사전학습의 중요성# ViT는 CNN과 달리 inductive bias가 적어서 대규모 데이터가 필수:">
  <meta property="og:locale" content="ko_KR">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">


  <meta itemprop="name" content="ViT">
  <meta itemprop="description" content="ViT (Vision Transformer)# 개요# 논문: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) 저자: Alexey Dosovitskiy et al. (Google) 핵심 기여: 이미지를 패치로 나누어 순수 Transformer로 처리 핵심 아이디어# “이미지를 단어처럼 취급하자”
이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.
구조# 전체 아키텍처# Input Image (224×224×3) ↓ Split into patches (14×14 = 196 patches of 16×16) ↓ Linear projection (flatten &#43; linear → D dim) ↓ &#43; Positional Embedding &#43; [CLS] token ↓ ┌─────────────────────────────┐ │ Transformer Encoder │ │ [Multi-Head Attn &#43; FFN]×L │ └─────────────────────────────┘ ↓ [CLS] token output ↓ MLP Head → Classification핵심 컴포넌트# 컴포넌트 설명 Patch Embedding 16×16×3 → D 차원 벡터 [CLS] Token 전체 이미지 표현 학습 Position Embedding 학습 가능한 위치 인코딩 Transformer Encoder L개 레이어 Patch Embedding# # 이미지를 패치로 분할 후 선형 변환 # (B, 3, 224, 224) → (B, 196, 768) class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super().__init__() self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): # (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim) x = self.proj(x) x = x.flatten(2).transpose(1, 2) return x 모델 변형# 모델 Layers Hidden Heads Params ViT-Base 12 768 12 86M ViT-Large 24 1024 16 307M ViT-Huge 32 1280 16 632M 학습# 사전학습의 중요성# ViT는 CNN과 달리 inductive bias가 적어서 대규모 데이터가 필수:">
  <meta itemprop="wordCount" content="536">

<title>ViT | Vision Engineer 지식 가이드</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/architecture/transformer/vit/">
<link rel="stylesheet" href="/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css" integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG&#43;T2l66Bw7pV8=" crossorigin="anonymous">


  <script defer src="/fuse.min.js"></script>
  <script defer src="/ko.search.min.26f28136458cea7ff1c69490935901eb5c29bceec55294f9efe562732e8a3b1a.js" integrity="sha256-JvKBNkWM6n/xxpSQk1kB61wpvO7FUpT57&#43;Vicy6KOxo=" crossorigin="anonymous"></script>



  
</head>
<body dir="ltr" class="book-kind-page book-type-docs">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Vision Engineer 지식 가이드</span>
  </a>
</h2>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  


  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button">
        <img src="/icons/translate.svg" class="book-icon" alt="Languages" />
        한국어
      </a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>

    <ul>
      
      <li>
        <a href="/en/" class="flex flex-auto">
          English
        </a>
      </li>
      
    </ul>
  </li>
</ul>












  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-14bcba7439f342435c91a57530ca56c8" class="toggle"  />
    <label for="section-14bcba7439f342435c91a57530ca56c8" class="flex">
      <a href="/docs/timeline/" class="">
        타임라인</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cb9657695dbd536c3a44d019c4d6ef98" class="toggle"  />
    <label for="section-cb9657695dbd536c3a44d019c4d6ef98" class="flex">
      <a href="/docs/topdown/" class="">
        Top-Down</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-e7f22db027b1c591ed84a4d4d3a6014f" class="toggle"  />
    <label for="section-e7f22db027b1c591ed84a4d4d3a6014f" class="flex">
      <a href="/docs/bottomup/" class="">
        Bottom-Up</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="toggle"  />
    <label for="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="flex">
      <a href="/docs/math/" class="">
        수학</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-32f2aee524dad19f5100855d6e2014e6" class="toggle" checked />
    <label for="section-32f2aee524dad19f5100855d6e2014e6" class="flex">
      <a href="/docs/architecture/" class="">
        아키텍처</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47444199c3019e8b479623bc90a9bd74" class="toggle"  />
    <label for="section-47444199c3019e8b479623bc90a9bd74" class="flex">
      <a href="/docs/architecture/classical/" class="">
        Classical CV</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/classical/sift-hog/" class="">
      SIFT &amp; HOG</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7bf44239f7ce2c72600738f8c315efe4" class="toggle"  />
    <label for="section-7bf44239f7ce2c72600738f8c315efe4" class="flex">
      <a href="/docs/architecture/cnn/" class="">
        CNN</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/cnn/alexnet/" class="">
      AlexNet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/cnn/vgg/" class="">
      VGG</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/cnn/resnet/" class="">
      ResNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-274b800b12326ef4e06255cf5f15c308" class="toggle"  />
    <label for="section-274b800b12326ef4e06255cf5f15c308" class="flex">
      <a href="/docs/architecture/detection/" class="">
        Detection</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/detection/faster-rcnn/" class="">
      Faster R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/detection/yolo/" class="">
      YOLO</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7246e67fd6c549f3bd6240d998b23a6f" class="toggle"  />
    <label for="section-7246e67fd6c549f3bd6240d998b23a6f" class="flex">
      <a href="/docs/architecture/segmentation/" class="">
        Segmentation</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/segmentation/unet/" class="">
      U-Net</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/segmentation/mask-rcnn/" class="">
      Mask R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/segmentation/sam/" class="">
      SAM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-3a9f4e0826e1b9e72550db30b2adcad7" class="toggle" checked />
    <label for="section-3a9f4e0826e1b9e72550db30b2adcad7" class="flex">
      <a href="/docs/architecture/transformer/" class="">
        Transformer</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/transformer/vit/" class="active">
      ViT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/transformer/dit/" class="">
      DiT</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2902afa0a62febfd7a5eb243df871e8c" class="toggle"  />
    <label for="section-2902afa0a62febfd7a5eb243df871e8c" class="flex">
      <a href="/docs/architecture/generative/" class="">
        Generative</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/generative/vae/" class="">
      VAE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/generative/gan/" class="">
      GAN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/generative/stable-diffusion/" class="">
      Stable Diffusion</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/generative/controlnet/" class="">
      ControlNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1670f136e85bf39bce97c9ee224d29d9" class="toggle"  />
    <label for="section-1670f136e85bf39bce97c9ee224d29d9" class="flex">
      <a href="/docs/architecture/multimodal/" class="">
        Multimodal</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/multimodal/clip/" class="">
      CLIP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/multimodal/vlm/" class="">
      VLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc1b0b4545ce791b47ab3fe55b854e56" class="toggle"  />
    <label for="section-fc1b0b4545ce791b47ab3fe55b854e56" class="flex">
      <a href="/docs/architecture/3d/" class="">
        3D Vision</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/3d/nerf/" class="">
      NeRF</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/architecture/3d/3dgs/" class="">
      3D Gaussian Splatting</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd575cc6fd4e25b684b552791b7a009a" class="toggle"  />
    <label for="section-cd575cc6fd4e25b684b552791b7a009a" class="flex">
      <a href="/docs/task/" class="">
        태스크</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fb690f19903e1435676f710c928c9ad4" class="toggle"  />
    <label for="section-fb690f19903e1435676f710c928c9ad4" class="flex">
      <a href="/docs/etc/" class="">
        기타 자료</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/icons/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>ViT</h3>

  <label for="toc-control">
    
    <img src="/icons/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어">핵심 아이디어</a></li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#전체-아키텍처">전체 아키텍처</a></li>
        <li><a href="#핵심-컴포넌트">핵심 컴포넌트</a></li>
      </ul>
    </li>
    <li><a href="#patch-embedding">Patch Embedding</a></li>
    <li><a href="#모델-변형">모델 변형</a></li>
    <li><a href="#학습">학습</a>
      <ul>
        <li><a href="#사전학습의-중요성">사전학습의 중요성</a></li>
        <li><a href="#학습-설정">학습 설정</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a>
      <ul>
        <li><a href="#huggingface-사용">HuggingFace 사용</a></li>
      </ul>
    </li>
    <li><a href="#vit의-특징">ViT의 특징</a>
      <ul>
        <li><a href="#attention-시각화">Attention 시각화</a></li>
        <li><a href="#장점">장점</a></li>
        <li><a href="#단점">단점</a></li>
      </ul>
    </li>
    <li><a href="#vit-변형">ViT 변형</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="vit-vision-transformer">ViT (Vision Transformer)<a class="anchor" href="#vit-vision-transformer">#</a></h1>
<h2 id="개요">개요<a class="anchor" href="#%ea%b0%9c%ec%9a%94">#</a></h2>
<ul>
<li><strong>논문</strong>: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)</li>
<li><strong>저자</strong>: Alexey Dosovitskiy et al. (Google)</li>
<li><strong>핵심 기여</strong>: 이미지를 패치로 나누어 순수 Transformer로 처리</li>
</ul>
<h2 id="핵심-아이디어">핵심 아이디어<a class="anchor" href="#%ed%95%b5%ec%8b%ac-%ec%95%84%ec%9d%b4%eb%94%94%ec%96%b4">#</a></h2>
<blockquote class='book-hint '>
<p>&ldquo;이미지를 단어처럼 취급하자&rdquo;</p>
</blockquote><p>이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.</p>
<hr>
<h2 id="구조">구조<a class="anchor" href="#%ea%b5%ac%ec%a1%b0">#</a></h2>
<h3 id="전체-아키텍처">전체 아키텍처<a class="anchor" href="#%ec%a0%84%ec%b2%b4-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98">#</a></h3>
<pre tabindex="0"><code>Input Image (224×224×3)
        ↓
Split into patches (14×14 = 196 patches of 16×16)
        ↓
Linear projection (flatten + linear → D dim)
        ↓
+ Positional Embedding + [CLS] token
        ↓
┌─────────────────────────────┐
│    Transformer Encoder      │
│  [Multi-Head Attn + FFN]×L │
└─────────────────────────────┘
        ↓
[CLS] token output
        ↓
MLP Head → Classification</code></pre><h3 id="핵심-컴포넌트">핵심 컴포넌트<a class="anchor" href="#%ed%95%b5%ec%8b%ac-%ec%bb%b4%ed%8f%ac%eb%84%8c%ed%8a%b8">#</a></h3>
<table>
  <thead>
      <tr>
          <th>컴포넌트</th>
          <th>설명</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Patch Embedding</td>
          <td>16×16×3 → D 차원 벡터</td>
      </tr>
      <tr>
          <td>[CLS] Token</td>
          <td>전체 이미지 표현 학습</td>
      </tr>
      <tr>
          <td>Position Embedding</td>
          <td>학습 가능한 위치 인코딩</td>
      </tr>
      <tr>
          <td>Transformer Encoder</td>
          <td>L개 레이어</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="patch-embedding">Patch Embedding<a class="anchor" href="#patch-embedding">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 이미지를 패치로 분할 후 선형 변환</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (B, 3, 224, 224) → (B, 196, 768)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PatchEmbedding</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, img_size<span style="color:#f92672">=</span><span style="color:#ae81ff">224</span>, patch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, embed_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_patches <span style="color:#f92672">=</span> (img_size <span style="color:#f92672">//</span> patch_size) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(in_channels, embed_dim,
</span></span><span style="display:flex;"><span>                              kernel_size<span style="color:#f92672">=</span>patch_size, stride<span style="color:#f92672">=</span>patch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proj(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x</span></span></code></pre></div><hr>
<h2 id="모델-변형">모델 변형<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%eb%b3%80%ed%98%95">#</a></h2>
<table>
  <thead>
      <tr>
          <th>모델</th>
          <th>Layers</th>
          <th>Hidden</th>
          <th>Heads</th>
          <th>Params</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ViT-Base</td>
          <td>12</td>
          <td>768</td>
          <td>12</td>
          <td>86M</td>
      </tr>
      <tr>
          <td>ViT-Large</td>
          <td>24</td>
          <td>1024</td>
          <td>16</td>
          <td>307M</td>
      </tr>
      <tr>
          <td>ViT-Huge</td>
          <td>32</td>
          <td>1280</td>
          <td>16</td>
          <td>632M</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="학습">학습<a class="anchor" href="#%ed%95%99%ec%8a%b5">#</a></h2>
<h3 id="사전학습의-중요성">사전학습의 중요성<a class="anchor" href="#%ec%82%ac%ec%a0%84%ed%95%99%ec%8a%b5%ec%9d%98-%ec%a4%91%ec%9a%94%ec%84%b1">#</a></h3>
<p>ViT는 CNN과 달리 inductive bias가 적어서 <strong>대규모 데이터</strong>가 필수:</p>
<table>
  <thead>
      <tr>
          <th>학습 데이터</th>
          <th>ImageNet Accuracy</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ImageNet-1k (1.2M)</td>
          <td>79.7% (ResNet보다 낮음)</td>
      </tr>
      <tr>
          <td>ImageNet-21k (14M)</td>
          <td>84.2%</td>
      </tr>
      <tr>
          <td>JFT-300M</td>
          <td><strong>88.5%</strong></td>
      </tr>
  </tbody>
</table>
<h3 id="학습-설정">학습 설정<a class="anchor" href="#%ed%95%99%ec%8a%b5-%ec%84%a4%ec%a0%95">#</a></h3>
<ul>
<li>Optimizer: Adam (β₁=0.9, β₂=0.999)</li>
<li>Learning rate: warmup + cosine decay</li>
<li>Augmentation: RandAugment, Mixup, CutMix</li>
<li>Regularization: Dropout, Stochastic Depth</li>
</ul>
<hr>
<h2 id="구현-예시">구현 예시<a class="anchor" href="#%ea%b5%ac%ed%98%84-%ec%98%88%ec%8b%9c">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ViT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        img_size<span style="color:#f92672">=</span><span style="color:#ae81ff">224</span>,
</span></span><span style="display:flex;"><span>        patch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>        in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>        num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>        embed_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>,
</span></span><span style="display:flex;"><span>        depth<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
</span></span><span style="display:flex;"><span>        num_heads<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
</span></span><span style="display:flex;"><span>        mlp_ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>        dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        num_patches <span style="color:#f92672">=</span> (img_size <span style="color:#f92672">//</span> patch_size) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Patch embedding</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>patch_embed <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(in_channels, embed_dim,
</span></span><span style="display:flex;"><span>                                      kernel_size<span style="color:#f92672">=</span>patch_size, stride<span style="color:#f92672">=</span>patch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># CLS token and position embedding</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cls_token <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, embed_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pos_embed <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, num_patches <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, embed_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Transformer encoder</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>blocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            TransformerBlock(embed_dim, num_heads, embed_dim <span style="color:#f92672">*</span> mlp_ratio, dropout)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(depth)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(embed_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Classification head</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embed_dim, num_classes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Patch embedding</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>patch_embed(x)<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Add CLS token</span>
</span></span><span style="display:flex;"><span>        cls_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>cls_token<span style="color:#f92672">.</span>expand(B, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([cls_tokens, x], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Add position embedding</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>pos_embed
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Transformer encoder</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>blocks:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Classification (CLS token만 사용)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>head(x[:, <span style="color:#ae81ff">0</span>])</span></span></code></pre></div><h3 id="huggingface-사용">HuggingFace 사용<a class="anchor" href="#huggingface-%ec%82%ac%ec%9a%a9">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> ViTForImageClassification, ViTImageProcessor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>processor <span style="color:#f92672">=</span> ViTImageProcessor<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;google/vit-base-patch16-224&#39;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> ViTForImageClassification<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;google/vit-base-patch16-224&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> processor(images<span style="color:#f92672">=</span>image, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs)
</span></span><span style="display:flex;"><span>predicted_class <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>logits<span style="color:#f92672">.</span>argmax(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()</span></span></code></pre></div><hr>
<h2 id="vit의-특징">ViT의 특징<a class="anchor" href="#vit%ec%9d%98-%ed%8a%b9%ec%a7%95">#</a></h2>
<h3 id="attention-시각화">Attention 시각화<a class="anchor" href="#attention-%ec%8b%9c%ea%b0%81%ed%99%94">#</a></h3>
<p>ViT는 이미지의 어떤 부분에 주목하는지 시각화 가능:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 마지막 레이어의 attention map</span>
</span></span><span style="display:flex;"><span>attentions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>get_last_selfattention(x)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># CLS token이 어디를 보는지 확인</span>
</span></span><span style="display:flex;"><span>cls_attention <span style="color:#f92672">=</span> attentions[:, :, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>:]  <span style="color:#75715e"># (B, heads, num_patches)</span></span></span></code></pre></div><h3 id="장점">장점<a class="anchor" href="#%ec%9e%a5%ec%a0%90">#</a></h3>
<ul>
<li>전역적 context 이해</li>
<li>확장성 우수 (모델/데이터 scale up 가능)</li>
<li>다양한 태스크 transfer 용이</li>
</ul>
<h3 id="단점">단점<a class="anchor" href="#%eb%8b%a8%ec%a0%90">#</a></h3>
<ul>
<li>대규모 데이터 필요</li>
<li>계산량 O(n²)</li>
<li>작은 객체/고해상도 처리 어려움</li>
</ul>
<hr>
<h2 id="vit-변형">ViT 변형<a class="anchor" href="#vit-%eb%b3%80%ed%98%95">#</a></h2>
<table>
  <thead>
      <tr>
          <th>모델</th>
          <th>핵심 개선</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>DeiT</strong></td>
          <td>Knowledge distillation으로 적은 데이터 학습</td>
      </tr>
      <tr>
          <td><strong>Swin</strong></td>
          <td>Shifted window로 효율적 계산</td>
      </tr>
      <tr>
          <td><strong>CvT</strong></td>
          <td>Conv 추가로 inductive bias 강화</td>
      </tr>
      <tr>
          <td><strong>BEiT</strong></td>
          <td>BERT 스타일 사전학습</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="관련-콘텐츠">관련 콘텐츠<a class="anchor" href="#%ea%b4%80%eb%a0%a8-%ec%bd%98%ed%85%90%ec%b8%a0">#</a></h2>
<ul>
<li><a href="/docs/architecture/transformer">Transformer</a> - 기반 아키텍처</li>
<li><a href="/docs/architecture/cnn/resnet">ResNet</a> - 대조되는 CNN 접근</li>
<li><a href="/docs/architecture/multimodal/clip">CLIP</a> - ViT를 image encoder로 사용</li>
<li><a href="/docs/architecture/transformer/dit">DiT</a> - Diffusion + Transformer</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/docs/architecture/transformer/" class="flex align-center">
        <img src="/icons/backward.svg" class="book-icon" alt="Backward" />
        <span>Transformer</span>
      </a>
    
    </span>
    <span>
    
      <a href="/docs/architecture/transformer/dit/" class="flex align-center">
        <span>DiT</span>
        <img src="/icons/forward.svg" class="book-icon" alt="Forward" />
      </a>
    
    </span>
  </div>
  


 
        
  
  <div class="book-comments">

</div>
  
 
        
        
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
  <aside class="book-toc">
    <div class="book-toc-content">
      
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어">핵심 아이디어</a></li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#전체-아키텍처">전체 아키텍처</a></li>
        <li><a href="#핵심-컴포넌트">핵심 컴포넌트</a></li>
      </ul>
    </li>
    <li><a href="#patch-embedding">Patch Embedding</a></li>
    <li><a href="#모델-변형">모델 변형</a></li>
    <li><a href="#학습">학습</a>
      <ul>
        <li><a href="#사전학습의-중요성">사전학습의 중요성</a></li>
        <li><a href="#학습-설정">학습 설정</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a>
      <ul>
        <li><a href="#huggingface-사용">HuggingFace 사용</a></li>
      </ul>
    </li>
    <li><a href="#vit의-특징">ViT의 특징</a>
      <ul>
        <li><a href="#attention-시각화">Attention 시각화</a></li>
        <li><a href="#장점">장점</a></li>
        <li><a href="#단점">단점</a></li>
      </ul>
    </li>
    <li><a href="#vit-변형">ViT 변형</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



    </div>
  </aside>
  
 
  </main>

  
</body>
</html>




















