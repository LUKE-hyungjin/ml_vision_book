<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimodal on Vision Engineer 지식 가이드</title>
    <link>http://localhost:1313/ko/docs/architecture/multimodal/</link>
    <description>Recent content in Multimodal on Vision Engineer 지식 가이드</description>
    <generator>Hugo</generator>
    <language>ko-KR</language>
    <atom:link href="http://localhost:1313/ko/docs/architecture/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CLIP</title>
      <link>http://localhost:1313/ko/docs/architecture/multimodal/clip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/ko/docs/architecture/multimodal/clip/</guid>
      <description>&lt;h1 id=&#34;clip-contrastive-language-image-pre-training&#34;&gt;CLIP (Contrastive Language-Image Pre-training)&lt;a class=&#34;anchor&#34; href=&#34;#clip-contrastive-language-image-pre-training&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;개요&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;논문&lt;/strong&gt;: Learning Transferable Visual Models From Natural Language Supervision (2021)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;저자&lt;/strong&gt;: Alec Radford et al. (OpenAI)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;핵심 기여&lt;/strong&gt;: 이미지와 텍스트를 같은 공간에 임베딩하여 zero-shot 분류&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;핵심-아이디어&#34;&gt;핵심 아이디어&lt;a class=&#34;anchor&#34; href=&#34;#%ed%95%b5%ec%8b%ac-%ec%95%84%ec%9d%b4%eb%94%94%ec%96%b4&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;blockquote class=&#39;book-hint &#39;&gt;&#xA;&lt;p&gt;&amp;ldquo;이미지와 텍스트 쌍을 대조 학습하여 공유 임베딩 공간 구축&amp;rdquo;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;p&gt;4억 개의 이미지-텍스트 쌍으로 학습하여, 한 번도 본 적 없는 클래스도 분류할 수 있습니다.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;구조&#34;&gt;구조&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b5%ac%ec%a1%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;전체-아키텍처&#34;&gt;전체 아키텍처&lt;a class=&#34;anchor&#34; href=&#34;#%ec%a0%84%ec%b2%b4-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;            ┌─────────────────┐&#xA;Image ────→ │  Image Encoder  │ ────→ Image Embedding (512-d)&#xA;            │  (ViT or ResNet)│                 ↓&#xA;            └─────────────────┘                 ↓ (cosine similarity)&#xA;                                                ↓&#xA;            ┌─────────────────┐                 ↓&#xA;Text  ────→ │   Text Encoder  │ ────→ Text Embedding (512-d)&#xA;            │  (Transformer)  │&#xA;            └─────────────────┘&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;인코더-옵션&#34;&gt;인코더 옵션&lt;a class=&#34;anchor&#34; href=&#34;#%ec%9d%b8%ec%bd%94%eb%8d%94-%ec%98%b5%ec%85%98&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Image Encoder&lt;/th&gt;&#xA;          &lt;th&gt;Text Encoder&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ViT-B/32, ViT-B/16, ViT-L/14&lt;/td&gt;&#xA;          &lt;td&gt;Transformer (12-layer)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ResNet-50, ResNet-101&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;contrastive-learning&#34;&gt;Contrastive Learning&lt;a class=&#34;anchor&#34; href=&#34;#contrastive-learning&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;학습-방식&#34;&gt;학습 방식&lt;a class=&#34;anchor&#34; href=&#34;#%ed%95%99%ec%8a%b5-%eb%b0%a9%ec%8b%9d&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;배치 내 N개의 이미지-텍스트 쌍:&lt;/p&gt;</description>
    </item>
    <item>
      <title>VLM</title>
      <link>http://localhost:1313/ko/docs/architecture/multimodal/vlm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/ko/docs/architecture/multimodal/vlm/</guid>
      <description>&lt;h1 id=&#34;vlm-vision-language-models&#34;&gt;VLM (Vision-Language Models)&lt;a class=&#34;anchor&#34; href=&#34;#vlm-vision-language-models&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;개요&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Vision-Language Model은 이미지와 텍스트를 함께 이해하고 생성하는 모델입니다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;발전-과정&#34;&gt;발전 과정&lt;a class=&#34;anchor&#34; href=&#34;#%eb%b0%9c%ec%a0%84-%ea%b3%bc%ec%a0%95&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CLIP (2021) → BLIP (2022) → BLIP-2 (2023) → LLaVA (2023) → GPT-4V (2023)&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;주요-모델들&#34;&gt;주요 모델들&lt;a class=&#34;anchor&#34; href=&#34;#%ec%a3%bc%ec%9a%94-%eb%aa%a8%eb%8d%b8%eb%93%a4&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-blip-bootstrapping-language-image-pre-training&#34;&gt;1. BLIP (Bootstrapping Language-Image Pre-training)&lt;a class=&#34;anchor&#34; href=&#34;#1-blip-bootstrapping-language-image-pre-training&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;개요-1&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94-1&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;논문&lt;/strong&gt;: BLIP: Bootstrapping Language-Image Pre-training (2022)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;저자&lt;/strong&gt;: Salesforce Research&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;핵심&lt;/strong&gt;: 캡션 생성과 필터링을 통한 데이터 품질 향상&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;구조&#34;&gt;구조&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b5%ac%ec%a1%b0&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;          Image ─────→ Image Encoder ─────┐&#xA;                                           ├──→ Multi-task Learning&#xA;          Text  ─────→ Text Encoder  ─────┘&#xA;&#xA;Tasks: ITC (Image-Text Contrastive)&#xA;       ITM (Image-Text Matching)&#xA;       LM  (Language Modeling)&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h3 id=&#34;2-blip-2&#34;&gt;2. BLIP-2&lt;a class=&#34;anchor&#34; href=&#34;#2-blip-2&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;개요-2&#34;&gt;개요&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b0%9c%ec%9a%94-2&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;논문&lt;/strong&gt;: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and LLMs (2023)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;저자&lt;/strong&gt;: Salesforce Research&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;핵심&lt;/strong&gt;: 사전학습된 Vision/Language 모델을 효율적으로 연결&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;구조-1&#34;&gt;구조&lt;a class=&#34;anchor&#34; href=&#34;#%ea%b5%ac%ec%a1%b0-1&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;┌─────────────────────────────────────────────────────────┐&#xA;│                       BLIP-2                             │&#xA;│                                                          │&#xA;│   Image ──→ [Frozen Image Encoder] ──→ Image Features   │&#xA;│                                              ↓           │&#xA;│                                    ┌─────────────────┐  │&#xA;│                                    │    Q-Former     │  │&#xA;│                                    │ (Learnable)     │  │&#xA;│                                    │ 32 Query Tokens │  │&#xA;│                                    └────────┬────────┘  │&#xA;│                                              ↓           │&#xA;│   Text ───────────────────────→ [Frozen LLM] ──→ Output │&#xA;│                                                          │&#xA;└─────────────────────────────────────────────────────────┘&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;q-former&#34;&gt;Q-Former&lt;a class=&#34;anchor&#34; href=&#34;#q-former&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;Vision과 Language 모델을 연결하는 경량 모듈:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
