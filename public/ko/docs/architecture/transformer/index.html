<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Transformer# 개요# 논문: Attention Is All You Need (2017) 저자: Vaswani et al. (Google) 핵심 기여: Self-attention으로 RNN 없이 시퀀스 처리 원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.
핵심 아이디어: Self-Attention# 모든 위치 간의 관계를 직접 모델링:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Q (Query): 현재 위치에서 찾고 싶은 것 K (Key): 다른 위치들의 특성 V (Value): 실제 정보 CNN과 비교# 측면 CNN Transformer 수용 영역 지역적 (kernel size) 전역적 (모든 위치) 관계 모델링 암시적 명시적 (attention) 위치 정보 구조에 내재 Positional encoding 필요 계산량 O(n) O(n²) 구조# Encoder-Decoder 구조# ┌─────────────────────┐ ┌─────────────────────┐ │ Encoder │ │ Decoder │ ├─────────────────────┤ ├─────────────────────┤ │ Multi-Head Attn │ │ Masked Multi-Head │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │────→│ Cross Attention │ │ ↓ │ │ ↓ │ │ Feed Forward │ │ Add &amp; Norm │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │ │ Feed Forward │ └─────────────────────┘ │ ↓ │ × N │ Add &amp; Norm │ └─────────────────────┘ × NMulti-Head Attention# 여러 관점에서 attention 수행:
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/ko/docs/architecture/transformer/">
  <meta property="og:site_name" content="Vision Engineer 지식 가이드">
  <meta property="og:title" content="Transformer">
  <meta property="og:description" content="Transformer# 개요# 논문: Attention Is All You Need (2017) 저자: Vaswani et al. (Google) 핵심 기여: Self-attention으로 RNN 없이 시퀀스 처리 원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.
핵심 아이디어: Self-Attention# 모든 위치 간의 관계를 직접 모델링:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Q (Query): 현재 위치에서 찾고 싶은 것 K (Key): 다른 위치들의 특성 V (Value): 실제 정보 CNN과 비교# 측면 CNN Transformer 수용 영역 지역적 (kernel size) 전역적 (모든 위치) 관계 모델링 암시적 명시적 (attention) 위치 정보 구조에 내재 Positional encoding 필요 계산량 O(n) O(n²) 구조# Encoder-Decoder 구조# ┌─────────────────────┐ ┌─────────────────────┐ │ Encoder │ │ Decoder │ ├─────────────────────┤ ├─────────────────────┤ │ Multi-Head Attn │ │ Masked Multi-Head │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │────→│ Cross Attention │ │ ↓ │ │ ↓ │ │ Feed Forward │ │ Add &amp; Norm │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │ │ Feed Forward │ └─────────────────────┘ │ ↓ │ × N │ Add &amp; Norm │ └─────────────────────┘ × NMulti-Head Attention# 여러 관점에서 attention 수행:">
  <meta property="og:locale" content="ko_KR">
  <meta property="og:type" content="website">


  <meta itemprop="name" content="Transformer">
  <meta itemprop="description" content="Transformer# 개요# 논문: Attention Is All You Need (2017) 저자: Vaswani et al. (Google) 핵심 기여: Self-attention으로 RNN 없이 시퀀스 처리 원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.
핵심 아이디어: Self-Attention# 모든 위치 간의 관계를 직접 모델링:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Q (Query): 현재 위치에서 찾고 싶은 것 K (Key): 다른 위치들의 특성 V (Value): 실제 정보 CNN과 비교# 측면 CNN Transformer 수용 영역 지역적 (kernel size) 전역적 (모든 위치) 관계 모델링 암시적 명시적 (attention) 위치 정보 구조에 내재 Positional encoding 필요 계산량 O(n) O(n²) 구조# Encoder-Decoder 구조# ┌─────────────────────┐ ┌─────────────────────┐ │ Encoder │ │ Decoder │ ├─────────────────────┤ ├─────────────────────┤ │ Multi-Head Attn │ │ Masked Multi-Head │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │────→│ Cross Attention │ │ ↓ │ │ ↓ │ │ Feed Forward │ │ Add &amp; Norm │ │ ↓ │ │ ↓ │ │ Add &amp; Norm │ │ Feed Forward │ └─────────────────────┘ │ ↓ │ × N │ Add &amp; Norm │ └─────────────────────┘ × NMulti-Head Attention# 여러 관점에서 attention 수행:">
  <meta itemprop="wordCount" content="484">

<title>Transformer | Vision Engineer 지식 가이드</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/ko/docs/architecture/transformer/">
<link rel="stylesheet" href="/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css" integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG&#43;T2l66Bw7pV8=" crossorigin="anonymous">


  <script defer src="/fuse.min.js"></script>
  <script defer src="/ko.search.min.e521ced8a1de353f186f5733d176d23d7236b1309fd3a485d880dd5743b5a505.js" integrity="sha256-5SHO2KHeNT8Yb1cz0XbSPXI2sTCf06SF2IDdV0O1pQU=" crossorigin="anonymous"></script>


<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/ko/docs/architecture/transformer/index.xml" title="Vision Engineer 지식 가이드" />

  <link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script><script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&#34;delimiters&#34;:[{&#34;left&#34;:&#34;$$&#34;,&#34;right&#34;:&#34;$$&#34;,&#34;display&#34;:true},{&#34;left&#34;:&#34;\\(&#34;,&#34;right&#34;:&#34;\\)&#34;,&#34;display&#34;:false},{&#34;left&#34;:&#34;\\[&#34;,&#34;right&#34;:&#34;\\]&#34;,&#34;display&#34;:true}]});"></script>
</head>
<body dir="ltr" class="book-kind-section book-type-docs">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/ko/"><span>Vision Engineer 지식 가이드</span>
  </a>
</h2>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  


  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button">
        <img src="/icons/translate.svg" class="book-icon" alt="Languages" />
        한국어
      </a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>

    <ul>
      
      <li>
        <a href="/" class="flex flex-auto">
          English
        </a>
      </li>
      
    </ul>
  </li>
</ul>












  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-14bcba7439f342435c91a57530ca56c8" class="toggle"  />
    <label for="section-14bcba7439f342435c91a57530ca56c8" class="flex">
      <a href="/ko/docs/timeline/" class="">
        타임라인</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cb9657695dbd536c3a44d019c4d6ef98" class="toggle"  />
    <label for="section-cb9657695dbd536c3a44d019c4d6ef98" class="flex">
      <a href="/ko/docs/topdown/" class="">
        Top-Down</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-e7f22db027b1c591ed84a4d4d3a6014f" class="toggle"  />
    <label for="section-e7f22db027b1c591ed84a4d4d3a6014f" class="flex">
      <a href="/ko/docs/bottomup/" class="">
        Bottom-Up</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="toggle"  />
    <label for="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="flex">
      <a href="/ko/docs/math/" class="">
        수학</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b7c22078b221fa1e5fe6dba5f036e317" class="toggle"  />
    <label for="section-b7c22078b221fa1e5fe6dba5f036e317" class="flex">
      <a href="/ko/docs/engineering/" class="">
        실무 기술</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="toggle"  />
    <label for="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="flex">
      <a href="/ko/docs/engineering/data/" class="">
        데이터</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/augmentation/" class="">
      데이터 증강</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/formats/" class="">
      데이터 포맷</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/pipeline/" class="">
      데이터 파이프라인</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/labeling/" class="">
      레이블링</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08b7417cd9477b9be27e6c7b561a01bc" class="toggle"  />
    <label for="section-08b7417cd9477b9be27e6c7b561a01bc" class="flex">
      <a href="/ko/docs/engineering/deployment/" class="">
        배포</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/onnx/" class="">
      ONNX</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/tensorrt/" class="">
      TensorRT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/serving/" class="">
      모델 서빙</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/optimization/" class="">
      모델 최적화</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="toggle"  />
    <label for="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="flex">
      <a href="/ko/docs/engineering/hardware/" class="">
        하드웨어</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/camera/" class="">
      카메라</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/lighting/" class="">
      조명</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/edge/" class="">
      엣지 디바이스</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-32f2aee524dad19f5100855d6e2014e6" class="toggle" checked />
    <label for="section-32f2aee524dad19f5100855d6e2014e6" class="flex">
      <a href="/ko/docs/architecture/" class="">
        아키텍처</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47444199c3019e8b479623bc90a9bd74" class="toggle"  />
    <label for="section-47444199c3019e8b479623bc90a9bd74" class="flex">
      <a href="/ko/docs/architecture/classical/" class="">
        Classical CV</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/classical/sift-hog/" class="">
      SIFT &amp; HOG</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7bf44239f7ce2c72600738f8c315efe4" class="toggle"  />
    <label for="section-7bf44239f7ce2c72600738f8c315efe4" class="flex">
      <a href="/ko/docs/architecture/cnn/" class="">
        CNN</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/alexnet/" class="">
      AlexNet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/vgg/" class="">
      VGG</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/resnet/" class="">
      ResNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-274b800b12326ef4e06255cf5f15c308" class="toggle"  />
    <label for="section-274b800b12326ef4e06255cf5f15c308" class="flex">
      <a href="/ko/docs/architecture/detection/" class="">
        Detection</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/faster-rcnn/" class="">
      Faster R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/yolo/" class="">
      YOLO</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7246e67fd6c549f3bd6240d998b23a6f" class="toggle"  />
    <label for="section-7246e67fd6c549f3bd6240d998b23a6f" class="flex">
      <a href="/ko/docs/architecture/segmentation/" class="">
        Segmentation</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/unet/" class="">
      U-Net</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/mask-rcnn/" class="">
      Mask R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/sam/" class="">
      SAM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-3a9f4e0826e1b9e72550db30b2adcad7" class="toggle" checked />
    <label for="section-3a9f4e0826e1b9e72550db30b2adcad7" class="flex">
      <a href="/ko/docs/architecture/transformer/" class="active">
        Transformer</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/vit/" class="">
      ViT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/dit/" class="">
      DiT</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2902afa0a62febfd7a5eb243df871e8c" class="toggle"  />
    <label for="section-2902afa0a62febfd7a5eb243df871e8c" class="flex">
      <a href="/ko/docs/architecture/generative/" class="">
        Generative</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/vae/" class="">
      VAE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/gan/" class="">
      GAN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/stable-diffusion/" class="">
      Stable Diffusion</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/controlnet/" class="">
      ControlNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1670f136e85bf39bce97c9ee224d29d9" class="toggle"  />
    <label for="section-1670f136e85bf39bce97c9ee224d29d9" class="flex">
      <a href="/ko/docs/architecture/multimodal/" class="">
        Multimodal</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/clip/" class="">
      CLIP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/vlm/" class="">
      VLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc1b0b4545ce791b47ab3fe55b854e56" class="toggle"  />
    <label for="section-fc1b0b4545ce791b47ab3fe55b854e56" class="flex">
      <a href="/ko/docs/architecture/3d/" class="">
        3D Vision</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/nerf/" class="">
      NeRF</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/3dgs/" class="">
      3D Gaussian Splatting</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd575cc6fd4e25b684b552791b7a009a" class="toggle"  />
    <label for="section-cd575cc6fd4e25b684b552791b7a009a" class="flex">
      <a href="/ko/docs/task/" class="">
        태스크</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fb690f19903e1435676f710c928c9ad4" class="toggle"  />
    <label for="section-fb690f19903e1435676f710c928c9ad4" class="flex">
      <a href="/ko/docs/etc/" class="">
        기타 자료</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/icons/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Transformer</h3>

  <label for="toc-control">
    
    <img src="/icons/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어-self-attention">핵심 아이디어: Self-Attention</a>
      <ul>
        <li><a href="#cnn과-비교">CNN과 비교</a></li>
      </ul>
    </li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#encoder-decoder-구조">Encoder-Decoder 구조</a></li>
        <li><a href="#multi-head-attention">Multi-Head Attention</a></li>
        <li><a href="#positional-encoding">Positional Encoding</a></li>
      </ul>
    </li>
    <li><a href="#vision에서의-transformer">Vision에서의 Transformer</a>
      <ul>
        <li><a href="#핵심-모델들">핵심 모델들</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a></li>
    <li><a href="#transformer의-영향">Transformer의 영향</a>
      <ul>
        <li><a href="#vision-분야">Vision 분야</a></li>
        <li><a href="#장점">장점</a></li>
        <li><a href="#단점">단점</a></li>
      </ul>
    </li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="transformer">Transformer<a class="anchor" href="#transformer">#</a></h1>
<h2 id="개요">개요<a class="anchor" href="#%ea%b0%9c%ec%9a%94">#</a></h2>
<ul>
<li><strong>논문</strong>: Attention Is All You Need (2017)</li>
<li><strong>저자</strong>: Vaswani et al. (Google)</li>
<li><strong>핵심 기여</strong>: Self-attention으로 RNN 없이 시퀀스 처리</li>
</ul>
<p>원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.</p>
<hr>
<h2 id="핵심-아이디어-self-attention">핵심 아이디어: Self-Attention<a class="anchor" href="#%ed%95%b5%ec%8b%ac-%ec%95%84%ec%9d%b4%eb%94%94%ec%96%b4-self-attention">#</a></h2>
<p>모든 위치 간의 관계를 직접 모델링:</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
<ul>
<li><strong>Q (Query)</strong>: 현재 위치에서 찾고 싶은 것</li>
<li><strong>K (Key)</strong>: 다른 위치들의 특성</li>
<li><strong>V (Value)</strong>: 실제 정보</li>
</ul>
<h3 id="cnn과-비교">CNN과 비교<a class="anchor" href="#cnn%ea%b3%bc-%eb%b9%84%ea%b5%90">#</a></h3>
<table>
  <thead>
      <tr>
          <th>측면</th>
          <th>CNN</th>
          <th>Transformer</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>수용 영역</td>
          <td>지역적 (kernel size)</td>
          <td>전역적 (모든 위치)</td>
      </tr>
      <tr>
          <td>관계 모델링</td>
          <td>암시적</td>
          <td>명시적 (attention)</td>
      </tr>
      <tr>
          <td>위치 정보</td>
          <td>구조에 내재</td>
          <td>Positional encoding 필요</td>
      </tr>
      <tr>
          <td>계산량</td>
          <td>O(n)</td>
          <td>O(n²)</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="구조">구조<a class="anchor" href="#%ea%b5%ac%ec%a1%b0">#</a></h2>
<h3 id="encoder-decoder-구조">Encoder-Decoder 구조<a class="anchor" href="#encoder-decoder-%ea%b5%ac%ec%a1%b0">#</a></h3>
<pre tabindex="0"><code>┌─────────────────────┐     ┌─────────────────────┐
│       Encoder       │     │       Decoder       │
├─────────────────────┤     ├─────────────────────┤
│  Multi-Head Attn    │     │  Masked Multi-Head  │
│         ↓           │     │         ↓           │
│  Add &amp; Norm         │────→│  Cross Attention    │
│         ↓           │     │         ↓           │
│  Feed Forward       │     │  Add &amp; Norm         │
│         ↓           │     │         ↓           │
│  Add &amp; Norm         │     │  Feed Forward       │
└─────────────────────┘     │         ↓           │
        × N                 │  Add &amp; Norm         │
                            └─────────────────────┘
                                    × N</code></pre><h3 id="multi-head-attention">Multi-Head Attention<a class="anchor" href="#multi-head-attention">#</a></h3>
<p>여러 관점에서 attention 수행:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 8개 head, 각 head는 64차원</span>
</span></span><span style="display:flex;"><span>MultiHead(Q, K, V) <span style="color:#f92672">=</span> Concat(head_1, <span style="color:#f92672">...</span>, head_8) <span style="color:#f92672">@</span> W_O
</span></span><span style="display:flex;"><span>where head_i <span style="color:#f92672">=</span> Attention(Q <span style="color:#f92672">@</span> W_Q_i, K <span style="color:#f92672">@</span> W_K_i, V <span style="color:#f92672">@</span> W_V_i)</span></span></code></pre></div><h3 id="positional-encoding">Positional Encoding<a class="anchor" href="#positional-encoding">#</a></h3>
<p>위치 정보 주입:</p>
<p>$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$</p>
<hr>
<h2 id="vision에서의-transformer">Vision에서의 Transformer<a class="anchor" href="#vision%ec%97%90%ec%84%9c%ec%9d%98-transformer">#</a></h2>
<h3 id="핵심-모델들">핵심 모델들<a class="anchor" href="#%ed%95%b5%ec%8b%ac-%eb%aa%a8%eb%8d%b8%eb%93%a4">#</a></h3>
<table>
  <thead>
      <tr>
          <th>모델</th>
          <th>연도</th>
          <th>입력 처리</th>
          <th>특징</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="/ko/docs/architecture/transformer/vit">ViT</a></td>
          <td>2020</td>
          <td>Image → Patches</td>
          <td>순수 Transformer</td>
      </tr>
      <tr>
          <td>DeiT</td>
          <td>2021</td>
          <td>Distillation</td>
          <td>효율적 학습</td>
      </tr>
      <tr>
          <td>Swin</td>
          <td>2021</td>
          <td>Shifted Windows</td>
          <td>계층적 구조</td>
      </tr>
      <tr>
          <td><a href="/ko/docs/architecture/transformer/dit">DiT</a></td>
          <td>2023</td>
          <td>Diffusion + Transformer</td>
          <td>이미지 생성</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="구현-예시">구현 예시<a class="anchor" href="#%ea%b5%ac%ed%98%84-%ec%98%88%ec%8b%9c">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, num_heads):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">=</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_k <span style="color:#f92672">=</span> d_model <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_o <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, Q, K, V, mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        batch_size <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Linear projections</span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_q(Q)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_k(K)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_v(V)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Attention</span>
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q, K<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>d_k)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(attn, V)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Concat and project</span>
</span></span><span style="display:flex;"><span>        context <span style="color:#f92672">=</span> context<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>d_k)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>W_o(context)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TransformerBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, d_model, num_heads, d_ff, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ffn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(d_model, d_ff),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>GELU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(d_ff, d_model),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Self-attention with residual</span>
</span></span><span style="display:flex;"><span>        attn_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attention(x, x, x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm1(x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>dropout(attn_out))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># FFN with residual</span>
</span></span><span style="display:flex;"><span>        ffn_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ffn(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm2(x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>dropout(ffn_out))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x</span></span></code></pre></div><hr>
<h2 id="transformer의-영향">Transformer의 영향<a class="anchor" href="#transformer%ec%9d%98-%ec%98%81%ed%96%a5">#</a></h2>
<h3 id="vision-분야">Vision 분야<a class="anchor" href="#vision-%eb%b6%84%ec%95%bc">#</a></h3>
<ul>
<li>Image Classification: ViT, DeiT, Swin</li>
<li>Object Detection: DETR, Deformable DETR</li>
<li>Segmentation: <a href="/ko/docs/architecture/segmentation/sam">SAM</a>, SegFormer</li>
<li>Generation: <a href="/ko/docs/architecture/transformer/dit">DiT</a></li>
</ul>
<h3 id="장점">장점<a class="anchor" href="#%ec%9e%a5%ec%a0%90">#</a></h3>
<ul>
<li>전역적 context 이해</li>
<li>확장성 (scaling law)</li>
<li>사전학습 효과적</li>
</ul>
<h3 id="단점">단점<a class="anchor" href="#%eb%8b%a8%ec%a0%90">#</a></h3>
<ul>
<li>O(n²) 계산량</li>
<li>많은 데이터 필요</li>
<li>위치 정보 취약</li>
</ul>
<hr>
<h2 id="관련-콘텐츠">관련 콘텐츠<a class="anchor" href="#%ea%b4%80%eb%a0%a8-%ec%bd%98%ed%85%90%ec%b8%a0">#</a></h2>
<ul>
<li><a href="/ko/docs/math/attention">Attention</a> - Attention 수식 상세</li>
<li><a href="/ko/docs/architecture/transformer/vit">ViT</a> - Vision Transformer</li>
<li><a href="/ko/docs/architecture/transformer/dit">DiT</a> - Diffusion Transformer</li>
<li><a href="/ko/docs/architecture/cnn">CNN 기초</a> - 대조되는 접근 방식</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/ko/docs/architecture/segmentation/sam/" class="flex align-center">
        <img src="/icons/backward.svg" class="book-icon" alt="Backward" />
        <span>SAM</span>
      </a>
    
    </span>
    <span>
    
      <a href="/ko/docs/architecture/transformer/vit/" class="flex align-center">
        <span>ViT</span>
        <img src="/icons/forward.svg" class="book-icon" alt="Forward" />
      </a>
    
    </span>
  </div>
  


 
        
  
  <div class="book-comments">

</div>
  
 
        
        
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
  <aside class="book-toc">
    <div class="book-toc-content">
      
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어-self-attention">핵심 아이디어: Self-Attention</a>
      <ul>
        <li><a href="#cnn과-비교">CNN과 비교</a></li>
      </ul>
    </li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#encoder-decoder-구조">Encoder-Decoder 구조</a></li>
        <li><a href="#multi-head-attention">Multi-Head Attention</a></li>
        <li><a href="#positional-encoding">Positional Encoding</a></li>
      </ul>
    </li>
    <li><a href="#vision에서의-transformer">Vision에서의 Transformer</a>
      <ul>
        <li><a href="#핵심-모델들">핵심 모델들</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a></li>
    <li><a href="#transformer의-영향">Transformer의 영향</a>
      <ul>
        <li><a href="#vision-분야">Vision 분야</a></li>
        <li><a href="#장점">장점</a></li>
        <li><a href="#단점">단점</a></li>
      </ul>
    </li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



    </div>
  </aside>
  
 
  </main>

  
</body>
</html>




















