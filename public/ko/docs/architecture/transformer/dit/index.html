<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="DiT (Diffusion Transformer)# 개요# 논문: Scalable Diffusion Models with Transformers (2023) 저자: William Peebles, Saining Xie (Meta AI, NYU) 핵심 기여: Diffusion 모델의 backbone을 U-Net에서 Transformer로 교체 핵심 아이디어# “U-Net 대신 Transformer를 쓰면 scaling이 더 잘 된다”
기존 Diffusion 모델(Stable Diffusion 등)은 U-Net을 사용하지만, DiT는 Transformer를 사용하여 더 나은 scaling 특성을 보여줍니다.
배경# 기존 Diffusion 모델의 backbone# Stable Diffusion: Latent → U-Net → Denoised Latent U-Net은 CNN 기반 Skip connection으로 세부 정보 보존 하지만 모델 크기 증가 시 효율 저하 DiT의 접근# Latent → Patchify → Transformer → Unpatchify → Denoised Latent ViT 스타일로 latent를 패치화 Transformer로 처리 모델 크기에 따라 일관된 성능 향상 구조# 전체 아키텍처# Input: Noisy Latent (z_t) &#43; Timestep (t) &#43; Class (c) ↓ Patchify (latent → patches) ↓ &#43; Position Embedding ↓ ┌─────────────────────────────────────┐ │ DiT Blocks × N │ │ ┌─────────────────────────────┐ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Multi-Head Self-Attn │ │ │ │ ↓ │ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Pointwise FFN │ │ │ │ ↓ │ │ │ │ &#43; AdaLN (t, c embed) │ │ │ └─────────────────────────────┘ │ └─────────────────────────────────────┘ ↓ Final Layer Norm ↓ Linear (predict noise &amp; variance) ↓ Unpatchify ↓ Output: Predicted Noise (ε) &#43; Variance (Σ)AdaLN (Adaptive Layer Norm)# 조건 정보(timestep, class)를 Layer Norm에 주입:
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/ko/docs/architecture/transformer/dit/">
  <meta property="og:site_name" content="Vision Engineer 지식 가이드">
  <meta property="og:title" content="DiT">
  <meta property="og:description" content="DiT (Diffusion Transformer)# 개요# 논문: Scalable Diffusion Models with Transformers (2023) 저자: William Peebles, Saining Xie (Meta AI, NYU) 핵심 기여: Diffusion 모델의 backbone을 U-Net에서 Transformer로 교체 핵심 아이디어# “U-Net 대신 Transformer를 쓰면 scaling이 더 잘 된다”
기존 Diffusion 모델(Stable Diffusion 등)은 U-Net을 사용하지만, DiT는 Transformer를 사용하여 더 나은 scaling 특성을 보여줍니다.
배경# 기존 Diffusion 모델의 backbone# Stable Diffusion: Latent → U-Net → Denoised Latent U-Net은 CNN 기반 Skip connection으로 세부 정보 보존 하지만 모델 크기 증가 시 효율 저하 DiT의 접근# Latent → Patchify → Transformer → Unpatchify → Denoised Latent ViT 스타일로 latent를 패치화 Transformer로 처리 모델 크기에 따라 일관된 성능 향상 구조# 전체 아키텍처# Input: Noisy Latent (z_t) &#43; Timestep (t) &#43; Class (c) ↓ Patchify (latent → patches) ↓ &#43; Position Embedding ↓ ┌─────────────────────────────────────┐ │ DiT Blocks × N │ │ ┌─────────────────────────────┐ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Multi-Head Self-Attn │ │ │ │ ↓ │ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Pointwise FFN │ │ │ │ ↓ │ │ │ │ &#43; AdaLN (t, c embed) │ │ │ └─────────────────────────────┘ │ └─────────────────────────────────────┘ ↓ Final Layer Norm ↓ Linear (predict noise &amp; variance) ↓ Unpatchify ↓ Output: Predicted Noise (ε) &#43; Variance (Σ)AdaLN (Adaptive Layer Norm)# 조건 정보(timestep, class)를 Layer Norm에 주입:">
  <meta property="og:locale" content="ko_KR">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">


  <meta itemprop="name" content="DiT">
  <meta itemprop="description" content="DiT (Diffusion Transformer)# 개요# 논문: Scalable Diffusion Models with Transformers (2023) 저자: William Peebles, Saining Xie (Meta AI, NYU) 핵심 기여: Diffusion 모델의 backbone을 U-Net에서 Transformer로 교체 핵심 아이디어# “U-Net 대신 Transformer를 쓰면 scaling이 더 잘 된다”
기존 Diffusion 모델(Stable Diffusion 등)은 U-Net을 사용하지만, DiT는 Transformer를 사용하여 더 나은 scaling 특성을 보여줍니다.
배경# 기존 Diffusion 모델의 backbone# Stable Diffusion: Latent → U-Net → Denoised Latent U-Net은 CNN 기반 Skip connection으로 세부 정보 보존 하지만 모델 크기 증가 시 효율 저하 DiT의 접근# Latent → Patchify → Transformer → Unpatchify → Denoised Latent ViT 스타일로 latent를 패치화 Transformer로 처리 모델 크기에 따라 일관된 성능 향상 구조# 전체 아키텍처# Input: Noisy Latent (z_t) &#43; Timestep (t) &#43; Class (c) ↓ Patchify (latent → patches) ↓ &#43; Position Embedding ↓ ┌─────────────────────────────────────┐ │ DiT Blocks × N │ │ ┌─────────────────────────────┐ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Multi-Head Self-Attn │ │ │ │ ↓ │ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Pointwise FFN │ │ │ │ ↓ │ │ │ │ &#43; AdaLN (t, c embed) │ │ │ └─────────────────────────────┘ │ └─────────────────────────────────────┘ ↓ Final Layer Norm ↓ Linear (predict noise &amp; variance) ↓ Unpatchify ↓ Output: Predicted Noise (ε) &#43; Variance (Σ)AdaLN (Adaptive Layer Norm)# 조건 정보(timestep, class)를 Layer Norm에 주입:">
  <meta itemprop="wordCount" content="695">

<title>DiT | Vision Engineer 지식 가이드</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/ko/docs/architecture/transformer/dit/">
<link rel="stylesheet" href="/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css" integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG&#43;T2l66Bw7pV8=" crossorigin="anonymous">


  <script defer src="/fuse.min.js"></script>
  <script defer src="/ko.search.min.e521ced8a1de353f186f5733d176d23d7236b1309fd3a485d880dd5743b5a505.js" integrity="sha256-5SHO2KHeNT8Yb1cz0XbSPXI2sTCf06SF2IDdV0O1pQU=" crossorigin="anonymous"></script>



  <link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script><script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&#34;delimiters&#34;:[{&#34;left&#34;:&#34;$$&#34;,&#34;right&#34;:&#34;$$&#34;,&#34;display&#34;:true},{&#34;left&#34;:&#34;\\(&#34;,&#34;right&#34;:&#34;\\)&#34;,&#34;display&#34;:false},{&#34;left&#34;:&#34;\\[&#34;,&#34;right&#34;:&#34;\\]&#34;,&#34;display&#34;:true}]});"></script>
</head>
<body dir="ltr" class="book-kind-page book-type-docs">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/ko/"><span>Vision Engineer 지식 가이드</span>
  </a>
</h2>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  


  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button">
        <img src="/icons/translate.svg" class="book-icon" alt="Languages" />
        한국어
      </a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>

    <ul>
      
      <li>
        <a href="/" class="flex flex-auto">
          English
        </a>
      </li>
      
    </ul>
  </li>
</ul>












  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-14bcba7439f342435c91a57530ca56c8" class="toggle"  />
    <label for="section-14bcba7439f342435c91a57530ca56c8" class="flex">
      <a href="/ko/docs/timeline/" class="">
        타임라인</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cb9657695dbd536c3a44d019c4d6ef98" class="toggle"  />
    <label for="section-cb9657695dbd536c3a44d019c4d6ef98" class="flex">
      <a href="/ko/docs/topdown/" class="">
        Top-Down</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-e7f22db027b1c591ed84a4d4d3a6014f" class="toggle"  />
    <label for="section-e7f22db027b1c591ed84a4d4d3a6014f" class="flex">
      <a href="/ko/docs/bottomup/" class="">
        Bottom-Up</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="toggle"  />
    <label for="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="flex">
      <a href="/ko/docs/math/" class="">
        수학</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b7c22078b221fa1e5fe6dba5f036e317" class="toggle"  />
    <label for="section-b7c22078b221fa1e5fe6dba5f036e317" class="flex">
      <a href="/ko/docs/engineering/" class="">
        실무 기술</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="toggle"  />
    <label for="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="flex">
      <a href="/ko/docs/engineering/data/" class="">
        데이터</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/augmentation/" class="">
      데이터 증강</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/formats/" class="">
      데이터 포맷</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/pipeline/" class="">
      데이터 파이프라인</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/labeling/" class="">
      레이블링</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08b7417cd9477b9be27e6c7b561a01bc" class="toggle"  />
    <label for="section-08b7417cd9477b9be27e6c7b561a01bc" class="flex">
      <a href="/ko/docs/engineering/deployment/" class="">
        배포</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/onnx/" class="">
      ONNX</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/tensorrt/" class="">
      TensorRT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/serving/" class="">
      모델 서빙</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/optimization/" class="">
      모델 최적화</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="toggle"  />
    <label for="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="flex">
      <a href="/ko/docs/engineering/hardware/" class="">
        하드웨어</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/camera/" class="">
      카메라</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/lighting/" class="">
      조명</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/edge/" class="">
      엣지 디바이스</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-32f2aee524dad19f5100855d6e2014e6" class="toggle" checked />
    <label for="section-32f2aee524dad19f5100855d6e2014e6" class="flex">
      <a href="/ko/docs/architecture/" class="">
        아키텍처</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47444199c3019e8b479623bc90a9bd74" class="toggle"  />
    <label for="section-47444199c3019e8b479623bc90a9bd74" class="flex">
      <a href="/ko/docs/architecture/classical/" class="">
        Classical CV</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/classical/sift-hog/" class="">
      SIFT &amp; HOG</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7bf44239f7ce2c72600738f8c315efe4" class="toggle"  />
    <label for="section-7bf44239f7ce2c72600738f8c315efe4" class="flex">
      <a href="/ko/docs/architecture/cnn/" class="">
        CNN</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/alexnet/" class="">
      AlexNet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/vgg/" class="">
      VGG</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/resnet/" class="">
      ResNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-274b800b12326ef4e06255cf5f15c308" class="toggle"  />
    <label for="section-274b800b12326ef4e06255cf5f15c308" class="flex">
      <a href="/ko/docs/architecture/detection/" class="">
        Detection</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/faster-rcnn/" class="">
      Faster R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/yolo/" class="">
      YOLO</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7246e67fd6c549f3bd6240d998b23a6f" class="toggle"  />
    <label for="section-7246e67fd6c549f3bd6240d998b23a6f" class="flex">
      <a href="/ko/docs/architecture/segmentation/" class="">
        Segmentation</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/unet/" class="">
      U-Net</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/mask-rcnn/" class="">
      Mask R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/sam/" class="">
      SAM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-3a9f4e0826e1b9e72550db30b2adcad7" class="toggle" checked />
    <label for="section-3a9f4e0826e1b9e72550db30b2adcad7" class="flex">
      <a href="/ko/docs/architecture/transformer/" class="">
        Transformer</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/vit/" class="">
      ViT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/dit/" class="active">
      DiT</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2902afa0a62febfd7a5eb243df871e8c" class="toggle"  />
    <label for="section-2902afa0a62febfd7a5eb243df871e8c" class="flex">
      <a href="/ko/docs/architecture/generative/" class="">
        Generative</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/vae/" class="">
      VAE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/gan/" class="">
      GAN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/stable-diffusion/" class="">
      Stable Diffusion</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/controlnet/" class="">
      ControlNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1670f136e85bf39bce97c9ee224d29d9" class="toggle"  />
    <label for="section-1670f136e85bf39bce97c9ee224d29d9" class="flex">
      <a href="/ko/docs/architecture/multimodal/" class="">
        Multimodal</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/clip/" class="">
      CLIP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/vlm/" class="">
      VLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc1b0b4545ce791b47ab3fe55b854e56" class="toggle"  />
    <label for="section-fc1b0b4545ce791b47ab3fe55b854e56" class="flex">
      <a href="/ko/docs/architecture/3d/" class="">
        3D Vision</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/nerf/" class="">
      NeRF</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/3dgs/" class="">
      3D Gaussian Splatting</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd575cc6fd4e25b684b552791b7a009a" class="toggle"  />
    <label for="section-cd575cc6fd4e25b684b552791b7a009a" class="flex">
      <a href="/ko/docs/task/" class="">
        태스크</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fb690f19903e1435676f710c928c9ad4" class="toggle"  />
    <label for="section-fb690f19903e1435676f710c928c9ad4" class="flex">
      <a href="/ko/docs/etc/" class="">
        기타 자료</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/icons/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>DiT</h3>

  <label for="toc-control">
    
    <img src="/icons/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어">핵심 아이디어</a></li>
    <li><a href="#배경">배경</a>
      <ul>
        <li><a href="#기존-diffusion-모델의-backbone">기존 Diffusion 모델의 backbone</a></li>
        <li><a href="#dit의-접근">DiT의 접근</a></li>
      </ul>
    </li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#전체-아키텍처">전체 아키텍처</a></li>
        <li><a href="#adaln-adaptive-layer-norm">AdaLN (Adaptive Layer Norm)</a></li>
      </ul>
    </li>
    <li><a href="#조건-주입-방식">조건 주입 방식</a>
      <ul>
        <li><a href="#세-가지-변형">세 가지 변형</a></li>
        <li><a href="#adaln-zero">AdaLN-Zero</a></li>
      </ul>
    </li>
    <li><a href="#모델-변형">모델 변형</a></li>
    <li><a href="#scaling-특성">Scaling 특성</a>
      <ul>
        <li><a href="#gflops-vs-fid">Gflops vs FID</a></li>
        <li><a href="#왜-transformer가-잘-scaling-되는가">왜 Transformer가 잘 scaling 되는가?</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a></li>
    <li><a href="#dit의-영향">DiT의 영향</a>
      <ul>
        <li><a href="#후속-모델들">후속 모델들</a></li>
        <li><a href="#왜-중요한가">왜 중요한가?</a></li>
      </ul>
    </li>
    <li><a href="#u-net-vs-dit">U-Net vs DiT</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="dit-diffusion-transformer">DiT (Diffusion Transformer)<a class="anchor" href="#dit-diffusion-transformer">#</a></h1>
<h2 id="개요">개요<a class="anchor" href="#%ea%b0%9c%ec%9a%94">#</a></h2>
<ul>
<li><strong>논문</strong>: Scalable Diffusion Models with Transformers (2023)</li>
<li><strong>저자</strong>: William Peebles, Saining Xie (Meta AI, NYU)</li>
<li><strong>핵심 기여</strong>: Diffusion 모델의 backbone을 U-Net에서 Transformer로 교체</li>
</ul>
<h2 id="핵심-아이디어">핵심 아이디어<a class="anchor" href="#%ed%95%b5%ec%8b%ac-%ec%95%84%ec%9d%b4%eb%94%94%ec%96%b4">#</a></h2>
<blockquote class='book-hint '>
<p>&ldquo;U-Net 대신 Transformer를 쓰면 scaling이 더 잘 된다&rdquo;</p>
</blockquote><p>기존 Diffusion 모델(Stable Diffusion 등)은 U-Net을 사용하지만, DiT는 Transformer를 사용하여 더 나은 scaling 특성을 보여줍니다.</p>
<hr>
<h2 id="배경">배경<a class="anchor" href="#%eb%b0%b0%ea%b2%bd">#</a></h2>
<h3 id="기존-diffusion-모델의-backbone">기존 Diffusion 모델의 backbone<a class="anchor" href="#%ea%b8%b0%ec%a1%b4-diffusion-%eb%aa%a8%eb%8d%b8%ec%9d%98-backbone">#</a></h3>
<pre tabindex="0"><code>Stable Diffusion: Latent → U-Net → Denoised Latent</code></pre><ul>
<li>U-Net은 CNN 기반</li>
<li>Skip connection으로 세부 정보 보존</li>
<li>하지만 모델 크기 증가 시 효율 저하</li>
</ul>
<h3 id="dit의-접근">DiT의 접근<a class="anchor" href="#dit%ec%9d%98-%ec%a0%91%ea%b7%bc">#</a></h3>
<pre tabindex="0"><code>Latent → Patchify → Transformer → Unpatchify → Denoised Latent</code></pre><ul>
<li>ViT 스타일로 latent를 패치화</li>
<li>Transformer로 처리</li>
<li>모델 크기에 따라 일관된 성능 향상</li>
</ul>
<hr>
<h2 id="구조">구조<a class="anchor" href="#%ea%b5%ac%ec%a1%b0">#</a></h2>
<h3 id="전체-아키텍처">전체 아키텍처<a class="anchor" href="#%ec%a0%84%ec%b2%b4-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98">#</a></h3>
<pre tabindex="0"><code>Input: Noisy Latent (z_t) + Timestep (t) + Class (c)
            ↓
    Patchify (latent → patches)
            ↓
    + Position Embedding
            ↓
┌─────────────────────────────────────┐
│         DiT Blocks × N             │
│  ┌─────────────────────────────┐   │
│  │     Layer Norm              │   │
│  │           ↓                 │   │
│  │    Multi-Head Self-Attn    │   │
│  │           ↓                 │   │
│  │     Layer Norm              │   │
│  │           ↓                 │   │
│  │    Pointwise FFN           │   │
│  │           ↓                 │   │
│  │    + AdaLN (t, c embed)    │   │
│  └─────────────────────────────┘   │
└─────────────────────────────────────┘
            ↓
    Final Layer Norm
            ↓
    Linear (predict noise &amp; variance)
            ↓
    Unpatchify
            ↓
Output: Predicted Noise (ε) + Variance (Σ)</code></pre><h3 id="adaln-adaptive-layer-norm">AdaLN (Adaptive Layer Norm)<a class="anchor" href="#adaln-adaptive-layer-norm">#</a></h3>
<p>조건 정보(timestep, class)를 Layer Norm에 주입:</p>
<p>$$\text{AdaLN}(h, y) = y_s \odot \text{LayerNorm}(h) + y_b$$</p>
<p>여기서 $y_s$, $y_b$는 timestep과 class embedding에서 예측된 scale, shift 값</p>
<hr>
<h2 id="조건-주입-방식">조건 주입 방식<a class="anchor" href="#%ec%a1%b0%ea%b1%b4-%ec%a3%bc%ec%9e%85-%eb%b0%a9%ec%8b%9d">#</a></h2>
<h3 id="세-가지-변형">세 가지 변형<a class="anchor" href="#%ec%84%b8-%ea%b0%80%ec%a7%80-%eb%b3%80%ed%98%95">#</a></h3>
<table>
  <thead>
      <tr>
          <th>방식</th>
          <th>설명</th>
          <th>성능</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>In-context</td>
          <td>조건을 추가 토큰으로</td>
          <td>중간</td>
      </tr>
      <tr>
          <td>Cross-attention</td>
          <td>조건에 cross-attention</td>
          <td>중간</td>
      </tr>
      <tr>
          <td><strong>AdaLN-Zero</strong></td>
          <td>Adaptive LN + zero 초기화</td>
          <td><strong>최고</strong></td>
      </tr>
  </tbody>
</table>
<h3 id="adaln-zero">AdaLN-Zero<a class="anchor" href="#adaln-zero">#</a></h3>
<p>각 블록의 출력을 0으로 초기화:</p>
<ul>
<li>학습 초기에 identity mapping</li>
<li>안정적인 학습</li>
</ul>
<hr>
<h2 id="모델-변형">모델 변형<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%eb%b3%80%ed%98%95">#</a></h2>
<table>
  <thead>
      <tr>
          <th>모델</th>
          <th>Layers</th>
          <th>Hidden</th>
          <th>Heads</th>
          <th>Params</th>
          <th>FID-50K</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>DiT-S/2</td>
          <td>12</td>
          <td>384</td>
          <td>6</td>
          <td>33M</td>
          <td>68.4</td>
      </tr>
      <tr>
          <td>DiT-B/2</td>
          <td>12</td>
          <td>768</td>
          <td>12</td>
          <td>130M</td>
          <td>43.5</td>
      </tr>
      <tr>
          <td>DiT-L/2</td>
          <td>24</td>
          <td>1024</td>
          <td>16</td>
          <td>458M</td>
          <td>23.3</td>
      </tr>
      <tr>
          <td>DiT-XL/2</td>
          <td>28</td>
          <td>1152</td>
          <td>16</td>
          <td>675M</td>
          <td><strong>2.27</strong></td>
      </tr>
  </tbody>
</table>
<ul>
<li><code>/2</code>는 patch size 2 의미 (latent space에서)</li>
<li>모델 크기 증가에 따른 일관된 성능 향상</li>
</ul>
<hr>
<h2 id="scaling-특성">Scaling 특성<a class="anchor" href="#scaling-%ed%8a%b9%ec%84%b1">#</a></h2>
<h3 id="gflops-vs-fid">Gflops vs FID<a class="anchor" href="#gflops-vs-fid">#</a></h3>
<p>DiT는 compute가 증가할수록 일관되게 FID가 개선:</p>
<pre tabindex="0"><code>Gflops:  10 → 100 → 1000
FID:     68 →  43 →  2.27</code></pre><p>이는 LLM에서 관찰되는 scaling law와 유사합니다.</p>
<h3 id="왜-transformer가-잘-scaling-되는가">왜 Transformer가 잘 scaling 되는가?<a class="anchor" href="#%ec%99%9c-transformer%ea%b0%80-%ec%9e%98-scaling-%eb%90%98%eb%8a%94%ea%b0%80">#</a></h3>
<ol>
<li><strong>Attention의 유연성</strong>: 전역적 패턴 학습</li>
<li><strong>표준화된 구조</strong>: 최적화 기법 적용 용이</li>
<li><strong>병렬화</strong>: 효율적인 학습</li>
</ol>
<hr>
<h2 id="구현-예시">구현 예시<a class="anchor" href="#%ea%b5%ac%ed%98%84-%ec%98%88%ec%8b%9c">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DiTBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, hidden_size, num_heads, mlp_ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">4.0</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_size, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MultiheadAttention(hidden_size, num_heads, batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_size, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mlp <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_size, int(hidden_size <span style="color:#f92672">*</span> mlp_ratio)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>GELU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(int(hidden_size <span style="color:#f92672">*</span> mlp_ratio), hidden_size),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># AdaLN parameters</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>adaLN_modulation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_size, <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> hidden_size)  <span style="color:#75715e"># scale, shift for both norms + gate</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, c):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># c: condition embedding (timestep + class)</span>
</span></span><span style="display:flex;"><span>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp <span style="color:#f92672">=</span> \
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>adaLN_modulation(c)<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">6</span>, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Self-attention with AdaLN</span>
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm1(x)
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> scale_msa<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)) <span style="color:#f92672">+</span> shift_msa<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        h, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn(h, h, h)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> gate_msa<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> h
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># FFN with AdaLN</span>
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm2(x)
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> h <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> scale_mlp<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)) <span style="color:#f92672">+</span> shift_mlp<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mlp(h)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> gate_mlp<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> h
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DiT</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, input_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, patch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1152</span>,
</span></span><span style="display:flex;"><span>                 depth<span style="color:#f92672">=</span><span style="color:#ae81ff">28</span>, num_heads<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_patches <span style="color:#f92672">=</span> (input_size <span style="color:#f92672">//</span> patch_size) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Patch embedding</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>x_embedder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">4</span>, hidden_size, patch_size, patch_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pos_embed <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_patches, hidden_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Condition embedding</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>t_embedder <span style="color:#f92672">=</span> TimestepEmbedder(hidden_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>y_embedder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(num_classes, hidden_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># DiT blocks</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>blocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            DiTBlock(hidden_size, num_heads) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(depth)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Output</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>final_layer <span style="color:#f92672">=</span> FinalLayer(hidden_size, patch_size, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, t, y):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Patchify</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>x_embedder(x)<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>pos_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Condition</span>
</span></span><span style="display:flex;"><span>        c <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>t_embedder(t) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>y_embedder(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># DiT blocks</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> block <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>blocks:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block(x, c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Unpatchify</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>final_layer(x, c)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x</span></span></code></pre></div><hr>
<h2 id="dit의-영향">DiT의 영향<a class="anchor" href="#dit%ec%9d%98-%ec%98%81%ed%96%a5">#</a></h2>
<h3 id="후속-모델들">후속 모델들<a class="anchor" href="#%ed%9b%84%ec%86%8d-%eb%aa%a8%eb%8d%b8%eb%93%a4">#</a></h3>
<ul>
<li><strong>Sora (OpenAI)</strong>: DiT 기반 비디오 생성</li>
<li><strong>Stable Diffusion 3</strong>: DiT 아키텍처 채택</li>
<li><strong>PixArt-α</strong>: 효율적인 DiT 학습</li>
</ul>
<h3 id="왜-중요한가">왜 중요한가?<a class="anchor" href="#%ec%99%9c-%ec%a4%91%ec%9a%94%ed%95%9c%ea%b0%80">#</a></h3>
<ol>
<li>Diffusion 모델의 새로운 방향 제시</li>
<li>Scaling law 확인</li>
<li>Transformer의 범용성 증명</li>
</ol>
<hr>
<h2 id="u-net-vs-dit">U-Net vs DiT<a class="anchor" href="#u-net-vs-dit">#</a></h2>
<table>
  <thead>
      <tr>
          <th>측면</th>
          <th>U-Net</th>
          <th>DiT</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>구조</td>
          <td>CNN + Skip</td>
          <td>Transformer</td>
      </tr>
      <tr>
          <td>Scaling</td>
          <td>제한적</td>
          <td>일관적 향상</td>
      </tr>
      <tr>
          <td>계산량</td>
          <td>상대적 효율</td>
          <td>더 많은 compute</td>
      </tr>
      <tr>
          <td>구현 복잡도</td>
          <td>복잡</td>
          <td>단순</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="관련-콘텐츠">관련 콘텐츠<a class="anchor" href="#%ea%b4%80%eb%a0%a8-%ec%bd%98%ed%85%90%ec%b8%a0">#</a></h2>
<ul>
<li><a href="/ko/docs/architecture/transformer">Transformer</a> - 기반 아키텍처</li>
<li><a href="/ko/docs/architecture/transformer/vit">ViT</a> - 이미지 패치화 아이디어</li>
<li><a href="/ko/docs/architecture/generative/stable-diffusion">Stable Diffusion</a> - U-Net 기반 Diffusion</li>
<li><a href="/ko/docs/math/diffusion-process">Diffusion Process</a> - Diffusion 수식</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/ko/docs/architecture/transformer/vit/" class="flex align-center">
        <img src="/icons/backward.svg" class="book-icon" alt="Backward" />
        <span>ViT</span>
      </a>
    
    </span>
    <span>
    
      <a href="/ko/docs/architecture/generative/" class="flex align-center">
        <span>Generative</span>
        <img src="/icons/forward.svg" class="book-icon" alt="Forward" />
      </a>
    
    </span>
  </div>
  


 
        
  
  <div class="book-comments">

</div>
  
 
        
        
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
  <aside class="book-toc">
    <div class="book-toc-content">
      
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#핵심-아이디어">핵심 아이디어</a></li>
    <li><a href="#배경">배경</a>
      <ul>
        <li><a href="#기존-diffusion-모델의-backbone">기존 Diffusion 모델의 backbone</a></li>
        <li><a href="#dit의-접근">DiT의 접근</a></li>
      </ul>
    </li>
    <li><a href="#구조">구조</a>
      <ul>
        <li><a href="#전체-아키텍처">전체 아키텍처</a></li>
        <li><a href="#adaln-adaptive-layer-norm">AdaLN (Adaptive Layer Norm)</a></li>
      </ul>
    </li>
    <li><a href="#조건-주입-방식">조건 주입 방식</a>
      <ul>
        <li><a href="#세-가지-변형">세 가지 변형</a></li>
        <li><a href="#adaln-zero">AdaLN-Zero</a></li>
      </ul>
    </li>
    <li><a href="#모델-변형">모델 변형</a></li>
    <li><a href="#scaling-특성">Scaling 특성</a>
      <ul>
        <li><a href="#gflops-vs-fid">Gflops vs FID</a></li>
        <li><a href="#왜-transformer가-잘-scaling-되는가">왜 Transformer가 잘 scaling 되는가?</a></li>
      </ul>
    </li>
    <li><a href="#구현-예시">구현 예시</a></li>
    <li><a href="#dit의-영향">DiT의 영향</a>
      <ul>
        <li><a href="#후속-모델들">후속 모델들</a></li>
        <li><a href="#왜-중요한가">왜 중요한가?</a></li>
      </ul>
    </li>
    <li><a href="#u-net-vs-dit">U-Net vs DiT</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



    </div>
  </aside>
  
 
  </main>

  
</body>
</html>




















