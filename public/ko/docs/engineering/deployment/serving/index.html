<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="모델 서빙# 개요# 학습된 모델을 API로 제공하는 인프라 구축입니다.
서빙 옵션# 도구 특징 적합한 경우 FastAPI 간단, Python 친화적 프로토타입, 소규모 Triton 고성능, 다중 모델 대규모 프로덕션 TorchServe PyTorch 통합 PyTorch 중심 TensorFlow Serving TF 최적화 TensorFlow 중심 BentoML ML 특화 빠른 배포 FastAPI 서빙# 기본 구조# from fastapi import FastAPI, File, UploadFile from PIL import Image import torch import io app = FastAPI() # 모델 로드 (시작 시 한 번) model = torch.load(&#34;model.pt&#34;) model.eval() @app.post(&#34;/predict&#34;) async def predict(file: UploadFile = File(...)): # 이미지 로드 image_bytes = await file.read() image = Image.open(io.BytesIO(image_bytes)) # 전처리 input_tensor = preprocess(image) # 추론 with torch.no_grad(): output = model(input_tensor) # 후처리 result = postprocess(output) return {&#34;prediction&#34;: result} # 실행: uvicorn main:app --host 0.0.0.0 --port 8000배치 처리# from fastapi import BackgroundTasks import asyncio from collections import deque batch_queue = deque() batch_size = 32 batch_timeout = 0.1 # 100ms async def batch_processor(): while True: if len(batch_queue) &gt;= batch_size or \ (len(batch_queue) &gt; 0 and time_since_first &gt; batch_timeout): # 배치 처리 batch = [batch_queue.popleft() for _ in range(min(len(batch_queue), batch_size))] inputs = torch.stack([item[&#39;input&#39;] for item in batch]) outputs = model(inputs) for item, output in zip(batch, outputs): item[&#39;future&#39;].set_result(output) await asyncio.sleep(0.01) @app.on_event(&#34;startup&#34;) async def startup(): asyncio.create_task(batch_processor()) Triton Inference Server# 모델 저장소# model_repository/ ├── resnet/ │ ├── config.pbtxt │ └── 1/ │ └── model.onnx └── yolo/ ├── config.pbtxt └── 1/ └── model.planconfig.pbtxt# name: &#34;resnet&#34; platform: &#34;onnxruntime_onnx&#34; max_batch_size: 32 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] dynamic_batching { preferred_batch_size: [8, 16, 32] max_queue_delay_microseconds: 100000 } instance_group [ { count: 2, kind: KIND_GPU } ]서버 실행# docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \ -v $(pwd)/model_repository:/models \ nvcr.io/nvidia/tritonserver:23.10-py3 \ tritonserver --model-repository=/models클라이언트# import tritonclient.http as httpclient import numpy as np client = httpclient.InferenceServerClient(url=&#34;localhost:8000&#34;) # 입력 준비 input_data = np.random.randn(1, 3, 224, 224).astype(np.float32) inputs = [httpclient.InferInput(&#34;input&#34;, input_data.shape, &#34;FP32&#34;)] inputs[0].set_data_from_numpy(input_data) # 추론 outputs = [httpclient.InferRequestedOutput(&#34;output&#34;)] result = client.infer(&#34;resnet&#34;, inputs, outputs=outputs) output_data = result.as_numpy(&#34;output&#34;) TorchServe# 모델 아카이브 생성# # handler.py 작성 torch-model-archiver --model-name resnet \ --version 1.0 \ --model-file model.py \ --serialized-file model.pt \ --handler image_classifier \ --export-path model_storehandler.py# from ts.torch_handler.vision_handler import VisionHandler class MyHandler(VisionHandler): def preprocess(self, data): images = [] for row in data: image = row.get(&#34;data&#34;) or row.get(&#34;body&#34;) image = Image.open(io.BytesIO(image)) image = self.transform(image) images.append(image) return torch.stack(images) def postprocess(self, output): probs = torch.softmax(output, dim=1) return probs.tolist()서버 실행# torchserve --start --model-store model_store --models resnet=resnet.mar BentoML# 모델 저장# import bentoml # 모델 저장 bentoml.pytorch.save_model(&#34;resnet&#34;, model)서비스 정의# # service.py import bentoml from bentoml.io import Image, JSON runner = bentoml.pytorch.get(&#34;resnet:latest&#34;).to_runner() svc = bentoml.Service(&#34;image_classifier&#34;, runners=[runner]) @svc.api(input=Image(), output=JSON()) async def predict(image): input_tensor = preprocess(image) result = await runner.async_run(input_tensor) return postprocess(result)빌드 및 배포# bentoml build bentoml serve service:svc 성능 최적화# 동시성# # FastAPI with uvicorn workers uvicorn main:app --workers 4 --host 0.0.0.0 --port 8000GPU 메모리 관리# # 모델별 GPU 할당 import os os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;0&#34; # 또는 Triton에서 instance_group 설정모니터링# from prometheus_client import Counter, Histogram, start_http_server REQUEST_COUNT = Counter(&#39;requests_total&#39;, &#39;Total requests&#39;) REQUEST_LATENCY = Histogram(&#39;request_latency_seconds&#39;, &#39;Request latency&#39;) @app.middleware(&#34;http&#34;) async def add_metrics(request, call_next): REQUEST_COUNT.inc() with REQUEST_LATENCY.time(): response = await call_next(request) return response Docker 배포# Dockerfile# FROM python:3.10-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [&#34;uvicorn&#34;, &#34;main:app&#34;, &#34;--host&#34;, &#34;0.0.0.0&#34;, &#34;--port&#34;, &#34;8000&#34;]docker-compose.yml# version: &#39;3.8&#39; services: api: build: . ports: - &#34;8000:8000&#34; deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] Kubernetes 배포# apiVersion: apps/v1 kind: Deployment metadata: name: model-server spec: replicas: 3 selector: matchLabels: app: model-server template: spec: containers: - name: model-server image: my-model-server:latest ports: - containerPort: 8000 resources: limits: nvidia.com/gpu: 1 --- apiVersion: v1 kind: Service metadata: name: model-server spec: selector: app: model-server ports: - port: 80 targetPort: 8000 type: LoadBalancer 관련 콘텐츠# TensorRT - GPU 최적화 ONNX - 모델 변환 최적화 - 모델 경량화 ">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/ko/docs/engineering/deployment/serving/">
  <meta property="og:site_name" content="Vision Engineer 지식 가이드">
  <meta property="og:title" content="모델 서빙">
  <meta property="og:description" content="모델 서빙# 개요# 학습된 모델을 API로 제공하는 인프라 구축입니다.
서빙 옵션# 도구 특징 적합한 경우 FastAPI 간단, Python 친화적 프로토타입, 소규모 Triton 고성능, 다중 모델 대규모 프로덕션 TorchServe PyTorch 통합 PyTorch 중심 TensorFlow Serving TF 최적화 TensorFlow 중심 BentoML ML 특화 빠른 배포 FastAPI 서빙# 기본 구조# from fastapi import FastAPI, File, UploadFile from PIL import Image import torch import io app = FastAPI() # 모델 로드 (시작 시 한 번) model = torch.load(&#34;model.pt&#34;) model.eval() @app.post(&#34;/predict&#34;) async def predict(file: UploadFile = File(...)): # 이미지 로드 image_bytes = await file.read() image = Image.open(io.BytesIO(image_bytes)) # 전처리 input_tensor = preprocess(image) # 추론 with torch.no_grad(): output = model(input_tensor) # 후처리 result = postprocess(output) return {&#34;prediction&#34;: result} # 실행: uvicorn main:app --host 0.0.0.0 --port 8000배치 처리# from fastapi import BackgroundTasks import asyncio from collections import deque batch_queue = deque() batch_size = 32 batch_timeout = 0.1 # 100ms async def batch_processor(): while True: if len(batch_queue) &gt;= batch_size or \ (len(batch_queue) &gt; 0 and time_since_first &gt; batch_timeout): # 배치 처리 batch = [batch_queue.popleft() for _ in range(min(len(batch_queue), batch_size))] inputs = torch.stack([item[&#39;input&#39;] for item in batch]) outputs = model(inputs) for item, output in zip(batch, outputs): item[&#39;future&#39;].set_result(output) await asyncio.sleep(0.01) @app.on_event(&#34;startup&#34;) async def startup(): asyncio.create_task(batch_processor()) Triton Inference Server# 모델 저장소# model_repository/ ├── resnet/ │ ├── config.pbtxt │ └── 1/ │ └── model.onnx └── yolo/ ├── config.pbtxt └── 1/ └── model.planconfig.pbtxt# name: &#34;resnet&#34; platform: &#34;onnxruntime_onnx&#34; max_batch_size: 32 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] dynamic_batching { preferred_batch_size: [8, 16, 32] max_queue_delay_microseconds: 100000 } instance_group [ { count: 2, kind: KIND_GPU } ]서버 실행# docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \ -v $(pwd)/model_repository:/models \ nvcr.io/nvidia/tritonserver:23.10-py3 \ tritonserver --model-repository=/models클라이언트# import tritonclient.http as httpclient import numpy as np client = httpclient.InferenceServerClient(url=&#34;localhost:8000&#34;) # 입력 준비 input_data = np.random.randn(1, 3, 224, 224).astype(np.float32) inputs = [httpclient.InferInput(&#34;input&#34;, input_data.shape, &#34;FP32&#34;)] inputs[0].set_data_from_numpy(input_data) # 추론 outputs = [httpclient.InferRequestedOutput(&#34;output&#34;)] result = client.infer(&#34;resnet&#34;, inputs, outputs=outputs) output_data = result.as_numpy(&#34;output&#34;) TorchServe# 모델 아카이브 생성# # handler.py 작성 torch-model-archiver --model-name resnet \ --version 1.0 \ --model-file model.py \ --serialized-file model.pt \ --handler image_classifier \ --export-path model_storehandler.py# from ts.torch_handler.vision_handler import VisionHandler class MyHandler(VisionHandler): def preprocess(self, data): images = [] for row in data: image = row.get(&#34;data&#34;) or row.get(&#34;body&#34;) image = Image.open(io.BytesIO(image)) image = self.transform(image) images.append(image) return torch.stack(images) def postprocess(self, output): probs = torch.softmax(output, dim=1) return probs.tolist()서버 실행# torchserve --start --model-store model_store --models resnet=resnet.mar BentoML# 모델 저장# import bentoml # 모델 저장 bentoml.pytorch.save_model(&#34;resnet&#34;, model)서비스 정의# # service.py import bentoml from bentoml.io import Image, JSON runner = bentoml.pytorch.get(&#34;resnet:latest&#34;).to_runner() svc = bentoml.Service(&#34;image_classifier&#34;, runners=[runner]) @svc.api(input=Image(), output=JSON()) async def predict(image): input_tensor = preprocess(image) result = await runner.async_run(input_tensor) return postprocess(result)빌드 및 배포# bentoml build bentoml serve service:svc 성능 최적화# 동시성# # FastAPI with uvicorn workers uvicorn main:app --workers 4 --host 0.0.0.0 --port 8000GPU 메모리 관리# # 모델별 GPU 할당 import os os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;0&#34; # 또는 Triton에서 instance_group 설정모니터링# from prometheus_client import Counter, Histogram, start_http_server REQUEST_COUNT = Counter(&#39;requests_total&#39;, &#39;Total requests&#39;) REQUEST_LATENCY = Histogram(&#39;request_latency_seconds&#39;, &#39;Request latency&#39;) @app.middleware(&#34;http&#34;) async def add_metrics(request, call_next): REQUEST_COUNT.inc() with REQUEST_LATENCY.time(): response = await call_next(request) return response Docker 배포# Dockerfile# FROM python:3.10-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [&#34;uvicorn&#34;, &#34;main:app&#34;, &#34;--host&#34;, &#34;0.0.0.0&#34;, &#34;--port&#34;, &#34;8000&#34;]docker-compose.yml# version: &#39;3.8&#39; services: api: build: . ports: - &#34;8000:8000&#34; deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] Kubernetes 배포# apiVersion: apps/v1 kind: Deployment metadata: name: model-server spec: replicas: 3 selector: matchLabels: app: model-server template: spec: containers: - name: model-server image: my-model-server:latest ports: - containerPort: 8000 resources: limits: nvidia.com/gpu: 1 --- apiVersion: v1 kind: Service metadata: name: model-server spec: selector: app: model-server ports: - port: 80 targetPort: 8000 type: LoadBalancer 관련 콘텐츠# TensorRT - GPU 최적화 ONNX - 모델 변환 최적화 - 모델 경량화">
  <meta property="og:locale" content="ko_KR">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">


  <meta itemprop="name" content="모델 서빙">
  <meta itemprop="description" content="모델 서빙# 개요# 학습된 모델을 API로 제공하는 인프라 구축입니다.
서빙 옵션# 도구 특징 적합한 경우 FastAPI 간단, Python 친화적 프로토타입, 소규모 Triton 고성능, 다중 모델 대규모 프로덕션 TorchServe PyTorch 통합 PyTorch 중심 TensorFlow Serving TF 최적화 TensorFlow 중심 BentoML ML 특화 빠른 배포 FastAPI 서빙# 기본 구조# from fastapi import FastAPI, File, UploadFile from PIL import Image import torch import io app = FastAPI() # 모델 로드 (시작 시 한 번) model = torch.load(&#34;model.pt&#34;) model.eval() @app.post(&#34;/predict&#34;) async def predict(file: UploadFile = File(...)): # 이미지 로드 image_bytes = await file.read() image = Image.open(io.BytesIO(image_bytes)) # 전처리 input_tensor = preprocess(image) # 추론 with torch.no_grad(): output = model(input_tensor) # 후처리 result = postprocess(output) return {&#34;prediction&#34;: result} # 실행: uvicorn main:app --host 0.0.0.0 --port 8000배치 처리# from fastapi import BackgroundTasks import asyncio from collections import deque batch_queue = deque() batch_size = 32 batch_timeout = 0.1 # 100ms async def batch_processor(): while True: if len(batch_queue) &gt;= batch_size or \ (len(batch_queue) &gt; 0 and time_since_first &gt; batch_timeout): # 배치 처리 batch = [batch_queue.popleft() for _ in range(min(len(batch_queue), batch_size))] inputs = torch.stack([item[&#39;input&#39;] for item in batch]) outputs = model(inputs) for item, output in zip(batch, outputs): item[&#39;future&#39;].set_result(output) await asyncio.sleep(0.01) @app.on_event(&#34;startup&#34;) async def startup(): asyncio.create_task(batch_processor()) Triton Inference Server# 모델 저장소# model_repository/ ├── resnet/ │ ├── config.pbtxt │ └── 1/ │ └── model.onnx └── yolo/ ├── config.pbtxt └── 1/ └── model.planconfig.pbtxt# name: &#34;resnet&#34; platform: &#34;onnxruntime_onnx&#34; max_batch_size: 32 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] dynamic_batching { preferred_batch_size: [8, 16, 32] max_queue_delay_microseconds: 100000 } instance_group [ { count: 2, kind: KIND_GPU } ]서버 실행# docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \ -v $(pwd)/model_repository:/models \ nvcr.io/nvidia/tritonserver:23.10-py3 \ tritonserver --model-repository=/models클라이언트# import tritonclient.http as httpclient import numpy as np client = httpclient.InferenceServerClient(url=&#34;localhost:8000&#34;) # 입력 준비 input_data = np.random.randn(1, 3, 224, 224).astype(np.float32) inputs = [httpclient.InferInput(&#34;input&#34;, input_data.shape, &#34;FP32&#34;)] inputs[0].set_data_from_numpy(input_data) # 추론 outputs = [httpclient.InferRequestedOutput(&#34;output&#34;)] result = client.infer(&#34;resnet&#34;, inputs, outputs=outputs) output_data = result.as_numpy(&#34;output&#34;) TorchServe# 모델 아카이브 생성# # handler.py 작성 torch-model-archiver --model-name resnet \ --version 1.0 \ --model-file model.py \ --serialized-file model.pt \ --handler image_classifier \ --export-path model_storehandler.py# from ts.torch_handler.vision_handler import VisionHandler class MyHandler(VisionHandler): def preprocess(self, data): images = [] for row in data: image = row.get(&#34;data&#34;) or row.get(&#34;body&#34;) image = Image.open(io.BytesIO(image)) image = self.transform(image) images.append(image) return torch.stack(images) def postprocess(self, output): probs = torch.softmax(output, dim=1) return probs.tolist()서버 실행# torchserve --start --model-store model_store --models resnet=resnet.mar BentoML# 모델 저장# import bentoml # 모델 저장 bentoml.pytorch.save_model(&#34;resnet&#34;, model)서비스 정의# # service.py import bentoml from bentoml.io import Image, JSON runner = bentoml.pytorch.get(&#34;resnet:latest&#34;).to_runner() svc = bentoml.Service(&#34;image_classifier&#34;, runners=[runner]) @svc.api(input=Image(), output=JSON()) async def predict(image): input_tensor = preprocess(image) result = await runner.async_run(input_tensor) return postprocess(result)빌드 및 배포# bentoml build bentoml serve service:svc 성능 최적화# 동시성# # FastAPI with uvicorn workers uvicorn main:app --workers 4 --host 0.0.0.0 --port 8000GPU 메모리 관리# # 모델별 GPU 할당 import os os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;0&#34; # 또는 Triton에서 instance_group 설정모니터링# from prometheus_client import Counter, Histogram, start_http_server REQUEST_COUNT = Counter(&#39;requests_total&#39;, &#39;Total requests&#39;) REQUEST_LATENCY = Histogram(&#39;request_latency_seconds&#39;, &#39;Request latency&#39;) @app.middleware(&#34;http&#34;) async def add_metrics(request, call_next): REQUEST_COUNT.inc() with REQUEST_LATENCY.time(): response = await call_next(request) return response Docker 배포# Dockerfile# FROM python:3.10-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [&#34;uvicorn&#34;, &#34;main:app&#34;, &#34;--host&#34;, &#34;0.0.0.0&#34;, &#34;--port&#34;, &#34;8000&#34;]docker-compose.yml# version: &#39;3.8&#39; services: api: build: . ports: - &#34;8000:8000&#34; deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] Kubernetes 배포# apiVersion: apps/v1 kind: Deployment metadata: name: model-server spec: replicas: 3 selector: matchLabels: app: model-server template: spec: containers: - name: model-server image: my-model-server:latest ports: - containerPort: 8000 resources: limits: nvidia.com/gpu: 1 --- apiVersion: v1 kind: Service metadata: name: model-server spec: selector: app: model-server ports: - port: 80 targetPort: 8000 type: LoadBalancer 관련 콘텐츠# TensorRT - GPU 최적화 ONNX - 모델 변환 최적화 - 모델 경량화">
  <meta itemprop="wordCount" content="620">

<title>모델 서빙 | Vision Engineer 지식 가이드</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/ko/docs/engineering/deployment/serving/">
<link rel="stylesheet" href="/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css" integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG&#43;T2l66Bw7pV8=" crossorigin="anonymous">


  <script defer src="/fuse.min.js"></script>
  <script defer src="/ko.search.min.e521ced8a1de353f186f5733d176d23d7236b1309fd3a485d880dd5743b5a505.js" integrity="sha256-5SHO2KHeNT8Yb1cz0XbSPXI2sTCf06SF2IDdV0O1pQU=" crossorigin="anonymous"></script>



  <link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script><script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&#34;delimiters&#34;:[{&#34;left&#34;:&#34;$$&#34;,&#34;right&#34;:&#34;$$&#34;,&#34;display&#34;:true},{&#34;left&#34;:&#34;\\(&#34;,&#34;right&#34;:&#34;\\)&#34;,&#34;display&#34;:false},{&#34;left&#34;:&#34;\\[&#34;,&#34;right&#34;:&#34;\\]&#34;,&#34;display&#34;:true}]});"></script>
</head>
<body dir="ltr" class="book-kind-page book-type-docs">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/ko/"><span>Vision Engineer 지식 가이드</span>
  </a>
</h2>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  


  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button">
        <img src="/icons/translate.svg" class="book-icon" alt="Languages" />
        한국어
      </a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>

    <ul>
      
      <li>
        <a href="/" class="flex flex-auto">
          English
        </a>
      </li>
      
    </ul>
  </li>
</ul>












  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-14bcba7439f342435c91a57530ca56c8" class="toggle"  />
    <label for="section-14bcba7439f342435c91a57530ca56c8" class="flex">
      <a href="/ko/docs/timeline/" class="">
        타임라인</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cb9657695dbd536c3a44d019c4d6ef98" class="toggle"  />
    <label for="section-cb9657695dbd536c3a44d019c4d6ef98" class="flex">
      <a href="/ko/docs/topdown/" class="">
        Top-Down</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-e7f22db027b1c591ed84a4d4d3a6014f" class="toggle"  />
    <label for="section-e7f22db027b1c591ed84a4d4d3a6014f" class="flex">
      <a href="/ko/docs/bottomup/" class="">
        Bottom-Up</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="toggle"  />
    <label for="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="flex">
      <a href="/ko/docs/math/" class="">
        수학</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b7c22078b221fa1e5fe6dba5f036e317" class="toggle" checked />
    <label for="section-b7c22078b221fa1e5fe6dba5f036e317" class="flex">
      <a href="/ko/docs/engineering/" class="">
        실무 기술</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="toggle"  />
    <label for="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="flex">
      <a href="/ko/docs/engineering/data/" class="">
        데이터</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/augmentation/" class="">
      데이터 증강</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/formats/" class="">
      데이터 포맷</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/pipeline/" class="">
      데이터 파이프라인</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/labeling/" class="">
      레이블링</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08b7417cd9477b9be27e6c7b561a01bc" class="toggle" checked />
    <label for="section-08b7417cd9477b9be27e6c7b561a01bc" class="flex">
      <a href="/ko/docs/engineering/deployment/" class="">
        배포</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/onnx/" class="">
      ONNX</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/tensorrt/" class="">
      TensorRT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/serving/" class="active">
      모델 서빙</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/optimization/" class="">
      모델 최적화</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="toggle"  />
    <label for="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="flex">
      <a href="/ko/docs/engineering/hardware/" class="">
        하드웨어</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/camera/" class="">
      카메라</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/lighting/" class="">
      조명</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/edge/" class="">
      엣지 디바이스</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-32f2aee524dad19f5100855d6e2014e6" class="toggle"  />
    <label for="section-32f2aee524dad19f5100855d6e2014e6" class="flex">
      <a href="/ko/docs/architecture/" class="">
        아키텍처</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47444199c3019e8b479623bc90a9bd74" class="toggle"  />
    <label for="section-47444199c3019e8b479623bc90a9bd74" class="flex">
      <a href="/ko/docs/architecture/classical/" class="">
        Classical CV</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/classical/sift-hog/" class="">
      SIFT &amp; HOG</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7bf44239f7ce2c72600738f8c315efe4" class="toggle"  />
    <label for="section-7bf44239f7ce2c72600738f8c315efe4" class="flex">
      <a href="/ko/docs/architecture/cnn/" class="">
        CNN</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/alexnet/" class="">
      AlexNet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/vgg/" class="">
      VGG</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/resnet/" class="">
      ResNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-274b800b12326ef4e06255cf5f15c308" class="toggle"  />
    <label for="section-274b800b12326ef4e06255cf5f15c308" class="flex">
      <a href="/ko/docs/architecture/detection/" class="">
        Detection</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/faster-rcnn/" class="">
      Faster R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/yolo/" class="">
      YOLO</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7246e67fd6c549f3bd6240d998b23a6f" class="toggle"  />
    <label for="section-7246e67fd6c549f3bd6240d998b23a6f" class="flex">
      <a href="/ko/docs/architecture/segmentation/" class="">
        Segmentation</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/unet/" class="">
      U-Net</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/mask-rcnn/" class="">
      Mask R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/sam/" class="">
      SAM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-3a9f4e0826e1b9e72550db30b2adcad7" class="toggle"  />
    <label for="section-3a9f4e0826e1b9e72550db30b2adcad7" class="flex">
      <a href="/ko/docs/architecture/transformer/" class="">
        Transformer</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/vit/" class="">
      ViT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/dit/" class="">
      DiT</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2902afa0a62febfd7a5eb243df871e8c" class="toggle"  />
    <label for="section-2902afa0a62febfd7a5eb243df871e8c" class="flex">
      <a href="/ko/docs/architecture/generative/" class="">
        Generative</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/vae/" class="">
      VAE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/gan/" class="">
      GAN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/stable-diffusion/" class="">
      Stable Diffusion</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/controlnet/" class="">
      ControlNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1670f136e85bf39bce97c9ee224d29d9" class="toggle"  />
    <label for="section-1670f136e85bf39bce97c9ee224d29d9" class="flex">
      <a href="/ko/docs/architecture/multimodal/" class="">
        Multimodal</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/clip/" class="">
      CLIP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/vlm/" class="">
      VLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc1b0b4545ce791b47ab3fe55b854e56" class="toggle"  />
    <label for="section-fc1b0b4545ce791b47ab3fe55b854e56" class="flex">
      <a href="/ko/docs/architecture/3d/" class="">
        3D Vision</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/nerf/" class="">
      NeRF</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/3dgs/" class="">
      3D Gaussian Splatting</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd575cc6fd4e25b684b552791b7a009a" class="toggle"  />
    <label for="section-cd575cc6fd4e25b684b552791b7a009a" class="flex">
      <a href="/ko/docs/task/" class="">
        태스크</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fb690f19903e1435676f710c928c9ad4" class="toggle"  />
    <label for="section-fb690f19903e1435676f710c928c9ad4" class="flex">
      <a href="/ko/docs/etc/" class="">
        기타 자료</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/icons/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>모델 서빙</h3>

  <label for="toc-control">
    
    <img src="/icons/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#서빙-옵션">서빙 옵션</a></li>
    <li><a href="#fastapi-서빙">FastAPI 서빙</a>
      <ul>
        <li><a href="#기본-구조">기본 구조</a></li>
        <li><a href="#배치-처리">배치 처리</a></li>
      </ul>
    </li>
    <li><a href="#triton-inference-server">Triton Inference Server</a>
      <ul>
        <li><a href="#모델-저장소">모델 저장소</a></li>
        <li><a href="#configpbtxt">config.pbtxt</a></li>
        <li><a href="#서버-실행">서버 실행</a></li>
        <li><a href="#클라이언트">클라이언트</a></li>
      </ul>
    </li>
    <li><a href="#torchserve">TorchServe</a>
      <ul>
        <li><a href="#모델-아카이브-생성">모델 아카이브 생성</a></li>
        <li><a href="#handlerpy">handler.py</a></li>
        <li><a href="#서버-실행-1">서버 실행</a></li>
      </ul>
    </li>
    <li><a href="#bentoml">BentoML</a>
      <ul>
        <li><a href="#모델-저장">모델 저장</a></li>
        <li><a href="#서비스-정의">서비스 정의</a></li>
        <li><a href="#빌드-및-배포">빌드 및 배포</a></li>
      </ul>
    </li>
    <li><a href="#성능-최적화">성능 최적화</a>
      <ul>
        <li><a href="#동시성">동시성</a></li>
        <li><a href="#gpu-메모리-관리">GPU 메모리 관리</a></li>
        <li><a href="#모니터링">모니터링</a></li>
      </ul>
    </li>
    <li><a href="#docker-배포">Docker 배포</a>
      <ul>
        <li><a href="#dockerfile">Dockerfile</a></li>
        <li><a href="#docker-composeyml">docker-compose.yml</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-배포">Kubernetes 배포</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="모델-서빙">모델 서빙<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%ec%84%9c%eb%b9%99">#</a></h1>
<h2 id="개요">개요<a class="anchor" href="#%ea%b0%9c%ec%9a%94">#</a></h2>
<p>학습된 모델을 API로 제공하는 인프라 구축입니다.</p>
<hr>
<h2 id="서빙-옵션">서빙 옵션<a class="anchor" href="#%ec%84%9c%eb%b9%99-%ec%98%b5%ec%85%98">#</a></h2>
<table>
  <thead>
      <tr>
          <th>도구</th>
          <th>특징</th>
          <th>적합한 경우</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>FastAPI</strong></td>
          <td>간단, Python 친화적</td>
          <td>프로토타입, 소규모</td>
      </tr>
      <tr>
          <td><strong>Triton</strong></td>
          <td>고성능, 다중 모델</td>
          <td>대규모 프로덕션</td>
      </tr>
      <tr>
          <td><strong>TorchServe</strong></td>
          <td>PyTorch 통합</td>
          <td>PyTorch 중심</td>
      </tr>
      <tr>
          <td><strong>TensorFlow Serving</strong></td>
          <td>TF 최적화</td>
          <td>TensorFlow 중심</td>
      </tr>
      <tr>
          <td><strong>BentoML</strong></td>
          <td>ML 특화</td>
          <td>빠른 배포</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="fastapi-서빙">FastAPI 서빙<a class="anchor" href="#fastapi-%ec%84%9c%eb%b9%99">#</a></h2>
<h3 id="기본-구조">기본 구조<a class="anchor" href="#%ea%b8%b0%eb%b3%b8-%ea%b5%ac%ec%a1%b0">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> fastapi <span style="color:#f92672">import</span> FastAPI, File, UploadFile
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> io
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>app <span style="color:#f92672">=</span> FastAPI()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 모델 로드 (시작 시 한 번)</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;model.pt&#34;</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/predict&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(file: UploadFile <span style="color:#f92672">=</span> File(<span style="color:#f92672">...</span>)):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 이미지 로드</span>
</span></span><span style="display:flex;"><span>    image_bytes <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> file<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>    image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(io<span style="color:#f92672">.</span>BytesIO(image_bytes))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 전처리</span>
</span></span><span style="display:flex;"><span>    input_tensor <span style="color:#f92672">=</span> preprocess(image)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 추론</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(input_tensor)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 후처리</span>
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> postprocess(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;prediction&#34;</span>: result}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 실행: uvicorn main:app --host 0.0.0.0 --port 8000</span></span></span></code></pre></div><h3 id="배치-처리">배치 처리<a class="anchor" href="#%eb%b0%b0%ec%b9%98-%ec%b2%98%eb%a6%ac">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> fastapi <span style="color:#f92672">import</span> BackgroundTasks
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> asyncio
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> deque
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>batch_queue <span style="color:#f92672">=</span> deque()
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>batch_timeout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>  <span style="color:#75715e"># 100ms</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_processor</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(batch_queue) <span style="color:#f92672">&gt;=</span> batch_size <span style="color:#f92672">or</span> \
</span></span><span style="display:flex;"><span>           (len(batch_queue) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> time_since_first <span style="color:#f92672">&gt;</span> batch_timeout):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 배치 처리</span>
</span></span><span style="display:flex;"><span>            batch <span style="color:#f92672">=</span> [batch_queue<span style="color:#f92672">.</span>popleft() <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(min(len(batch_queue), batch_size))]
</span></span><span style="display:flex;"><span>            inputs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([item[<span style="color:#e6db74">&#39;input&#39;</span>] <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> batch])
</span></span><span style="display:flex;"><span>            outputs <span style="color:#f92672">=</span> model(inputs)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> item, output <span style="color:#f92672">in</span> zip(batch, outputs):
</span></span><span style="display:flex;"><span>                item[<span style="color:#e6db74">&#39;future&#39;</span>]<span style="color:#f92672">.</span>set_result(output)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">await</span> asyncio<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.on_event</span>(<span style="color:#e6db74">&#34;startup&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">startup</span>():
</span></span><span style="display:flex;"><span>    asyncio<span style="color:#f92672">.</span>create_task(batch_processor())</span></span></code></pre></div><hr>
<h2 id="triton-inference-server">Triton Inference Server<a class="anchor" href="#triton-inference-server">#</a></h2>
<h3 id="모델-저장소">모델 저장소<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%ec%a0%80%ec%9e%a5%ec%86%8c">#</a></h3>
<pre tabindex="0"><code>model_repository/
├── resnet/
│   ├── config.pbtxt
│   └── 1/
│       └── model.onnx
└── yolo/
    ├── config.pbtxt
    └── 1/
        └── model.plan</code></pre><h3 id="configpbtxt">config.pbtxt<a class="anchor" href="#configpbtxt">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-protobuf" data-lang="protobuf"><span style="display:flex;"><span>name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;resnet&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>platform<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;onnxruntime_onnx&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>max_batch_size<span style="color:#f92672">:</span> <span style="color:#ae81ff">32</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>input [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;input&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    data_type<span style="color:#f92672">:</span> TYPE_FP32<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    dims<span style="color:#f92672">:</span> [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>output [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;output&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    data_type<span style="color:#f92672">:</span> TYPE_FP32<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    dims<span style="color:#f92672">:</span> [<span style="color:#ae81ff">1000</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>dynamic_batching {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  preferred_batch_size<span style="color:#f92672">:</span> [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  max_queue_delay_microseconds<span style="color:#f92672">:</span> <span style="color:#ae81ff">100000</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>}<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>instance_group [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  { count<span style="color:#f92672">:</span> <span style="color:#ae81ff">2</span>, kind<span style="color:#f92672">:</span> KIND_GPU }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]</span></span></code></pre></div><h3 id="서버-실행">서버 실행<a class="anchor" href="#%ec%84%9c%eb%b2%84-%ec%8b%a4%ed%96%89">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/model_repository:/models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  nvcr.io/nvidia/tritonserver:23.10-py3 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  tritonserver --model-repository<span style="color:#f92672">=</span>/models</span></span></code></pre></div><h3 id="클라이언트">클라이언트<a class="anchor" href="#%ed%81%b4%eb%9d%bc%ec%9d%b4%ec%96%b8%ed%8a%b8">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tritonclient.http <span style="color:#66d9ef">as</span> httpclient
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> httpclient<span style="color:#f92672">.</span>InferenceServerClient(url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;localhost:8000&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 입력 준비</span>
</span></span><span style="display:flex;"><span>input_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> [httpclient<span style="color:#f92672">.</span>InferInput(<span style="color:#e6db74">&#34;input&#34;</span>, input_data<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">&#34;FP32&#34;</span>)]
</span></span><span style="display:flex;"><span>inputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_data_from_numpy(input_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 추론</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> [httpclient<span style="color:#f92672">.</span>InferRequestedOutput(<span style="color:#e6db74">&#34;output&#34;</span>)]
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>infer(<span style="color:#e6db74">&#34;resnet&#34;</span>, inputs, outputs<span style="color:#f92672">=</span>outputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output_data <span style="color:#f92672">=</span> result<span style="color:#f92672">.</span>as_numpy(<span style="color:#e6db74">&#34;output&#34;</span>)</span></span></code></pre></div><hr>
<h2 id="torchserve">TorchServe<a class="anchor" href="#torchserve">#</a></h2>
<h3 id="모델-아카이브-생성">모델 아카이브 생성<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%ec%95%84%ec%b9%b4%ec%9d%b4%eb%b8%8c-%ec%83%9d%ec%84%b1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># handler.py 작성</span>
</span></span><span style="display:flex;"><span>torch-model-archiver --model-name resnet <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --version 1.0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --model-file model.py <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --serialized-file model.pt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --handler image_classifier <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>    --export-path model_store</span></span></code></pre></div><h3 id="handlerpy">handler.py<a class="anchor" href="#handlerpy">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> ts.torch_handler.vision_handler <span style="color:#f92672">import</span> VisionHandler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyHandler</span>(VisionHandler):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocess</span>(self, data):
</span></span><span style="display:flex;"><span>        images <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>            image <span style="color:#f92672">=</span> row<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;data&#34;</span>) <span style="color:#f92672">or</span> row<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;body&#34;</span>)
</span></span><span style="display:flex;"><span>            image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(io<span style="color:#f92672">.</span>BytesIO(image))
</span></span><span style="display:flex;"><span>            image <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transform(image)
</span></span><span style="display:flex;"><span>            images<span style="color:#f92672">.</span>append(image)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>stack(images)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">postprocess</span>(self, output):
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> probs<span style="color:#f92672">.</span>tolist()</span></span></code></pre></div><h3 id="서버-실행-1">서버 실행<a class="anchor" href="#%ec%84%9c%eb%b2%84-%ec%8b%a4%ed%96%89-1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>torchserve --start --model-store model_store --models resnet<span style="color:#f92672">=</span>resnet.mar</span></span></code></pre></div><hr>
<h2 id="bentoml">BentoML<a class="anchor" href="#bentoml">#</a></h2>
<h3 id="모델-저장">모델 저장<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%ec%a0%80%ec%9e%a5">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 모델 저장</span>
</span></span><span style="display:flex;"><span>bentoml<span style="color:#f92672">.</span>pytorch<span style="color:#f92672">.</span>save_model(<span style="color:#e6db74">&#34;resnet&#34;</span>, model)</span></span></code></pre></div><h3 id="서비스-정의">서비스 정의<a class="anchor" href="#%ec%84%9c%eb%b9%84%ec%8a%a4-%ec%a0%95%ec%9d%98">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># service.py</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bentoml.io <span style="color:#f92672">import</span> Image, JSON
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>runner <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>pytorch<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;resnet:latest&#34;</span>)<span style="color:#f92672">.</span>to_runner()
</span></span><span style="display:flex;"><span>svc <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>Service(<span style="color:#e6db74">&#34;image_classifier&#34;</span>, runners<span style="color:#f92672">=</span>[runner])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@svc.api</span>(input<span style="color:#f92672">=</span>Image(), output<span style="color:#f92672">=</span>JSON())
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(image):
</span></span><span style="display:flex;"><span>    input_tensor <span style="color:#f92672">=</span> preprocess(image)
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> runner<span style="color:#f92672">.</span>async_run(input_tensor)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> postprocess(result)</span></span></code></pre></div><h3 id="빌드-및-배포">빌드 및 배포<a class="anchor" href="#%eb%b9%8c%eb%93%9c-%eb%b0%8f-%eb%b0%b0%ed%8f%ac">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>bentoml build
</span></span><span style="display:flex;"><span>bentoml serve service:svc</span></span></code></pre></div><hr>
<h2 id="성능-최적화">성능 최적화<a class="anchor" href="#%ec%84%b1%eb%8a%a5-%ec%b5%9c%ec%a0%81%ed%99%94">#</a></h2>
<h3 id="동시성">동시성<a class="anchor" href="#%eb%8f%99%ec%8b%9c%ec%84%b1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># FastAPI with uvicorn workers</span>
</span></span><span style="display:flex;"><span>uvicorn main:app <span style="color:#f92672">--</span>workers <span style="color:#ae81ff">4</span> <span style="color:#f92672">--</span>host <span style="color:#ae81ff">0.0.0.0</span> <span style="color:#f92672">--</span>port <span style="color:#ae81ff">8000</span></span></span></code></pre></div><h3 id="gpu-메모리-관리">GPU 메모리 관리<a class="anchor" href="#gpu-%eb%a9%94%eb%aa%a8%eb%a6%ac-%ea%b4%80%eb%a6%ac">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 모델별 GPU 할당</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;CUDA_VISIBLE_DEVICES&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;0&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 또는 Triton에서 instance_group 설정</span></span></span></code></pre></div><h3 id="모니터링">모니터링<a class="anchor" href="#%eb%aa%a8%eb%8b%88%ed%84%b0%eb%a7%81">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> prometheus_client <span style="color:#f92672">import</span> Counter, Histogram, start_http_server
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>REQUEST_COUNT <span style="color:#f92672">=</span> Counter(<span style="color:#e6db74">&#39;requests_total&#39;</span>, <span style="color:#e6db74">&#39;Total requests&#39;</span>)
</span></span><span style="display:flex;"><span>REQUEST_LATENCY <span style="color:#f92672">=</span> Histogram(<span style="color:#e6db74">&#39;request_latency_seconds&#39;</span>, <span style="color:#e6db74">&#39;Request latency&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.middleware</span>(<span style="color:#e6db74">&#34;http&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add_metrics</span>(request, call_next):
</span></span><span style="display:flex;"><span>    REQUEST_COUNT<span style="color:#f92672">.</span>inc()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> REQUEST_LATENCY<span style="color:#f92672">.</span>time():
</span></span><span style="display:flex;"><span>        response <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> call_next(request)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> response</span></span></code></pre></div><hr>
<h2 id="docker-배포">Docker 배포<a class="anchor" href="#docker-%eb%b0%b0%ed%8f%ac">#</a></h2>
<h3 id="dockerfile">Dockerfile<a class="anchor" href="#dockerfile">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-dockerfile" data-lang="dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span> <span style="color:#e6db74">python:3.10-slim</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">WORKDIR</span> <span style="color:#e6db74">/app</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">COPY</span> requirements.txt .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">RUN</span> pip install -r requirements.txt<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">COPY</span> . .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">EXPOSE</span> <span style="color:#e6db74">8000</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">CMD</span> [<span style="color:#e6db74">&#34;uvicorn&#34;</span>, <span style="color:#e6db74">&#34;main:app&#34;</span>, <span style="color:#e6db74">&#34;--host&#34;</span>, <span style="color:#e6db74">&#34;0.0.0.0&#34;</span>, <span style="color:#e6db74">&#34;--port&#34;</span>, <span style="color:#e6db74">&#34;8000&#34;</span>]</span></span></code></pre></div><h3 id="docker-composeyml">docker-compose.yml<a class="anchor" href="#docker-composeyml">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#e6db74">&#39;3.8&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">api</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">build</span>: <span style="color:#ae81ff">.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;8000:8000&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">deploy</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">reservations</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">devices</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">driver</span>: <span style="color:#ae81ff">nvidia</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">count</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">capabilities</span>: [<span style="color:#ae81ff">gpu]</span></span></span></code></pre></div><hr>
<h2 id="kubernetes-배포">Kubernetes 배포<a class="anchor" href="#kubernetes-%eb%b0%b0%ed%8f%ac">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">model-server</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">model-server</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">model-server</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">my-model-server:latest</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">nvidia.com/gpu</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">model-server</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">model-server</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">LoadBalancer</span></span></span></code></pre></div><hr>
<h2 id="관련-콘텐츠">관련 콘텐츠<a class="anchor" href="#%ea%b4%80%eb%a0%a8-%ec%bd%98%ed%85%90%ec%b8%a0">#</a></h2>
<ul>
<li><a href="/ko/docs/engineering/deployment/tensorrt">TensorRT</a> - GPU 최적화</li>
<li><a href="/ko/docs/engineering/deployment/onnx">ONNX</a> - 모델 변환</li>
<li><a href="/ko/docs/engineering/deployment/optimization">최적화</a> - 모델 경량화</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/ko/docs/engineering/deployment/tensorrt/" class="flex align-center">
        <img src="/icons/backward.svg" class="book-icon" alt="Backward" />
        <span>TensorRT</span>
      </a>
    
    </span>
    <span>
    
      <a href="/ko/docs/engineering/deployment/optimization/" class="flex align-center">
        <span>모델 최적화</span>
        <img src="/icons/forward.svg" class="book-icon" alt="Forward" />
      </a>
    
    </span>
  </div>
  


 
        
  
  <div class="book-comments">

</div>
  
 
        
        
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
  <aside class="book-toc">
    <div class="book-toc-content">
      
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#서빙-옵션">서빙 옵션</a></li>
    <li><a href="#fastapi-서빙">FastAPI 서빙</a>
      <ul>
        <li><a href="#기본-구조">기본 구조</a></li>
        <li><a href="#배치-처리">배치 처리</a></li>
      </ul>
    </li>
    <li><a href="#triton-inference-server">Triton Inference Server</a>
      <ul>
        <li><a href="#모델-저장소">모델 저장소</a></li>
        <li><a href="#configpbtxt">config.pbtxt</a></li>
        <li><a href="#서버-실행">서버 실행</a></li>
        <li><a href="#클라이언트">클라이언트</a></li>
      </ul>
    </li>
    <li><a href="#torchserve">TorchServe</a>
      <ul>
        <li><a href="#모델-아카이브-생성">모델 아카이브 생성</a></li>
        <li><a href="#handlerpy">handler.py</a></li>
        <li><a href="#서버-실행-1">서버 실행</a></li>
      </ul>
    </li>
    <li><a href="#bentoml">BentoML</a>
      <ul>
        <li><a href="#모델-저장">모델 저장</a></li>
        <li><a href="#서비스-정의">서비스 정의</a></li>
        <li><a href="#빌드-및-배포">빌드 및 배포</a></li>
      </ul>
    </li>
    <li><a href="#성능-최적화">성능 최적화</a>
      <ul>
        <li><a href="#동시성">동시성</a></li>
        <li><a href="#gpu-메모리-관리">GPU 메모리 관리</a></li>
        <li><a href="#모니터링">모니터링</a></li>
      </ul>
    </li>
    <li><a href="#docker-배포">Docker 배포</a>
      <ul>
        <li><a href="#dockerfile">Dockerfile</a></li>
        <li><a href="#docker-composeyml">docker-compose.yml</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-배포">Kubernetes 배포</a></li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



    </div>
  </aside>
  
 
  </main>

  
</body>
</html>




















