<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="TensorRT# 개요# TensorRT는 NVIDIA GPU에서 딥러닝 추론을 최적화하는 라이브러리입니다.
ONNX 모델 → TensorRT 최적화 → 최대 10x 빠른 추론 주요 최적화# 기법 설명 Layer Fusion 여러 레이어를 하나로 병합 Precision Calibration FP32 → FP16/INT8 변환 Kernel Auto-tuning 최적의 CUDA 커널 선택 Dynamic Tensor Memory 메모리 재사용 Multi-stream Execution 병렬 처리 ONNX → TensorRT 변환# trtexec (CLI)# # FP16 엔진 생성 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --fp16 # INT8 엔진 생성 (캘리브레이션 필요) trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --int8 \ --calib=calibration_cache.bin # 동적 배치 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --minShapes=input:1x3x224x224 \ --optShapes=input:8x3x224x224 \ --maxShapes=input:16x3x224x224Python API# import tensorrt as trt logger = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(logger) network = builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, logger) # ONNX 파싱 with open(&#34;model.onnx&#34;, &#34;rb&#34;) as f: parser.parse(f.read()) # 빌더 설정 config = builder.create_builder_config() config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 30) # 1GB config.set_flag(trt.BuilderFlag.FP16) # FP16 활성화 # 엔진 빌드 serialized_engine = builder.build_serialized_network(network, config) # 저장 with open(&#34;model.engine&#34;, &#34;wb&#34;) as f: f.write(serialized_engine) TensorRT 추론# import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit import numpy as np # 엔진 로드 with open(&#34;model.engine&#34;, &#34;rb&#34;) as f: runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) engine = runtime.deserialize_cuda_engine(f.read()) context = engine.create_execution_context() # 메모리 할당 input_shape = (1, 3, 224, 224) output_shape = (1, 1000) d_input = cuda.mem_alloc(np.prod(input_shape) * 4) # float32 d_output = cuda.mem_alloc(np.prod(output_shape) * 4) # 추론 def infer(input_data): cuda.memcpy_htod(d_input, input_data) context.execute_v2([int(d_input), int(d_output)]) output = np.empty(output_shape, dtype=np.float32) cuda.memcpy_dtoh(output, d_output) return output # 사용 input_data = np.random.randn(*input_shape).astype(np.float32) result = infer(input_data) INT8 양자화# 캘리브레이션# import tensorrt as trt import numpy as np class Calibrator(trt.IInt8EntropyCalibrator2): def __init__(self, data_loader, cache_file): super().__init__() self.data_loader = data_loader self.cache_file = cache_file self.batch_idx = 0 # 디바이스 메모리 할당 self.d_input = cuda.mem_alloc(self.data_loader.batch_size * 3 * 224 * 224 * 4) def get_batch_size(self): return self.data_loader.batch_size def get_batch(self, names): try: batch = next(self.data_loader) cuda.memcpy_htod(self.d_input, batch) return [int(self.d_input)] except StopIteration: return None def read_calibration_cache(self): if os.path.exists(self.cache_file): with open(self.cache_file, &#34;rb&#34;) as f: return f.read() def write_calibration_cache(self, cache): with open(self.cache_file, &#34;wb&#34;) as f: f.write(cache) # 빌더 설정 config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = Calibrator(calibration_loader, &#34;calibration.cache&#34;) 동적 Shape# # 프로파일 설정 profile = builder.create_optimization_profile() profile.set_shape( &#34;input&#34;, min=(1, 3, 224, 224), # 최소 opt=(8, 3, 224, 224), # 최적 max=(32, 3, 224, 224) # 최대 ) config.add_optimization_profile(profile) # 추론 시 shape 설정 context.set_input_shape(&#34;input&#34;, (batch_size, 3, 224, 224)) torch2trt (간편한 변환)# from torch2trt import torch2trt import torch model = MyModel().cuda().eval() x = torch.randn(1, 3, 224, 224).cuda() # TensorRT 변환 model_trt = torch2trt( model, [x], fp16_mode=True, max_batch_size=16, ) # 추론 y_trt = model_trt(x) # 저장/로드 torch.save(model_trt.state_dict(), &#34;model_trt.pth&#34;) Triton과 통합# 모델 저장소 구조# model_repository/ └── my_model/ ├── config.pbtxt └── 1/ └── model.plan # TensorRT 엔진config.pbtxt# name: &#34;my_model&#34; platform: &#34;tensorrt_plan&#34; max_batch_size: 16 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [0] } ] 성능 비교# 벤치마크 예시 (ResNet-50, RTX 3090)# Precision Latency Throughput PyTorch FP32 5.2 ms 192 FPS TensorRT FP32 2.1 ms 476 FPS TensorRT FP16 1.0 ms 1000 FPS TensorRT INT8 0.6 ms 1667 FPS 문제 해결# 지원되지 않는 연산# # 플러그인 사용 # 또는 ONNX 레벨에서 수정 # 예: 지원되지 않는 레이어를 여러 지원 레이어로 분해정확도 저하 (INT8)# # 1. 캘리브레이션 데이터 확인 (대표성 있는 데이터 사용) # 2. 민감한 레이어는 FP16 유지 config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)메모리 부족# # 워크스페이스 크기 조정 config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 28) # 256MB 관련 콘텐츠# ONNX - 모델 변환 모델 서빙 - Triton 서버 최적화 - 양자화 이론 ">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/ko/docs/engineering/deployment/tensorrt/">
  <meta property="og:site_name" content="Vision Engineer 지식 가이드">
  <meta property="og:title" content="TensorRT">
  <meta property="og:description" content="TensorRT# 개요# TensorRT는 NVIDIA GPU에서 딥러닝 추론을 최적화하는 라이브러리입니다.
ONNX 모델 → TensorRT 최적화 → 최대 10x 빠른 추론 주요 최적화# 기법 설명 Layer Fusion 여러 레이어를 하나로 병합 Precision Calibration FP32 → FP16/INT8 변환 Kernel Auto-tuning 최적의 CUDA 커널 선택 Dynamic Tensor Memory 메모리 재사용 Multi-stream Execution 병렬 처리 ONNX → TensorRT 변환# trtexec (CLI)# # FP16 엔진 생성 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --fp16 # INT8 엔진 생성 (캘리브레이션 필요) trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --int8 \ --calib=calibration_cache.bin # 동적 배치 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --minShapes=input:1x3x224x224 \ --optShapes=input:8x3x224x224 \ --maxShapes=input:16x3x224x224Python API# import tensorrt as trt logger = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(logger) network = builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, logger) # ONNX 파싱 with open(&#34;model.onnx&#34;, &#34;rb&#34;) as f: parser.parse(f.read()) # 빌더 설정 config = builder.create_builder_config() config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 30) # 1GB config.set_flag(trt.BuilderFlag.FP16) # FP16 활성화 # 엔진 빌드 serialized_engine = builder.build_serialized_network(network, config) # 저장 with open(&#34;model.engine&#34;, &#34;wb&#34;) as f: f.write(serialized_engine) TensorRT 추론# import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit import numpy as np # 엔진 로드 with open(&#34;model.engine&#34;, &#34;rb&#34;) as f: runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) engine = runtime.deserialize_cuda_engine(f.read()) context = engine.create_execution_context() # 메모리 할당 input_shape = (1, 3, 224, 224) output_shape = (1, 1000) d_input = cuda.mem_alloc(np.prod(input_shape) * 4) # float32 d_output = cuda.mem_alloc(np.prod(output_shape) * 4) # 추론 def infer(input_data): cuda.memcpy_htod(d_input, input_data) context.execute_v2([int(d_input), int(d_output)]) output = np.empty(output_shape, dtype=np.float32) cuda.memcpy_dtoh(output, d_output) return output # 사용 input_data = np.random.randn(*input_shape).astype(np.float32) result = infer(input_data) INT8 양자화# 캘리브레이션# import tensorrt as trt import numpy as np class Calibrator(trt.IInt8EntropyCalibrator2): def __init__(self, data_loader, cache_file): super().__init__() self.data_loader = data_loader self.cache_file = cache_file self.batch_idx = 0 # 디바이스 메모리 할당 self.d_input = cuda.mem_alloc(self.data_loader.batch_size * 3 * 224 * 224 * 4) def get_batch_size(self): return self.data_loader.batch_size def get_batch(self, names): try: batch = next(self.data_loader) cuda.memcpy_htod(self.d_input, batch) return [int(self.d_input)] except StopIteration: return None def read_calibration_cache(self): if os.path.exists(self.cache_file): with open(self.cache_file, &#34;rb&#34;) as f: return f.read() def write_calibration_cache(self, cache): with open(self.cache_file, &#34;wb&#34;) as f: f.write(cache) # 빌더 설정 config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = Calibrator(calibration_loader, &#34;calibration.cache&#34;) 동적 Shape# # 프로파일 설정 profile = builder.create_optimization_profile() profile.set_shape( &#34;input&#34;, min=(1, 3, 224, 224), # 최소 opt=(8, 3, 224, 224), # 최적 max=(32, 3, 224, 224) # 최대 ) config.add_optimization_profile(profile) # 추론 시 shape 설정 context.set_input_shape(&#34;input&#34;, (batch_size, 3, 224, 224)) torch2trt (간편한 변환)# from torch2trt import torch2trt import torch model = MyModel().cuda().eval() x = torch.randn(1, 3, 224, 224).cuda() # TensorRT 변환 model_trt = torch2trt( model, [x], fp16_mode=True, max_batch_size=16, ) # 추론 y_trt = model_trt(x) # 저장/로드 torch.save(model_trt.state_dict(), &#34;model_trt.pth&#34;) Triton과 통합# 모델 저장소 구조# model_repository/ └── my_model/ ├── config.pbtxt └── 1/ └── model.plan # TensorRT 엔진config.pbtxt# name: &#34;my_model&#34; platform: &#34;tensorrt_plan&#34; max_batch_size: 16 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [0] } ] 성능 비교# 벤치마크 예시 (ResNet-50, RTX 3090)# Precision Latency Throughput PyTorch FP32 5.2 ms 192 FPS TensorRT FP32 2.1 ms 476 FPS TensorRT FP16 1.0 ms 1000 FPS TensorRT INT8 0.6 ms 1667 FPS 문제 해결# 지원되지 않는 연산# # 플러그인 사용 # 또는 ONNX 레벨에서 수정 # 예: 지원되지 않는 레이어를 여러 지원 레이어로 분해정확도 저하 (INT8)# # 1. 캘리브레이션 데이터 확인 (대표성 있는 데이터 사용) # 2. 민감한 레이어는 FP16 유지 config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)메모리 부족# # 워크스페이스 크기 조정 config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 28) # 256MB 관련 콘텐츠# ONNX - 모델 변환 모델 서빙 - Triton 서버 최적화 - 양자화 이론">
  <meta property="og:locale" content="ko_KR">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">


  <meta itemprop="name" content="TensorRT">
  <meta itemprop="description" content="TensorRT# 개요# TensorRT는 NVIDIA GPU에서 딥러닝 추론을 최적화하는 라이브러리입니다.
ONNX 모델 → TensorRT 최적화 → 최대 10x 빠른 추론 주요 최적화# 기법 설명 Layer Fusion 여러 레이어를 하나로 병합 Precision Calibration FP32 → FP16/INT8 변환 Kernel Auto-tuning 최적의 CUDA 커널 선택 Dynamic Tensor Memory 메모리 재사용 Multi-stream Execution 병렬 처리 ONNX → TensorRT 변환# trtexec (CLI)# # FP16 엔진 생성 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --fp16 # INT8 엔진 생성 (캘리브레이션 필요) trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --int8 \ --calib=calibration_cache.bin # 동적 배치 trtexec --onnx=model.onnx \ --saveEngine=model.engine \ --minShapes=input:1x3x224x224 \ --optShapes=input:8x3x224x224 \ --maxShapes=input:16x3x224x224Python API# import tensorrt as trt logger = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(logger) network = builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, logger) # ONNX 파싱 with open(&#34;model.onnx&#34;, &#34;rb&#34;) as f: parser.parse(f.read()) # 빌더 설정 config = builder.create_builder_config() config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 30) # 1GB config.set_flag(trt.BuilderFlag.FP16) # FP16 활성화 # 엔진 빌드 serialized_engine = builder.build_serialized_network(network, config) # 저장 with open(&#34;model.engine&#34;, &#34;wb&#34;) as f: f.write(serialized_engine) TensorRT 추론# import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit import numpy as np # 엔진 로드 with open(&#34;model.engine&#34;, &#34;rb&#34;) as f: runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) engine = runtime.deserialize_cuda_engine(f.read()) context = engine.create_execution_context() # 메모리 할당 input_shape = (1, 3, 224, 224) output_shape = (1, 1000) d_input = cuda.mem_alloc(np.prod(input_shape) * 4) # float32 d_output = cuda.mem_alloc(np.prod(output_shape) * 4) # 추론 def infer(input_data): cuda.memcpy_htod(d_input, input_data) context.execute_v2([int(d_input), int(d_output)]) output = np.empty(output_shape, dtype=np.float32) cuda.memcpy_dtoh(output, d_output) return output # 사용 input_data = np.random.randn(*input_shape).astype(np.float32) result = infer(input_data) INT8 양자화# 캘리브레이션# import tensorrt as trt import numpy as np class Calibrator(trt.IInt8EntropyCalibrator2): def __init__(self, data_loader, cache_file): super().__init__() self.data_loader = data_loader self.cache_file = cache_file self.batch_idx = 0 # 디바이스 메모리 할당 self.d_input = cuda.mem_alloc(self.data_loader.batch_size * 3 * 224 * 224 * 4) def get_batch_size(self): return self.data_loader.batch_size def get_batch(self, names): try: batch = next(self.data_loader) cuda.memcpy_htod(self.d_input, batch) return [int(self.d_input)] except StopIteration: return None def read_calibration_cache(self): if os.path.exists(self.cache_file): with open(self.cache_file, &#34;rb&#34;) as f: return f.read() def write_calibration_cache(self, cache): with open(self.cache_file, &#34;wb&#34;) as f: f.write(cache) # 빌더 설정 config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = Calibrator(calibration_loader, &#34;calibration.cache&#34;) 동적 Shape# # 프로파일 설정 profile = builder.create_optimization_profile() profile.set_shape( &#34;input&#34;, min=(1, 3, 224, 224), # 최소 opt=(8, 3, 224, 224), # 최적 max=(32, 3, 224, 224) # 최대 ) config.add_optimization_profile(profile) # 추론 시 shape 설정 context.set_input_shape(&#34;input&#34;, (batch_size, 3, 224, 224)) torch2trt (간편한 변환)# from torch2trt import torch2trt import torch model = MyModel().cuda().eval() x = torch.randn(1, 3, 224, 224).cuda() # TensorRT 변환 model_trt = torch2trt( model, [x], fp16_mode=True, max_batch_size=16, ) # 추론 y_trt = model_trt(x) # 저장/로드 torch.save(model_trt.state_dict(), &#34;model_trt.pth&#34;) Triton과 통합# 모델 저장소 구조# model_repository/ └── my_model/ ├── config.pbtxt └── 1/ └── model.plan # TensorRT 엔진config.pbtxt# name: &#34;my_model&#34; platform: &#34;tensorrt_plan&#34; max_batch_size: 16 input [ { name: &#34;input&#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: &#34;output&#34; data_type: TYPE_FP32 dims: [1000] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [0] } ] 성능 비교# 벤치마크 예시 (ResNet-50, RTX 3090)# Precision Latency Throughput PyTorch FP32 5.2 ms 192 FPS TensorRT FP32 2.1 ms 476 FPS TensorRT FP16 1.0 ms 1000 FPS TensorRT INT8 0.6 ms 1667 FPS 문제 해결# 지원되지 않는 연산# # 플러그인 사용 # 또는 ONNX 레벨에서 수정 # 예: 지원되지 않는 레이어를 여러 지원 레이어로 분해정확도 저하 (INT8)# # 1. 캘리브레이션 데이터 확인 (대표성 있는 데이터 사용) # 2. 민감한 레이어는 FP16 유지 config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)메모리 부족# # 워크스페이스 크기 조정 config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 &lt;&lt; 28) # 256MB 관련 콘텐츠# ONNX - 모델 변환 모델 서빙 - Triton 서버 최적화 - 양자화 이론">
  <meta itemprop="wordCount" content="560">

<title>TensorRT | Vision Engineer 지식 가이드</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/ko/docs/engineering/deployment/tensorrt/">
<link rel="stylesheet" href="/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css" integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG&#43;T2l66Bw7pV8=" crossorigin="anonymous">


  <script defer src="/fuse.min.js"></script>
  <script defer src="/ko.search.min.e521ced8a1de353f186f5733d176d23d7236b1309fd3a485d880dd5743b5a505.js" integrity="sha256-5SHO2KHeNT8Yb1cz0XbSPXI2sTCf06SF2IDdV0O1pQU=" crossorigin="anonymous"></script>



  <link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script><script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&#34;delimiters&#34;:[{&#34;left&#34;:&#34;$$&#34;,&#34;right&#34;:&#34;$$&#34;,&#34;display&#34;:true},{&#34;left&#34;:&#34;\\(&#34;,&#34;right&#34;:&#34;\\)&#34;,&#34;display&#34;:false},{&#34;left&#34;:&#34;\\[&#34;,&#34;right&#34;:&#34;\\]&#34;,&#34;display&#34;:true}]});"></script>
</head>
<body dir="ltr" class="book-kind-page book-type-docs">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/ko/"><span>Vision Engineer 지식 가이드</span>
  </a>
</h2>


<div class="book-search hidden">
  <input id="book-search-input" type="text" 
    placeholder="Search"
    aria-label="Search"
    maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  


  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button">
        <img src="/icons/translate.svg" class="book-icon" alt="Languages" />
        한국어
      </a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>

    <ul>
      
      <li>
        <a href="/" class="flex flex-auto">
          English
        </a>
      </li>
      
    </ul>
  </li>
</ul>












  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-14bcba7439f342435c91a57530ca56c8" class="toggle"  />
    <label for="section-14bcba7439f342435c91a57530ca56c8" class="flex">
      <a href="/ko/docs/timeline/" class="">
        타임라인</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cb9657695dbd536c3a44d019c4d6ef98" class="toggle"  />
    <label for="section-cb9657695dbd536c3a44d019c4d6ef98" class="flex">
      <a href="/ko/docs/topdown/" class="">
        Top-Down</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-e7f22db027b1c591ed84a4d4d3a6014f" class="toggle"  />
    <label for="section-e7f22db027b1c591ed84a4d4d3a6014f" class="flex">
      <a href="/ko/docs/bottomup/" class="">
        Bottom-Up</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="toggle"  />
    <label for="section-ce31bba1d1f88a1d133a7e7232f1b4ec" class="flex">
      <a href="/ko/docs/math/" class="">
        수학</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b7c22078b221fa1e5fe6dba5f036e317" class="toggle" checked />
    <label for="section-b7c22078b221fa1e5fe6dba5f036e317" class="flex">
      <a href="/ko/docs/engineering/" class="">
        실무 기술</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="toggle"  />
    <label for="section-41bc70f3b465f23cc6f68dbb9cf6930e" class="flex">
      <a href="/ko/docs/engineering/data/" class="">
        데이터</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/augmentation/" class="">
      데이터 증강</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/formats/" class="">
      데이터 포맷</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/pipeline/" class="">
      데이터 파이프라인</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/data/labeling/" class="">
      레이블링</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-08b7417cd9477b9be27e6c7b561a01bc" class="toggle" checked />
    <label for="section-08b7417cd9477b9be27e6c7b561a01bc" class="flex">
      <a href="/ko/docs/engineering/deployment/" class="">
        배포</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/onnx/" class="">
      ONNX</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/tensorrt/" class="active">
      TensorRT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/serving/" class="">
      모델 서빙</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/deployment/optimization/" class="">
      모델 최적화</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="toggle"  />
    <label for="section-d367cbc8b1f6018b8f46a12cb86b41d6" class="flex">
      <a href="/ko/docs/engineering/hardware/" class="">
        하드웨어</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/camera/" class="">
      카메라</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/lighting/" class="">
      조명</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/engineering/hardware/edge/" class="">
      엣지 디바이스</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-32f2aee524dad19f5100855d6e2014e6" class="toggle"  />
    <label for="section-32f2aee524dad19f5100855d6e2014e6" class="flex">
      <a href="/ko/docs/architecture/" class="">
        아키텍처</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-47444199c3019e8b479623bc90a9bd74" class="toggle"  />
    <label for="section-47444199c3019e8b479623bc90a9bd74" class="flex">
      <a href="/ko/docs/architecture/classical/" class="">
        Classical CV</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/classical/sift-hog/" class="">
      SIFT &amp; HOG</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7bf44239f7ce2c72600738f8c315efe4" class="toggle"  />
    <label for="section-7bf44239f7ce2c72600738f8c315efe4" class="flex">
      <a href="/ko/docs/architecture/cnn/" class="">
        CNN</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/alexnet/" class="">
      AlexNet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/vgg/" class="">
      VGG</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/cnn/resnet/" class="">
      ResNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-274b800b12326ef4e06255cf5f15c308" class="toggle"  />
    <label for="section-274b800b12326ef4e06255cf5f15c308" class="flex">
      <a href="/ko/docs/architecture/detection/" class="">
        Detection</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/faster-rcnn/" class="">
      Faster R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/detection/yolo/" class="">
      YOLO</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7246e67fd6c549f3bd6240d998b23a6f" class="toggle"  />
    <label for="section-7246e67fd6c549f3bd6240d998b23a6f" class="flex">
      <a href="/ko/docs/architecture/segmentation/" class="">
        Segmentation</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/unet/" class="">
      U-Net</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/mask-rcnn/" class="">
      Mask R-CNN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/segmentation/sam/" class="">
      SAM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-3a9f4e0826e1b9e72550db30b2adcad7" class="toggle"  />
    <label for="section-3a9f4e0826e1b9e72550db30b2adcad7" class="flex">
      <a href="/ko/docs/architecture/transformer/" class="">
        Transformer</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/vit/" class="">
      ViT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/transformer/dit/" class="">
      DiT</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2902afa0a62febfd7a5eb243df871e8c" class="toggle"  />
    <label for="section-2902afa0a62febfd7a5eb243df871e8c" class="flex">
      <a href="/ko/docs/architecture/generative/" class="">
        Generative</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/vae/" class="">
      VAE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/gan/" class="">
      GAN</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/stable-diffusion/" class="">
      Stable Diffusion</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/generative/controlnet/" class="">
      ControlNet</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1670f136e85bf39bce97c9ee224d29d9" class="toggle"  />
    <label for="section-1670f136e85bf39bce97c9ee224d29d9" class="flex">
      <a href="/ko/docs/architecture/multimodal/" class="">
        Multimodal</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/clip/" class="">
      CLIP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/multimodal/vlm/" class="">
      VLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc1b0b4545ce791b47ab3fe55b854e56" class="toggle"  />
    <label for="section-fc1b0b4545ce791b47ab3fe55b854e56" class="flex">
      <a href="/ko/docs/architecture/3d/" class="">
        3D Vision</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/nerf/" class="">
      NeRF</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ko/docs/architecture/3d/3dgs/" class="">
      3D Gaussian Splatting</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd575cc6fd4e25b684b552791b7a009a" class="toggle"  />
    <label for="section-cd575cc6fd4e25b684b552791b7a009a" class="flex">
      <a href="/ko/docs/task/" class="">
        태스크</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fb690f19903e1435676f710c928c9ad4" class="toggle"  />
    <label for="section-fb690f19903e1435676f710c928c9ad4" class="flex">
      <a href="/ko/docs/etc/" class="">
        기타 자료</a>
      <img src="/icons/chevron-right.svg" class="book-icon" alt="Expand" />
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>













</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header hidden">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/icons/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>TensorRT</h3>

  <label for="toc-control">
    
    <img src="/icons/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#주요-최적화">주요 최적화</a></li>
    <li><a href="#onnx--tensorrt-변환">ONNX → TensorRT 변환</a>
      <ul>
        <li><a href="#trtexec-cli">trtexec (CLI)</a></li>
        <li><a href="#python-api">Python API</a></li>
      </ul>
    </li>
    <li><a href="#tensorrt-추론">TensorRT 추론</a></li>
    <li><a href="#int8-양자화">INT8 양자화</a>
      <ul>
        <li><a href="#캘리브레이션">캘리브레이션</a></li>
      </ul>
    </li>
    <li><a href="#동적-shape">동적 Shape</a></li>
    <li><a href="#torch2trt-간편한-변환">torch2trt (간편한 변환)</a></li>
    <li><a href="#triton과-통합">Triton과 통합</a>
      <ul>
        <li><a href="#모델-저장소-구조">모델 저장소 구조</a></li>
        <li><a href="#configpbtxt">config.pbtxt</a></li>
      </ul>
    </li>
    <li><a href="#성능-비교">성능 비교</a>
      <ul>
        <li><a href="#벤치마크-예시-resnet-50-rtx-3090">벤치마크 예시 (ResNet-50, RTX 3090)</a></li>
      </ul>
    </li>
    <li><a href="#문제-해결">문제 해결</a>
      <ul>
        <li><a href="#지원되지-않는-연산">지원되지 않는 연산</a></li>
        <li><a href="#정확도-저하-int8">정확도 저하 (INT8)</a></li>
        <li><a href="#메모리-부족">메모리 부족</a></li>
      </ul>
    </li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="tensorrt">TensorRT<a class="anchor" href="#tensorrt">#</a></h1>
<h2 id="개요">개요<a class="anchor" href="#%ea%b0%9c%ec%9a%94">#</a></h2>
<p>TensorRT는 NVIDIA GPU에서 딥러닝 추론을 최적화하는 라이브러리입니다.</p>
<pre tabindex="0"><code>ONNX 모델 → TensorRT 최적화 → 최대 10x 빠른 추론</code></pre><hr>
<h2 id="주요-최적화">주요 최적화<a class="anchor" href="#%ec%a3%bc%ec%9a%94-%ec%b5%9c%ec%a0%81%ed%99%94">#</a></h2>
<table>
  <thead>
      <tr>
          <th>기법</th>
          <th>설명</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Layer Fusion</td>
          <td>여러 레이어를 하나로 병합</td>
      </tr>
      <tr>
          <td>Precision Calibration</td>
          <td>FP32 → FP16/INT8 변환</td>
      </tr>
      <tr>
          <td>Kernel Auto-tuning</td>
          <td>최적의 CUDA 커널 선택</td>
      </tr>
      <tr>
          <td>Dynamic Tensor Memory</td>
          <td>메모리 재사용</td>
      </tr>
      <tr>
          <td>Multi-stream Execution</td>
          <td>병렬 처리</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="onnx--tensorrt-변환">ONNX → TensorRT 변환<a class="anchor" href="#onnx--tensorrt-%eb%b3%80%ed%99%98">#</a></h2>
<h3 id="trtexec-cli">trtexec (CLI)<a class="anchor" href="#trtexec-cli">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># FP16 엔진 생성</span>
</span></span><span style="display:flex;"><span>trtexec --onnx<span style="color:#f92672">=</span>model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --saveEngine<span style="color:#f92672">=</span>model.engine <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --fp16
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INT8 엔진 생성 (캘리브레이션 필요)</span>
</span></span><span style="display:flex;"><span>trtexec --onnx<span style="color:#f92672">=</span>model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --saveEngine<span style="color:#f92672">=</span>model.engine <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --int8 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --calib<span style="color:#f92672">=</span>calibration_cache.bin
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 동적 배치</span>
</span></span><span style="display:flex;"><span>trtexec --onnx<span style="color:#f92672">=</span>model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --saveEngine<span style="color:#f92672">=</span>model.engine <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --minShapes<span style="color:#f92672">=</span>input:1x3x224x224 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --optShapes<span style="color:#f92672">=</span>input:8x3x224x224 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>        --maxShapes<span style="color:#f92672">=</span>input:16x3x224x224</span></span></code></pre></div><h3 id="python-api">Python API<a class="anchor" href="#python-api">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>logger <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Logger(trt<span style="color:#f92672">.</span>Logger<span style="color:#f92672">.</span>WARNING)
</span></span><span style="display:flex;"><span>builder <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Builder(logger)
</span></span><span style="display:flex;"><span>network <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_network(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> int(trt<span style="color:#f92672">.</span>NetworkDefinitionCreationFlag<span style="color:#f92672">.</span>EXPLICIT_BATCH))
</span></span><span style="display:flex;"><span>parser <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>OnnxParser(network, logger)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ONNX 파싱</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;model.onnx&#34;</span>, <span style="color:#e6db74">&#34;rb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>parse(f<span style="color:#f92672">.</span>read())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 빌더 설정</span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_builder_config()
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_memory_pool_limit(trt<span style="color:#f92672">.</span>MemoryPoolType<span style="color:#f92672">.</span>WORKSPACE, <span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> <span style="color:#ae81ff">30</span>)  <span style="color:#75715e"># 1GB</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>FP16)  <span style="color:#75715e"># FP16 활성화</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 엔진 빌드</span>
</span></span><span style="display:flex;"><span>serialized_engine <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>build_serialized_network(network, config)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 저장</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;model.engine&#34;</span>, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(serialized_engine)</span></span></code></pre></div><hr>
<h2 id="tensorrt-추론">TensorRT 추론<a class="anchor" href="#tensorrt-%ec%b6%94%eb%a1%a0">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pycuda.driver <span style="color:#66d9ef">as</span> cuda
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pycuda.autoinit
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 엔진 로드</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;model.engine&#34;</span>, <span style="color:#e6db74">&#34;rb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    runtime <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Runtime(trt<span style="color:#f92672">.</span>Logger(trt<span style="color:#f92672">.</span>Logger<span style="color:#f92672">.</span>WARNING))
</span></span><span style="display:flex;"><span>    engine <span style="color:#f92672">=</span> runtime<span style="color:#f92672">.</span>deserialize_cuda_engine(f<span style="color:#f92672">.</span>read())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context <span style="color:#f92672">=</span> engine<span style="color:#f92672">.</span>create_execution_context()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 메모리 할당</span>
</span></span><span style="display:flex;"><span>input_shape <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>)
</span></span><span style="display:flex;"><span>output_shape <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d_input <span style="color:#f92672">=</span> cuda<span style="color:#f92672">.</span>mem_alloc(np<span style="color:#f92672">.</span>prod(input_shape) <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)  <span style="color:#75715e"># float32</span>
</span></span><span style="display:flex;"><span>d_output <span style="color:#f92672">=</span> cuda<span style="color:#f92672">.</span>mem_alloc(np<span style="color:#f92672">.</span>prod(output_shape) <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 추론</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">infer</span>(input_data):
</span></span><span style="display:flex;"><span>    cuda<span style="color:#f92672">.</span>memcpy_htod(d_input, input_data)
</span></span><span style="display:flex;"><span>    context<span style="color:#f92672">.</span>execute_v2([int(d_input), int(d_output)])
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>empty(output_shape, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>    cuda<span style="color:#f92672">.</span>memcpy_dtoh(output, d_output)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> output
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 사용</span>
</span></span><span style="display:flex;"><span>input_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#f92672">*</span>input_shape)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> infer(input_data)</span></span></code></pre></div><hr>
<h2 id="int8-양자화">INT8 양자화<a class="anchor" href="#int8-%ec%96%91%ec%9e%90%ed%99%94">#</a></h2>
<h3 id="캘리브레이션">캘리브레이션<a class="anchor" href="#%ec%ba%98%eb%a6%ac%eb%b8%8c%eb%a0%88%ec%9d%b4%ec%85%98">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Calibrator</span>(trt<span style="color:#f92672">.</span>IInt8EntropyCalibrator2):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, data_loader, cache_file):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data_loader <span style="color:#f92672">=</span> data_loader
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cache_file <span style="color:#f92672">=</span> cache_file
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 디바이스 메모리 할당</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_input <span style="color:#f92672">=</span> cuda<span style="color:#f92672">.</span>mem_alloc(self<span style="color:#f92672">.</span>data_loader<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">*</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">224</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">224</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_batch_size</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>data_loader<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_batch</span>(self, names):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            batch <span style="color:#f92672">=</span> next(self<span style="color:#f92672">.</span>data_loader)
</span></span><span style="display:flex;"><span>            cuda<span style="color:#f92672">.</span>memcpy_htod(self<span style="color:#f92672">.</span>d_input, batch)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> [int(self<span style="color:#f92672">.</span>d_input)]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">StopIteration</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_calibration_cache</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(self<span style="color:#f92672">.</span>cache_file):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> open(self<span style="color:#f92672">.</span>cache_file, <span style="color:#e6db74">&#34;rb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">return</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_calibration_cache</span>(self, cache):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> open(self<span style="color:#f92672">.</span>cache_file, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>            f<span style="color:#f92672">.</span>write(cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 빌더 설정</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>INT8)
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>int8_calibrator <span style="color:#f92672">=</span> Calibrator(calibration_loader, <span style="color:#e6db74">&#34;calibration.cache&#34;</span>)</span></span></code></pre></div><hr>
<h2 id="동적-shape">동적 Shape<a class="anchor" href="#%eb%8f%99%ec%a0%81-shape">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 프로파일 설정</span>
</span></span><span style="display:flex;"><span>profile <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_optimization_profile()
</span></span><span style="display:flex;"><span>profile<span style="color:#f92672">.</span>set_shape(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;input&#34;</span>,
</span></span><span style="display:flex;"><span>    min<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>),   <span style="color:#75715e"># 최소</span>
</span></span><span style="display:flex;"><span>    opt<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>),   <span style="color:#75715e"># 최적</span>
</span></span><span style="display:flex;"><span>    max<span style="color:#f92672">=</span>(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>)   <span style="color:#75715e"># 최대</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>add_optimization_profile(profile)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 추론 시 shape 설정</span>
</span></span><span style="display:flex;"><span>context<span style="color:#f92672">.</span>set_input_shape(<span style="color:#e6db74">&#34;input&#34;</span>, (batch_size, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>))</span></span></code></pre></div><hr>
<h2 id="torch2trt-간편한-변환">torch2trt (간편한 변환)<a class="anchor" href="#torch2trt-%ea%b0%84%ed%8e%b8%ed%95%9c-%eb%b3%80%ed%99%98">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch2trt <span style="color:#f92672">import</span> torch2trt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> MyModel()<span style="color:#f92672">.</span>cuda()<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>)<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TensorRT 변환</span>
</span></span><span style="display:flex;"><span>model_trt <span style="color:#f92672">=</span> torch2trt(
</span></span><span style="display:flex;"><span>    model,
</span></span><span style="display:flex;"><span>    [x],
</span></span><span style="display:flex;"><span>    fp16_mode<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    max_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 추론</span>
</span></span><span style="display:flex;"><span>y_trt <span style="color:#f92672">=</span> model_trt(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 저장/로드</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>save(model_trt<span style="color:#f92672">.</span>state_dict(), <span style="color:#e6db74">&#34;model_trt.pth&#34;</span>)</span></span></code></pre></div><hr>
<h2 id="triton과-통합">Triton과 통합<a class="anchor" href="#triton%ea%b3%bc-%ed%86%b5%ed%95%a9">#</a></h2>
<h3 id="모델-저장소-구조">모델 저장소 구조<a class="anchor" href="#%eb%aa%a8%eb%8d%b8-%ec%a0%80%ec%9e%a5%ec%86%8c-%ea%b5%ac%ec%a1%b0">#</a></h3>
<pre tabindex="0"><code>model_repository/
└── my_model/
    ├── config.pbtxt
    └── 1/
        └── model.plan  # TensorRT 엔진</code></pre><h3 id="configpbtxt">config.pbtxt<a class="anchor" href="#configpbtxt">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-protobuf" data-lang="protobuf"><span style="display:flex;"><span>name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;my_model&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>platform<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;tensorrt_plan&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>max_batch_size<span style="color:#f92672">:</span> <span style="color:#ae81ff">16</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>input [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;input&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    data_type<span style="color:#f92672">:</span> TYPE_FP32<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    dims<span style="color:#f92672">:</span> [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>output [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    name<span style="color:#f92672">:</span> <span style="color:#e6db74">&#34;output&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    data_type<span style="color:#f92672">:</span> TYPE_FP32<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    dims<span style="color:#f92672">:</span> [<span style="color:#ae81ff">1000</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>instance_group [<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  {<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    count<span style="color:#f92672">:</span> <span style="color:#ae81ff">1</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    kind<span style="color:#f92672">:</span> KIND_GPU<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>    gpus<span style="color:#f92672">:</span> [<span style="color:#ae81ff">0</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>  }<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span>]</span></span></code></pre></div><hr>
<h2 id="성능-비교">성능 비교<a class="anchor" href="#%ec%84%b1%eb%8a%a5-%eb%b9%84%ea%b5%90">#</a></h2>
<h3 id="벤치마크-예시-resnet-50-rtx-3090">벤치마크 예시 (ResNet-50, RTX 3090)<a class="anchor" href="#%eb%b2%a4%ec%b9%98%eb%a7%88%ed%81%ac-%ec%98%88%ec%8b%9c-resnet-50-rtx-3090">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Precision</th>
          <th>Latency</th>
          <th>Throughput</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>PyTorch FP32</td>
          <td>5.2 ms</td>
          <td>192 FPS</td>
      </tr>
      <tr>
          <td>TensorRT FP32</td>
          <td>2.1 ms</td>
          <td>476 FPS</td>
      </tr>
      <tr>
          <td>TensorRT FP16</td>
          <td>1.0 ms</td>
          <td>1000 FPS</td>
      </tr>
      <tr>
          <td>TensorRT INT8</td>
          <td>0.6 ms</td>
          <td>1667 FPS</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="문제-해결">문제 해결<a class="anchor" href="#%eb%ac%b8%ec%a0%9c-%ed%95%b4%ea%b2%b0">#</a></h2>
<h3 id="지원되지-않는-연산">지원되지 않는 연산<a class="anchor" href="#%ec%a7%80%ec%9b%90%eb%90%98%ec%a7%80-%ec%95%8a%eb%8a%94-%ec%97%b0%ec%82%b0">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 플러그인 사용</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 또는 ONNX 레벨에서 수정</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 예: 지원되지 않는 레이어를 여러 지원 레이어로 분해</span></span></span></code></pre></div><h3 id="정확도-저하-int8">정확도 저하 (INT8)<a class="anchor" href="#%ec%a0%95%ed%99%95%eb%8f%84-%ec%a0%80%ed%95%98-int8">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. 캘리브레이션 데이터 확인 (대표성 있는 데이터 사용)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 민감한 레이어는 FP16 유지</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>OBEY_PRECISION_CONSTRAINTS)</span></span></code></pre></div><h3 id="메모리-부족">메모리 부족<a class="anchor" href="#%eb%a9%94%eb%aa%a8%eb%a6%ac-%eb%b6%80%ec%a1%b1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 워크스페이스 크기 조정</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_memory_pool_limit(trt<span style="color:#f92672">.</span>MemoryPoolType<span style="color:#f92672">.</span>WORKSPACE, <span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> <span style="color:#ae81ff">28</span>)  <span style="color:#75715e"># 256MB</span></span></span></code></pre></div><hr>
<h2 id="관련-콘텐츠">관련 콘텐츠<a class="anchor" href="#%ea%b4%80%eb%a0%a8-%ec%bd%98%ed%85%90%ec%b8%a0">#</a></h2>
<ul>
<li><a href="/ko/docs/engineering/deployment/onnx">ONNX</a> - 모델 변환</li>
<li><a href="/ko/docs/engineering/deployment/serving">모델 서빙</a> - Triton 서버</li>
<li><a href="/ko/docs/engineering/deployment/optimization">최적화</a> - 양자화 이론</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">

<div>

</div>

<div>

</div>

</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/ko/docs/engineering/deployment/onnx/" class="flex align-center">
        <img src="/icons/backward.svg" class="book-icon" alt="Backward" />
        <span>ONNX</span>
      </a>
    
    </span>
    <span>
    
      <a href="/ko/docs/engineering/deployment/serving/" class="flex align-center">
        <span>모델 서빙</span>
        <img src="/icons/forward.svg" class="book-icon" alt="Forward" />
      </a>
    
    </span>
  </div>
  


 
        
  
  <div class="book-comments">

</div>
  
 
        
        
  
 
        
  
  
    <script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script>
  

      </footer>

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  
  <aside class="book-toc">
    <div class="book-toc-content">
      
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#개요">개요</a></li>
    <li><a href="#주요-최적화">주요 최적화</a></li>
    <li><a href="#onnx--tensorrt-변환">ONNX → TensorRT 변환</a>
      <ul>
        <li><a href="#trtexec-cli">trtexec (CLI)</a></li>
        <li><a href="#python-api">Python API</a></li>
      </ul>
    </li>
    <li><a href="#tensorrt-추론">TensorRT 추론</a></li>
    <li><a href="#int8-양자화">INT8 양자화</a>
      <ul>
        <li><a href="#캘리브레이션">캘리브레이션</a></li>
      </ul>
    </li>
    <li><a href="#동적-shape">동적 Shape</a></li>
    <li><a href="#torch2trt-간편한-변환">torch2trt (간편한 변환)</a></li>
    <li><a href="#triton과-통합">Triton과 통합</a>
      <ul>
        <li><a href="#모델-저장소-구조">모델 저장소 구조</a></li>
        <li><a href="#configpbtxt">config.pbtxt</a></li>
      </ul>
    </li>
    <li><a href="#성능-비교">성능 비교</a>
      <ul>
        <li><a href="#벤치마크-예시-resnet-50-rtx-3090">벤치마크 예시 (ResNet-50, RTX 3090)</a></li>
      </ul>
    </li>
    <li><a href="#문제-해결">문제 해결</a>
      <ul>
        <li><a href="#지원되지-않는-연산">지원되지 않는 연산</a></li>
        <li><a href="#정확도-저하-int8">정확도 저하 (INT8)</a></li>
        <li><a href="#메모리-부족">메모리 부족</a></li>
      </ul>
    </li>
    <li><a href="#관련-콘텐츠">관련 콘텐츠</a></li>
  </ul>
</nav>



    </div>
  </aside>
  
 
  </main>

  
</body>
</html>




















