[{"id":0,"href":"/ko/docs/architecture/cnn/alexnet/","title":"AlexNet","section":"CNN","content":"AlexNet# 개요# 논문: ImageNet Classification with Deep Convolutional Neural Networks (2012) 저자: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton 핵심 기여: 딥러닝 시대의 시작을 알린 모델. ImageNet 2012에서 압도적 성능으로 우승 왜 중요한가?# AlexNet은 ImageNet에서 top-5 error를 **26% → 16%**로 크게 낮추며 딥러닝의 가능성을 증명했습니다. 이후 모든 컴퓨터 비전 연구가 CNN 기반으로 전환되는 계기가 되었습니다.\n구조# 전체 아키텍처# Input (224×224×3) ↓ Conv1: 96 filters, 11×11, stride 4 → ReLU → LRN → MaxPool ↓ Conv2: 256 filters, 5×5 → ReLU → LRN → MaxPool ↓ Conv3: 384 filters, 3×3 → ReLU ↓ Conv4: 384 filters, 3×3 → ReLU ↓ Conv5: 256 filters, 3×3 → ReLU → MaxPool ↓ FC6: 4096 → ReLU → Dropout ↓ FC7: 4096 → ReLU → Dropout ↓ FC8: 1000 (softmax)주요 특징# 요소 설명 ReLU Sigmoid 대신 사용하여 학습 속도 6배 향상 GPU 학습 2개의 GTX 580 GPU로 병렬 학습 Dropout FC 레이어에 0.5 비율로 적용하여 과적합 방지 LRN Local Response Normalization (현재는 사용 안 함) Data Augmentation 랜덤 크롭, 수평 뒤집기, 색상 변환 핵심 기술 상세# 1. ReLU (Rectified Linear Unit)# $$f(x) = \\max(0, x)$$\n장점:\nSigmoid/tanh보다 계산이 단순 Gradient vanishing 문제 완화 학습 속도 대폭 향상 2. Dropout# 학습 시 뉴런을 랜덤하게 비활성화하여 과적합 방지:\n$$\\hat{y} = \\frac{1}{1-p} \\cdot y \\cdot \\text{mask}$$\n3. Data Augmentation# 원본 256×256 이미지에서:\n224×224 랜덤 크롭 수평 뒤집기 PCA 기반 색상 변환 파라미터 수# 레이어 파라미터 Conv 레이어 ~2.3M FC 레이어 ~58.6M 총합 ~60M 대부분의 파라미터가 FC 레이어에 집중되어 있습니다.\n구현 예시# import torch.nn as nn class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.classifier = nn.Sequential( nn.Dropout(0.5), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x 한계점# 큰 필터 크기 (11×11, 5×5) → 비효율적 LRN은 효과 미미 → 이후 사용 안 함 FC 레이어에 파라미터 집중 → 메모리 비효율 이러한 한계는 VGG와 ResNet에서 개선되었습니다.\n관련 콘텐츠# CNN 기초 - CNN의 기본 구조 VGG - 더 깊은 네트워크 ResNet - Skip Connection Classification - 이미지 분류 태스크 "},{"id":1,"href":"/ko/docs/architecture/classical/","title":"Classical CV","section":"아키텍처","content":"Classical Computer Vision# 딥러닝 이전의 전통적인 컴퓨터 비전 기법들입니다.\n특징# 수작업으로 설계된 특징 추출기 (Hand-crafted features) 수학적으로 해석 가능한 알고리즘 적은 데이터로도 동작 계산 비용이 상대적으로 낮음 주요 기법# SIFT \u0026amp; HOG - 대표적인 특징 추출 기법 "},{"id":2,"href":"/ko/docs/architecture/detection/faster-rcnn/","title":"Faster R-CNN","section":"Detection","content":"Faster R-CNN# 개요# 논문: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015) 저자: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun 핵심 기여: Region Proposal Network(RPN)으로 end-to-end 학습 가능한 detector R-CNN 계보# R-CNN (2014) ↓ ROI Pooling 도입 Fast R-CNN (2015) ↓ RPN 도입 Faster R-CNN (2015) 모델 Region Proposal 속도 R-CNN Selective Search ~47s/image Fast R-CNN Selective Search ~2s/image Faster R-CNN RPN (학습됨) ~0.2s/image 구조# 전체 아키텍처# Input Image ↓ Backbone (ResNet/VGG) → Feature Map ↓ ┌───────────────────────────────────┐ │ Region Proposal Network │ │ Feature Map → Anchors → RoIs │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ RoI Pooling/Align │ │ 다양한 크기 RoI → 고정 크기 특징 │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ Detection Head (R-CNN Head) │ │ Classification + Box Regression │ └───────────────────────────────────┘ ↓ Output: [class, x, y, w, h] 핵심 컴포넌트# 1. Backbone# 이미지에서 특징 맵 추출:\nVGG-16: 원 논문 ResNet-50/101: 현재 주로 사용 FPN: Feature Pyramid Network 추가 시 성능 향상 2. Region Proposal Network (RPN)# Feature map 위에서 객체 후보 영역 생성:\nFeature Map (H×W×C) ↓ 3×3 Conv ↓ ┌─────────────────────────────────┐ │ 1×1 Conv (cls) 1×1 Conv (reg) │ │ 2k scores 4k coordinates │ └─────────────────────────────────┘Anchor Box:\n각 위치에서 k개의 앵커 (기본: 9개) 3가지 크기 × 3가지 비율 = 9 anchors 크기: 128², 256², 512² 비율: 1:1, 1:2, 2:1 3. RoI Pooling / RoI Align# 다양한 크기의 RoI를 고정 크기(예: 7×7)로 변환:\nRoI Pooling: 양자화로 인한 오차 발생 RoI Align: 양자화 없이 bilinear interpolation 사용 (Mask R-CNN에서 제안)\n4. Detection Head# 각 RoI에 대해:\nClassification: softmax로 클래스 예측 Box Regression: bounding box 좌표 미세 조정 학습# Multi-task Loss# $$L = L_{cls} + \\lambda L_{reg}$$\nClassification Loss (Cross-entropy): $$L_{cls} = -\\log p_{c^*}$$\nRegression Loss (Smooth L1): $$L_{reg} = \\sum_{i \\in {x,y,w,h}} \\text{smooth}_{L_1}(t_i - t_i^*)$$\nBox Parameterization# $$t_x = (x - x_a) / w_a, \\quad t_y = (y - y_a) / h_a$$ $$t_w = \\log(w / w_a), \\quad t_h = \\log(h / h_a)$$\nTraining Strategy# RPN과 R-CNN Head를 번갈아 학습 (원 논문) 또는 end-to-end joint training (현재 주로 사용) 구현 예시# import torch import torchvision from torchvision.models.detection import fasterrcnn_resnet50_fpn # 사전 학습된 모델 로드 model = fasterrcnn_resnet50_fpn(pretrained=True) model.eval() # 추론 with torch.no_grad(): predictions = model([image]) # 결과 형태 # predictions[0][\u0026#39;boxes\u0026#39;]: (N, 4) - bounding boxes # predictions[0][\u0026#39;labels\u0026#39;]: (N,) - class labels # predictions[0][\u0026#39;scores\u0026#39;]: (N,) - confidence scores커스텀 클래스로 학습# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # backbone 고정, head만 교체 model = fasterrcnn_resnet50_fpn(pretrained=True) num_classes = 10 # 배경 포함 # Box predictor 교체 in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 성능# COCO Dataset# Backbone mAP mAP@50 VGG-16 21.9 42.7 ResNet-50 35.7 57.0 ResNet-101 37.4 58.8 ResNet-50-FPN 37.0 58.5 한계점# Two-stage 방식으로 속도 제한 작은 객체 검출 어려움 (FPN으로 완화) Anchor 설계에 도메인 지식 필요 관련 콘텐츠# YOLO - One-stage detector Mask R-CNN - Faster R-CNN + segmentation ResNet - 주로 사용되는 backbone IoU \u0026amp; NMS - Detection 핵심 개념 Detection 태스크 - 평가 지표 "},{"id":3,"href":"/ko/docs/architecture/classical/sift-hog/","title":"SIFT \u0026 HOG","section":"Classical CV","content":"SIFT \u0026amp; HOG# 개요# 딥러닝 이전 시대의 대표적인 특징 추출 기법입니다.\n기법 연도 핵심 아이디어 SIFT 2004 스케일 불변 키포인트 검출 HOG 2005 그래디언트 방향 히스토그램 SIFT (Scale-Invariant Feature Transform)# 핵심 아이디어# 이미지의 크기나 회전에 관계없이 동일한 특징점을 찾아내는 알고리즘입니다.\n동작 과정# Scale-space 생성: Gaussian blur를 다양한 스케일로 적용 DoG (Difference of Gaussian): 인접한 스케일 간 차이 계산 키포인트 검출: DoG에서 극값(extrema) 찾기 방향 할당: 키포인트 주변의 그래디언트 방향 계산 디스크립터 생성: 128차원 벡터로 특징 표현 수식# Gaussian blur: $$L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)$$\nDifference of Gaussian: $$D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)$$\n특징# 장점: 스케일, 회전, 조명 변화에 강건 단점: 계산 비용이 높음, 실시간 처리 어려움 HOG (Histogram of Oriented Gradients)# 핵심 아이디어# 이미지를 셀 단위로 나누고, 각 셀에서 그래디언트 방향의 히스토그램을 계산합니다.\n동작 과정# 그래디언트 계산: 각 픽셀에서 x, y 방향 그래디언트 계산 셀 분할: 이미지를 8x8 픽셀 셀로 분할 히스토그램 생성: 각 셀에서 9개 방향(0°~180°)의 히스토그램 계산 블록 정규화: 2x2 셀을 하나의 블록으로 묶어 정규화 특징 벡터 생성: 모든 블록의 히스토그램을 연결 수식# 그래디언트 크기와 방향: $$G = \\sqrt{G_x^2 + G_y^2}$$ $$\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n특징# 장점: 조명 변화에 강건, 계산이 상대적으로 빠름 단점: 스케일 변화에 민감 주요 용도: 보행자 검출 (Dalal \u0026amp; Triggs, 2005) 구현 예시# import cv2 # SIFT sift = cv2.SIFT_create() keypoints, descriptors = sift.detectAndCompute(image, None) # HOG hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) boxes, weights = hog.detectMultiScale(image) 딥러닝과의 비교# 측면 SIFT/HOG CNN 특징 설계 수작업 학습 기반 데이터 요구량 적음 많음 성능 제한적 우수 해석 가능성 높음 낮음 일반화 제한적 우수 현재는 대부분의 태스크에서 딥러닝이 SIFT/HOG를 대체했지만, 특징 매칭이나 엣지 케이스에서는 여전히 사용됩니다.\n관련 콘텐츠# 선형대수 - 그래디언트 계산의 기초 CNN 기초 - SIFT/HOG를 대체한 딥러닝 기법 "},{"id":4,"href":"/ko/docs/architecture/segmentation/unet/","title":"U-Net","section":"Segmentation","content":"U-Net# 개요# 논문: U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) 저자: Olaf Ronneberger, Philipp Fischer, Thomas Brox 핵심 기여: Encoder-Decoder + Skip Connection으로 정밀한 segmentation 핵심 아이디어# \u0026ldquo;수축 경로(encoder)로 context를 잡고, 확장 경로(decoder)로 정밀한 localization\u0026rdquo;\n의료 영상처럼 데이터가 적은 상황에서도 효과적으로 동작합니다.\n구조# 전체 아키텍처# Input (572×572) ↓ ┌───────────────────────────────────────────────────────────┐ │ Contracting Path │ │ [Conv3×3 → ReLU → Conv3×3 → ReLU → MaxPool] × 4 │ │ 64 → 128 → 256 → 512 → 1024 │ └───────────────────────────────────────────────────────────┘ ↓ ┌───────────────────────────────────────────────────────────┐ │ Expanding Path │ │ [UpConv2×2 → Concat(skip) → Conv3×3 → ReLU × 2] × 4 │ │ 1024 → 512 → 256 → 128 → 64 │ └───────────────────────────────────────────────────────────┘ ↓ Conv 1×1 → Output (388×388×num_classes)U자 형태# Input Output ↓ ↑ [Conv]─────────────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]───────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]─────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]──────────────────→[UpConv+Concat] ↓ ↑ [Bottleneck] 핵심 컴포넌트# 1. Contracting Path (Encoder)# 3×3 Conv (unpadded) + ReLU × 2 2×2 Max Pooling (stride 2) 채널 수 2배씩 증가 2. Expanding Path (Decoder)# 2×2 Up-convolution (채널 절반) Skip connection으로 encoder 특징 concat 3×3 Conv + ReLU × 2 3. Skip Connection# Encoder의 고해상도 특징을 decoder로 전달:\n위치 정보 보존 정밀한 경계 복원 구현 예시# import torch import torch.nn as nn class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super().__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): return self.conv(x) class UNet(nn.Module): def __init__(self, in_channels=3, out_channels=1): super().__init__() # Encoder self.enc1 = DoubleConv(in_channels, 64) self.enc2 = DoubleConv(64, 128) self.enc3 = DoubleConv(128, 256) self.enc4 = DoubleConv(256, 512) self.pool = nn.MaxPool2d(2) # Bottleneck self.bottleneck = DoubleConv(512, 1024) # Decoder self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2) self.dec4 = DoubleConv(1024, 512) self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2) self.dec3 = DoubleConv(512, 256) self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2) self.dec2 = DoubleConv(256, 128) self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2) self.dec1 = DoubleConv(128, 64) # Output self.out = nn.Conv2d(64, out_channels, 1) def forward(self, x): # Encoder e1 = self.enc1(x) e2 = self.enc2(self.pool(e1)) e3 = self.enc3(self.pool(e2)) e4 = self.enc4(self.pool(e3)) # Bottleneck b = self.bottleneck(self.pool(e4)) # Decoder + Skip connections d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1)) d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1)) d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1)) d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1)) return self.out(d1) 학습# Loss Function# Binary Segmentation: $$L = \\text{BCE}(y, \\hat{y}) + \\text{Dice Loss}$$\nMulti-class: $$L = \\text{CrossEntropy}(y, \\hat{y})$$\nData Augmentation# 의료 영상은 데이터가 적으므로 augmentation이 중요:\nElastic deformation 회전, 뒤집기 그레이스케일 변환 U-Net 변형# 변형 특징 U-Net++ 중첩된 skip connection Attention U-Net Attention gate 추가 ResUNet ResNet 블록 사용 TransUNet Transformer encoder 활용 분야# 의료 영상: 세포, 종양, 장기 segmentation 위성 영상: 건물, 도로 추출 자율주행: 도로 영역 분할 생성 모델: Stable Diffusion의 구조에 영향 관련 콘텐츠# Transposed Convolution - Upsampling 원리 ResNet - Skip connection 아이디어 Mask R-CNN - Instance segmentation Segmentation 태스크 - 평가 지표 "},{"id":5,"href":"/ko/docs/timeline/","title":"타임라인","section":"Docs","content":"타임라인으로 배우는 Vision# 연대순으로 Computer Vision의 발전 과정을 따라가며 학습합니다.\n~2012: Classical Computer Vision# 선형대수 기하학 SIFT \u0026amp; HOG 2012-2015: CNN 시대의 시작# Convolution Backpropagation AlexNet VGG ResNet Classification 2015-2017: Detection \u0026amp; Segmentation# IoU \u0026amp; NMS YOLO Faster R-CNN U-Net Detection Segmentation 2017-2019: Attention의 등장# Attention Transformer 2020-2021: Vision Transformer \u0026amp; CLIP# ViT CLIP Contrastive Learning Self-supervised Learning 2021-2022: Diffusion 시대# Diffusion Process Stable Diffusion Generation 2023: Controllable Generation \u0026amp; SAM# ControlNet SAM 2023-2024: VLM \u0026amp; DiT# VLM DiT Vision-Language 2024-현재: 3D \u0026amp; Video Generation# NeRF 3D Gaussian Splatting 3D Vision "},{"id":6,"href":"/ko/docs/architecture/cnn/","title":"CNN","section":"아키텍처","content":"CNN (Convolutional Neural Network)# 개요# CNN은 이미지 처리에 특화된 신경망 구조로, 2012년 AlexNet의 등장 이후 컴퓨터 비전의 핵심이 되었습니다.\n핵심 구성 요소# 1. Convolution Layer# 필터(커널)를 이미지에 슬라이딩하며 특징을 추출합니다.\n$$\\text{Output}(i,j) = \\sum_{m}\\sum_{n} \\text{Input}(i+m, j+n) \\cdot \\text{Kernel}(m,n)$$\n특징:\n파라미터 공유로 효율적 지역적 패턴 학습 Translation equivariance 2. Pooling Layer# 공간 해상도를 줄이고 위치 불변성을 제공합니다.\nMax Pooling: 영역 내 최댓값 선택 Average Pooling: 영역 내 평균값 계산 3. Activation Function# 비선형성을 추가합니다.\n$$\\text{ReLU}(x) = \\max(0, x)$$\n4. Fully Connected Layer# 최종 분류를 위한 전결합 층입니다.\n전형적인 CNN 구조# Input → [Conv → ReLU → Pool] × N → Flatten → FC → Output 주요 발전 과정# 연도 모델 핵심 기여 2012 AlexNet GPU 학습, ReLU, Dropout 2014 VGG 깊은 네트워크, 3x3 필터 2015 ResNet Skip Connection, 152층 구현 예시# import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self, num_classes=10): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), ) self.classifier = nn.Sequential( nn.Flatten(), nn.Linear(128 * 8 * 8, 256), nn.ReLU(), nn.Linear(256, num_classes), ) def forward(self, x): x = self.features(x) x = self.classifier(x) return x 관련 콘텐츠# Convolution - Convolution 연산의 수학적 이해 Backpropagation - CNN 학습 원리 Classification - CNN의 대표적 응용 "},{"id":7,"href":"/ko/docs/topdown/","title":"Top-Down","section":"Docs","content":"Top-Down 학습# 문제를 정하고, 필요한 지식을 찾아갑니다.\nImage Classification# Cross-entropy Loss CNN 기초 ResNet Classification Object Detection# IoU \u0026amp; NMS Anchor Box Faster R-CNN YOLO Detection Segmentation# Transposed Convolution U-Net Mask R-CNN Segmentation Image Generation# 확률분포 VAE GAN Diffusion Stable Diffusion ControlNet DiT Generation Vision-Language (VLM)# Contrastive Learning CLIP VLM Vision-Language 3D Vision# 카메라 모델 NeRF 3D Vision 모델 배포# Quantization Deployment "},{"id":8,"href":"/ko/docs/architecture/cnn/vgg/","title":"VGG","section":"CNN","content":"VGG# 개요# 논문: Very Deep Convolutional Networks for Large-Scale Image Recognition (2014) 저자: Karen Simonyan, Andrew Zisserman (Oxford) 핵심 기여: 작은 필터(3×3)를 깊게 쌓는 것이 효과적임을 증명 핵심 아이디어# \u0026ldquo;3×3 필터를 여러 번 쌓으면 큰 필터와 같은 receptive field를 가지면서 파라미터는 더 적다\u0026rdquo;\n예시: 3×3 두 번 = 5×5 한 번의 receptive field\n방식 Receptive Field 파라미터 5×5 한 번 5×5 25C² 3×3 두 번 5×5 18C² 구조# VGG-16 아키텍처# Input (224×224×3) ↓ Block 1: [Conv3-64] × 2 → MaxPool ↓ Block 2: [Conv3-128] × 2 → MaxPool ↓ Block 3: [Conv3-256] × 3 → MaxPool ↓ Block 4: [Conv3-512] × 3 → MaxPool ↓ Block 5: [Conv3-512] × 3 → MaxPool ↓ FC: 4096 → 4096 → 1000VGG 변형# 모델 레이어 수 파라미터 VGG-11 11 133M VGG-13 13 133M VGG-16 16 138M VGG-19 19 144M 설계 원칙# 1. 작은 필터 (3×3)# 모든 Conv 레이어에서 3×3 필터 사용:\n더 많은 비선형성 (ReLU 더 많이 적용) 파라미터 효율성 더 깊은 네트워크 가능 2. 일관된 구조# 모든 Conv: 3×3, stride 1, padding 1 모든 Pool: 2×2, stride 2 채널 수: 64 → 128 → 256 → 512 → 512 3. 깊이의 중요성# 레이어를 깊게 쌓을수록 성능 향상:\nVGG-11: 10.4% top-5 error VGG-16: 7.4% top-5 error VGG-19: 7.3% top-5 error 구현 예시# import torch.nn as nn def make_layers(cfg): layers = [] in_channels = 3 for v in cfg: if v == \u0026#39;M\u0026#39;: layers.append(nn.MaxPool2d(kernel_size=2, stride=2)) else: layers.append(nn.Conv2d(in_channels, v, kernel_size=3, padding=1)) layers.append(nn.ReLU(inplace=True)) in_channels = v return nn.Sequential(*layers) # VGG-16 config cfg_vgg16 = [64, 64, \u0026#39;M\u0026#39;, 128, 128, \u0026#39;M\u0026#39;, 256, 256, 256, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;] class VGG16(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = make_layers(cfg_vgg16) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x 특징 추출기로서의 VGG# VGG의 중간 레이어는 범용적인 특징 추출기로 활용됩니다:\n# Perceptual Loss에서 VGG 특징 사용 vgg = models.vgg16(pretrained=True).features[:16] # relu3_3까지 def perceptual_loss(pred, target): pred_features = vgg(pred) target_features = vgg(target) return F.mse_loss(pred_features, target_features)활용 분야:\nStyle Transfer Perceptual Loss Feature Matching 한계점# 파라미터 과다: 138M 파라미터 (대부분 FC 레이어) 메모리 사용량: 학습 시 많은 GPU 메모리 필요 학습 어려움: 깊은 네트워크에서 gradient vanishing 이러한 한계는 ResNet의 Skip Connection으로 해결되었습니다.\n관련 콘텐츠# AlexNet - 이전 모델 ResNet - Skip Connection으로 깊이 문제 해결 CNN 기초 - Convolution의 원리 "},{"id":9,"href":"/ko/docs/architecture/detection/yolo/","title":"YOLO","section":"Detection","content":"YOLO (You Only Look Once)# 개요# 논문: You Only Look Once: Unified, Real-Time Object Detection (2016) 저자: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi 핵심 기여: Detection을 단일 회귀 문제로 재정의, 실시간 처리 가능 핵심 아이디어# \u0026ldquo;Detection을 한 번의 forward pass로 해결\u0026rdquo;\nTwo-stage detector와 달리, 이미지를 그리드로 나누고 각 그리드에서 직접 박스와 클래스를 예측합니다.\nYOLO 버전 변화# 버전 연도 핵심 개선 mAP (COCO) FPS YOLOv1 2016 첫 one-stage detector 63.4 (VOC) 45 YOLOv2 2016 Batch Norm, Anchor 78.6 (VOC) 67 YOLOv3 2018 Multi-scale, FPN 33.0 30 YOLOv4 2020 CSPNet, Mish 43.5 65 YOLOv5 2020 PyTorch, 편의성 50.7 140 YOLOv8 2023 Anchor-free, SOTA 53.9 100+ YOLOv1 구조# 동작 원리# 1. 이미지를 S×S 그리드로 분할 (S=7) 2. 각 그리드 셀이 B개의 bounding box 예측 (B=2) 3. 각 박스: (x, y, w, h, confidence) 4. 각 그리드 셀이 C개 클래스 확률 예측 (C=20) Output: S × S × (B×5 + C) = 7 × 7 × 30네트워크 구조# Input (448×448×3) ↓ 24 Conv Layers (GoogLeNet inspired) ↓ 2 FC Layers ↓ Output (7×7×30)Confidence Score# $$\\text{Confidence} = P(\\text{Object}) \\times \\text{IoU}_{pred}^{truth}$$\nLoss Function# $$L = \\lambda_{coord} L_{coord} + L_{obj} + \\lambda_{noobj} L_{noobj} + L_{class}$$\n$\\lambda_{coord} = 5$: 좌표 loss 가중치 $\\lambda_{noobj} = 0.5$: 객체 없는 셀의 loss 가중치 YOLOv3 구조# 개선점# Multi-scale Prediction: 3가지 스케일에서 예측 FPN-like Structure: 다양한 크기의 객체 검출 Anchor Boxes: 9개의 anchor (3 scales × 3 ratios) Darknet-53: ResNet-like backbone 아키텍처# Input (416×416) ↓ Darknet-53 Backbone ↓ ┌─────────────────────────────────────┐ │ FPN-like Neck │ │ 52×52 (small) + 26×26 + 13×13 (large) │ └─────────────────────────────────────┘ ↓ 3 Detection Heads ↓ Output: - 13×13×255 (대형 객체) - 26×26×255 (중형 객체) - 52×52×255 (소형 객체) 255 = 3 anchors × (4 coords + 1 obj + 80 classes) YOLOv8 구조 (최신)# 주요 특징# Anchor-free: anchor box 없이 직접 예측 Decoupled Head: classification과 regression 분리 C2f Module: CSP + 더 효율적인 특징 융합 Task-specific Heads: detection, segmentation, pose 등 모델 변형# 모델 파라미터 mAP FPS (V100) YOLOv8n 3.2M 37.3 430 YOLOv8s 11.2M 44.9 350 YOLOv8m 25.9M 50.2 220 YOLOv8l 43.7M 52.9 160 YOLOv8x 68.2M 53.9 100 구현 예시# YOLOv8 (Ultralytics)# from ultralytics import YOLO # 모델 로드 model = YOLO(\u0026#39;yolov8n.pt\u0026#39;) # nano 모델 # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 확인 for result in results: boxes = result.boxes # Bounding boxes print(boxes.xyxy) # x1, y1, x2, y2 형식 print(boxes.conf) # confidence scores print(boxes.cls) # class indices # 학습 model.train(data=\u0026#39;coco128.yaml\u0026#39;, epochs=100) # Export model.export(format=\u0026#39;onnx\u0026#39;)YOLOv5 (torch hub)# import torch # 모델 로드 model = torch.hub.load(\u0026#39;ultralytics/yolov5\u0026#39;, \u0026#39;yolov5s\u0026#39;, pretrained=True) # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 출력 results.print() results.show() # pandas DataFrame으로 결과 df = results.pandas().xyxy[0] YOLO vs Faster R-CNN# 측면 YOLO Faster R-CNN 방식 One-stage Two-stage 속도 빠름 (30-100+ FPS) 느림 (~7 FPS) 정확도 상대적으로 낮음 높음 작은 객체 어려움 상대적으로 좋음 실시간 가능 어려움 구현 단순 복잡 실제 활용# 자율주행: 실시간 객체 검출 CCTV 분석: 사람/차량 검출 산업 자동화: 제품 불량 검출 모바일: 경량화 모델 (YOLOv8n) 관련 콘텐츠# Faster R-CNN - Two-stage detector IoU \u0026amp; NMS - Detection 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표 "},{"id":10,"href":"/ko/docs/bottomup/","title":"Bottom-Up","section":"Docs","content":"Bottom-Up 학습# 기초 개념부터 차근차근 쌓아올립니다.\nLevel 1: 수학 기초# 선형대수 미적분 \u0026amp; Chain Rule 확률/통계 Level 2: 딥러닝 기초# Convolution Backpropagation Loss Functions Optimization Level 3: 기본 아키텍처# CNN 기초 AlexNet VGG ResNet Level 4: 기본 태스크# Classification Detection Segmentation Level 5: 고급 개념# Attention Transformer ViT Level 6: 생성 모델# Diffusion Process Stable Diffusion ControlNet DiT Generation Level 7: Multimodal \u0026amp; 3D# Contrastive Learning CLIP VLM Vision-Language NeRF 3D Vision "},{"id":11,"href":"/ko/docs/architecture/detection/","title":"Detection","section":"아키텍처","content":"Object Detection 모델# 이미지에서 객체의 위치(bounding box)와 클래스를 동시에 예측하는 모델들입니다.\n두 가지 접근 방식# Two-Stage Detector# Region Proposal: 객체가 있을 법한 영역 제안 Classification: 각 영역을 분류 특징: 정확도 높음, 속도 느림\nR-CNN → Fast R-CNN → Faster R-CNN One-Stage Detector# 한 번에 위치와 클래스를 예측\n특징: 속도 빠름, 실시간 가능\nYOLO SSD RetinaNet 모델 비교# 모델 유형 FPS mAP (COCO) 특징 Faster R-CNN Two-stage ~7 42.0 높은 정확도 YOLOv3 One-stage ~30 33.0 실시간 YOLOv8 One-stage ~100 53.9 SOTA DETR Transformer ~28 42.0 End-to-end 관련 콘텐츠# IoU \u0026amp; NMS - Detection의 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표, 데이터셋 "},{"id":12,"href":"/ko/docs/architecture/cnn/resnet/","title":"ResNet","section":"CNN","content":"ResNet (Residual Network)# 개요# 논문: Deep Residual Learning for Image Recognition (2015) 저자: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research) 핵심 기여: Skip Connection으로 100+ 레이어 학습 가능하게 함 해결한 문제: Degradation Problem# 네트워크가 깊어지면 성능이 오히려 떨어지는 현상:\n20-layer: 7.4% error 56-layer: 8.5% error ← 더 깊은데 성능이 나빠짐이는 과적합이 아니라 최적화의 어려움 때문입니다.\n핵심 아이디어: Residual Learning# Skip Connection (Shortcut)# 기존 학습: $$H(x) = F(x)$$\nResidual 학습: $$H(x) = F(x) + x$$\n여기서 $F(x)$는 **잔차(residual)**를 학습합니다.\n직관# 입력을 그대로 출력에 더함 네트워크는 \u0026ldquo;변화량\u0026quot;만 학습 Identity mapping이 쉬워짐 → 깊은 네트워크 학습 가능 구조# Basic Block (ResNet-18/34)# x → Conv3×3 → BN → ReLU → Conv3×3 → BN → (+x) → ReLU └─────────────────────────────────────┘ skip connectionBottleneck Block (ResNet-50/101/152)# x → Conv1×1 → BN → ReLU → Conv3×3 → BN → ReLU → Conv1×1 → BN → (+x) → ReLU └────────────────────────────────────────────────────────────┘ skip connection1×1 Conv로 채널을 줄였다가 늘려 계산량 감소.\nResNet 변형# 모델 레이어 파라미터 Top-5 Error ResNet-18 18 11.7M 10.92% ResNet-34 34 21.8M 9.46% ResNet-50 50 25.6M 7.13% ResNet-101 101 44.5M 6.44% ResNet-152 152 60.2M 6.16% 전체 아키텍처 (ResNet-50)# Input (224×224×3) ↓ Conv 7×7, 64, stride 2 ↓ MaxPool 3×3, stride 2 ↓ Stage 1: [Bottleneck(64, 256)] × 3 ↓ Stage 2: [Bottleneck(128, 512)] × 4 ↓ Stage 3: [Bottleneck(256, 1024)] × 6 ↓ Stage 4: [Bottleneck(512, 2048)] × 3 ↓ Global Average Pool ↓ FC → 1000 구현 예시# import torch.nn as nn class BasicBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) # Skip connection (차원 맞추기) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) out += identity # Skip connection out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, in_channels, mid_channels, stride=1): super().__init__() out_channels = mid_channels * self.expansion self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False) self.bn1 = nn.BatchNorm2d(mid_channels) self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, stride, 1, bias=False) self.bn2 = nn.BatchNorm2d(mid_channels) self.conv3 = nn.Conv2d(mid_channels, out_channels, 1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += identity out = self.relu(out) return out ResNet의 영향# Backbone으로 널리 사용# Object Detection: Faster R-CNN Segmentation: Mask R-CNN 대부분의 Vision 태스크에서 기본 backbone 후속 연구# ResNeXt: 그룹 컨볼루션 추가 DenseNet: 모든 레이어를 연결 SE-Net: 채널 attention 추가 EfficientNet: 최적 스케일링 왜 Skip Connection이 효과적인가?# 1. Gradient Flow 개선# $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\left(1 + \\frac{\\partial F}{\\partial x}\\right)$$\n항상 1이 더해지므로 gradient가 소실되지 않습니다.\n2. Ensemble 효과# Skip connection은 다양한 깊이의 경로를 만들어 앙상블과 유사한 효과를 냅니다.\n관련 콘텐츠# VGG - 깊이의 한계를 보여준 모델 CNN 기초 - Convolution의 원리 Detection - ResNet을 backbone으로 사용 ViT - ResNet을 대체하는 새로운 패러다임 "},{"id":13,"href":"/ko/docs/architecture/segmentation/","title":"Segmentation","section":"아키텍처","content":"Segmentation 모델# 이미지의 각 픽셀에 레이블을 할당하는 모델들입니다.\nSegmentation 유형# Semantic Segmentation# 각 픽셀을 클래스로 분류 (객체 구분 X)\nInput: 고양이 2마리 이미지 Output: 모든 고양이 픽셀 → \u0026#34;고양이\u0026#34; 클래스Instance Segmentation# 각 객체 인스턴스를 구분\nInput: 고양이 2마리 이미지 Output: 고양이1 픽셀, 고양이2 픽셀 (분리)Panoptic Segmentation# Semantic + Instance 통합\nOutput: 배경(semantic) + 각 객체(instance) 주요 모델# 모델 유형 연도 특징 U-Net Semantic 2015 Encoder-Decoder, 의료영상 Mask R-CNN Instance 2017 Faster R-CNN + Mask DeepLab Semantic 2017 Atrous Convolution SAM Promptable 2023 Foundation Model 관련 콘텐츠# Transposed Convolution - Upsampling 기법 Segmentation 태스크 - 평가 지표, 데이터셋 "},{"id":14,"href":"/ko/docs/math/","title":"수학","section":"Docs","content":"수학 기초# Vision 엔지니어에게 필요한 수학 지식입니다.\n"},{"id":15,"href":"/ko/docs/architecture/","title":"아키텍처","section":"Docs","content":"아키텍처# Computer Vision 모델 및 네트워크 구조입니다.\n카테고리# Classical CV - SIFT, HOG 등 전통적 특징 추출 CNN - Convolutional Neural Network 계열 Detection - 객체 탐지 모델 Segmentation - 세그멘테이션 모델 Transformer - Transformer 기반 모델 Generative - 생성 모델 Multimodal - 멀티모달 모델 3D - 3D 비전 모델 "},{"id":16,"href":"/ko/docs/task/","title":"태스크","section":"Docs","content":"태스크# 문제 정의, 평가지표, 데이터셋입니다.\n"},{"id":17,"href":"/ko/docs/etc/","title":"기타 자료","section":"Docs","content":"논문 \u0026amp; 자료# 필수 논문과 학습 자료 모음입니다.\n"}]