[{"id":0,"href":"/ko/docs/architecture/cnn/alexnet/","title":"AlexNet","section":"CNN","content":"AlexNet# 개요# 논문: ImageNet Classification with Deep Convolutional Neural Networks (2012) 저자: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton 핵심 기여: 딥러닝 시대의 시작을 알린 모델. ImageNet 2012에서 압도적 성능으로 우승 왜 중요한가?# AlexNet은 ImageNet에서 top-5 error를 **26% → 16%**로 크게 낮추며 딥러닝의 가능성을 증명했습니다. 이후 모든 컴퓨터 비전 연구가 CNN 기반으로 전환되는 계기가 되었습니다.\n구조# 전체 아키텍처# Input (224×224×3) ↓ Conv1: 96 filters, 11×11, stride 4 → ReLU → LRN → MaxPool ↓ Conv2: 256 filters, 5×5 → ReLU → LRN → MaxPool ↓ Conv3: 384 filters, 3×3 → ReLU ↓ Conv4: 384 filters, 3×3 → ReLU ↓ Conv5: 256 filters, 3×3 → ReLU → MaxPool ↓ FC6: 4096 → ReLU → Dropout ↓ FC7: 4096 → ReLU → Dropout ↓ FC8: 1000 (softmax)주요 특징# 요소 설명 ReLU Sigmoid 대신 사용하여 학습 속도 6배 향상 GPU 학습 2개의 GTX 580 GPU로 병렬 학습 Dropout FC 레이어에 0.5 비율로 적용하여 과적합 방지 LRN Local Response Normalization (현재는 사용 안 함) Data Augmentation 랜덤 크롭, 수평 뒤집기, 색상 변환 핵심 기술 상세# 1. ReLU (Rectified Linear Unit)# $$f(x) = \\max(0, x)$$\n장점:\nSigmoid/tanh보다 계산이 단순 Gradient vanishing 문제 완화 학습 속도 대폭 향상 2. Dropout# 학습 시 뉴런을 랜덤하게 비활성화하여 과적합 방지:\n$$\\hat{y} = \\frac{1}{1-p} \\cdot y \\cdot \\text{mask}$$\n3. Data Augmentation# 원본 256×256 이미지에서:\n224×224 랜덤 크롭 수평 뒤집기 PCA 기반 색상 변환 파라미터 수# 레이어 파라미터 Conv 레이어 ~2.3M FC 레이어 ~58.6M 총합 ~60M 대부분의 파라미터가 FC 레이어에 집중되어 있습니다.\n구현 예시# import torch.nn as nn class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.classifier = nn.Sequential( nn.Dropout(0.5), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x 한계점# 큰 필터 크기 (11×11, 5×5) → 비효율적 LRN은 효과 미미 → 이후 사용 안 함 FC 레이어에 파라미터 집중 → 메모리 비효율 이러한 한계는 VGG와 ResNet에서 개선되었습니다.\n관련 콘텐츠# CNN 기초 - CNN의 기본 구조 VGG - 더 깊은 네트워크 ResNet - Skip Connection Classification - 이미지 분류 태스크 "},{"id":1,"href":"/ko/docs/architecture/classical/","title":"Classical CV","section":"아키텍처","content":"Classical Computer Vision# 딥러닝 이전의 전통적인 컴퓨터 비전 기법들입니다.\n특징# 수작업으로 설계된 특징 추출기 (Hand-crafted features) 수학적으로 해석 가능한 알고리즘 적은 데이터로도 동작 계산 비용이 상대적으로 낮음 주요 기법# SIFT \u0026amp; HOG - 대표적인 특징 추출 기법 "},{"id":2,"href":"/ko/docs/architecture/detection/faster-rcnn/","title":"Faster R-CNN","section":"Detection","content":"Faster R-CNN# 개요# 논문: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015) 저자: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun 핵심 기여: Region Proposal Network(RPN)으로 end-to-end 학습 가능한 detector R-CNN 계보# R-CNN (2014) ↓ ROI Pooling 도입 Fast R-CNN (2015) ↓ RPN 도입 Faster R-CNN (2015) 모델 Region Proposal 속도 R-CNN Selective Search ~47s/image Fast R-CNN Selective Search ~2s/image Faster R-CNN RPN (학습됨) ~0.2s/image 구조# 전체 아키텍처# Input Image ↓ Backbone (ResNet/VGG) → Feature Map ↓ ┌───────────────────────────────────┐ │ Region Proposal Network │ │ Feature Map → Anchors → RoIs │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ RoI Pooling/Align │ │ 다양한 크기 RoI → 고정 크기 특징 │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ Detection Head (R-CNN Head) │ │ Classification + Box Regression │ └───────────────────────────────────┘ ↓ Output: [class, x, y, w, h] 핵심 컴포넌트# 1. Backbone# 이미지에서 특징 맵 추출:\nVGG-16: 원 논문 ResNet-50/101: 현재 주로 사용 FPN: Feature Pyramid Network 추가 시 성능 향상 2. Region Proposal Network (RPN)# Feature map 위에서 객체 후보 영역 생성:\nFeature Map (H×W×C) ↓ 3×3 Conv ↓ ┌─────────────────────────────────┐ │ 1×1 Conv (cls) 1×1 Conv (reg) │ │ 2k scores 4k coordinates │ └─────────────────────────────────┘Anchor Box:\n각 위치에서 k개의 앵커 (기본: 9개) 3가지 크기 × 3가지 비율 = 9 anchors 크기: 128², 256², 512² 비율: 1:1, 1:2, 2:1 3. RoI Pooling / RoI Align# 다양한 크기의 RoI를 고정 크기(예: 7×7)로 변환:\nRoI Pooling: 양자화로 인한 오차 발생 RoI Align: 양자화 없이 bilinear interpolation 사용 (Mask R-CNN에서 제안)\n4. Detection Head# 각 RoI에 대해:\nClassification: softmax로 클래스 예측 Box Regression: bounding box 좌표 미세 조정 학습# Multi-task Loss# $$L = L_{cls} + \\lambda L_{reg}$$\nClassification Loss (Cross-entropy): $$L_{cls} = -\\log p_{c^*}$$\nRegression Loss (Smooth L1): $$L_{reg} = \\sum_{i \\in {x,y,w,h}} \\text{smooth}_{L_1}(t_i - t_i^*)$$\nBox Parameterization# $$t_x = (x - x_a) / w_a, \\quad t_y = (y - y_a) / h_a$$ $$t_w = \\log(w / w_a), \\quad t_h = \\log(h / h_a)$$\nTraining Strategy# RPN과 R-CNN Head를 번갈아 학습 (원 논문) 또는 end-to-end joint training (현재 주로 사용) 구현 예시# import torch import torchvision from torchvision.models.detection import fasterrcnn_resnet50_fpn # 사전 학습된 모델 로드 model = fasterrcnn_resnet50_fpn(pretrained=True) model.eval() # 추론 with torch.no_grad(): predictions = model([image]) # 결과 형태 # predictions[0][\u0026#39;boxes\u0026#39;]: (N, 4) - bounding boxes # predictions[0][\u0026#39;labels\u0026#39;]: (N,) - class labels # predictions[0][\u0026#39;scores\u0026#39;]: (N,) - confidence scores커스텀 클래스로 학습# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # backbone 고정, head만 교체 model = fasterrcnn_resnet50_fpn(pretrained=True) num_classes = 10 # 배경 포함 # Box predictor 교체 in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 성능# COCO Dataset# Backbone mAP mAP@50 VGG-16 21.9 42.7 ResNet-50 35.7 57.0 ResNet-101 37.4 58.8 ResNet-50-FPN 37.0 58.5 한계점# Two-stage 방식으로 속도 제한 작은 객체 검출 어려움 (FPN으로 완화) Anchor 설계에 도메인 지식 필요 관련 콘텐츠# YOLO - One-stage detector Mask R-CNN - Faster R-CNN + segmentation ResNet - 주로 사용되는 backbone IoU \u0026amp; NMS - Detection 핵심 개념 Detection 태스크 - 평가 지표 "},{"id":3,"href":"/ko/docs/architecture/classical/sift-hog/","title":"SIFT \u0026 HOG","section":"Classical CV","content":"SIFT \u0026amp; HOG# 개요# 딥러닝 이전 시대의 대표적인 특징 추출 기법입니다.\n기법 연도 핵심 아이디어 SIFT 2004 스케일 불변 키포인트 검출 HOG 2005 그래디언트 방향 히스토그램 SIFT (Scale-Invariant Feature Transform)# 핵심 아이디어# 이미지의 크기나 회전에 관계없이 동일한 특징점을 찾아내는 알고리즘입니다.\n동작 과정# Scale-space 생성: Gaussian blur를 다양한 스케일로 적용 DoG (Difference of Gaussian): 인접한 스케일 간 차이 계산 키포인트 검출: DoG에서 극값(extrema) 찾기 방향 할당: 키포인트 주변의 그래디언트 방향 계산 디스크립터 생성: 128차원 벡터로 특징 표현 수식# Gaussian blur: $$L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)$$\nDifference of Gaussian: $$D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)$$\n특징# 장점: 스케일, 회전, 조명 변화에 강건 단점: 계산 비용이 높음, 실시간 처리 어려움 HOG (Histogram of Oriented Gradients)# 핵심 아이디어# 이미지를 셀 단위로 나누고, 각 셀에서 그래디언트 방향의 히스토그램을 계산합니다.\n동작 과정# 그래디언트 계산: 각 픽셀에서 x, y 방향 그래디언트 계산 셀 분할: 이미지를 8x8 픽셀 셀로 분할 히스토그램 생성: 각 셀에서 9개 방향(0°~180°)의 히스토그램 계산 블록 정규화: 2x2 셀을 하나의 블록으로 묶어 정규화 특징 벡터 생성: 모든 블록의 히스토그램을 연결 수식# 그래디언트 크기와 방향: $$G = \\sqrt{G_x^2 + G_y^2}$$ $$\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n특징# 장점: 조명 변화에 강건, 계산이 상대적으로 빠름 단점: 스케일 변화에 민감 주요 용도: 보행자 검출 (Dalal \u0026amp; Triggs, 2005) 구현 예시# import cv2 # SIFT sift = cv2.SIFT_create() keypoints, descriptors = sift.detectAndCompute(image, None) # HOG hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) boxes, weights = hog.detectMultiScale(image) 딥러닝과의 비교# 측면 SIFT/HOG CNN 특징 설계 수작업 학습 기반 데이터 요구량 적음 많음 성능 제한적 우수 해석 가능성 높음 낮음 일반화 제한적 우수 현재는 대부분의 태스크에서 딥러닝이 SIFT/HOG를 대체했지만, 특징 매칭이나 엣지 케이스에서는 여전히 사용됩니다.\n관련 콘텐츠# 선형대수 - 그래디언트 계산의 기초 CNN 기초 - SIFT/HOG를 대체한 딥러닝 기법 "},{"id":4,"href":"/ko/docs/architecture/segmentation/unet/","title":"U-Net","section":"Segmentation","content":"U-Net# 개요# 논문: U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) 저자: Olaf Ronneberger, Philipp Fischer, Thomas Brox 핵심 기여: Encoder-Decoder + Skip Connection으로 정밀한 segmentation 핵심 아이디어# \u0026ldquo;수축 경로(encoder)로 context를 잡고, 확장 경로(decoder)로 정밀한 localization\u0026rdquo;\n의료 영상처럼 데이터가 적은 상황에서도 효과적으로 동작합니다.\n구조# 전체 아키텍처# Input (572×572) ↓ ┌───────────────────────────────────────────────────────────┐ │ Contracting Path │ │ [Conv3×3 → ReLU → Conv3×3 → ReLU → MaxPool] × 4 │ │ 64 → 128 → 256 → 512 → 1024 │ └───────────────────────────────────────────────────────────┘ ↓ ┌───────────────────────────────────────────────────────────┐ │ Expanding Path │ │ [UpConv2×2 → Concat(skip) → Conv3×3 → ReLU × 2] × 4 │ │ 1024 → 512 → 256 → 128 → 64 │ └───────────────────────────────────────────────────────────┘ ↓ Conv 1×1 → Output (388×388×num_classes)U자 형태# Input Output ↓ ↑ [Conv]─────────────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]───────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]─────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]──────────────────→[UpConv+Concat] ↓ ↑ [Bottleneck] 핵심 컴포넌트# 1. Contracting Path (Encoder)# 3×3 Conv (unpadded) + ReLU × 2 2×2 Max Pooling (stride 2) 채널 수 2배씩 증가 2. Expanding Path (Decoder)# 2×2 Up-convolution (채널 절반) Skip connection으로 encoder 특징 concat 3×3 Conv + ReLU × 2 3. Skip Connection# Encoder의 고해상도 특징을 decoder로 전달:\n위치 정보 보존 정밀한 경계 복원 구현 예시# import torch import torch.nn as nn class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super().__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): return self.conv(x) class UNet(nn.Module): def __init__(self, in_channels=3, out_channels=1): super().__init__() # Encoder self.enc1 = DoubleConv(in_channels, 64) self.enc2 = DoubleConv(64, 128) self.enc3 = DoubleConv(128, 256) self.enc4 = DoubleConv(256, 512) self.pool = nn.MaxPool2d(2) # Bottleneck self.bottleneck = DoubleConv(512, 1024) # Decoder self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2) self.dec4 = DoubleConv(1024, 512) self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2) self.dec3 = DoubleConv(512, 256) self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2) self.dec2 = DoubleConv(256, 128) self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2) self.dec1 = DoubleConv(128, 64) # Output self.out = nn.Conv2d(64, out_channels, 1) def forward(self, x): # Encoder e1 = self.enc1(x) e2 = self.enc2(self.pool(e1)) e3 = self.enc3(self.pool(e2)) e4 = self.enc4(self.pool(e3)) # Bottleneck b = self.bottleneck(self.pool(e4)) # Decoder + Skip connections d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1)) d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1)) d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1)) d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1)) return self.out(d1) 학습# Loss Function# Binary Segmentation: $$L = \\text{BCE}(y, \\hat{y}) + \\text{Dice Loss}$$\nMulti-class: $$L = \\text{CrossEntropy}(y, \\hat{y})$$\nData Augmentation# 의료 영상은 데이터가 적으므로 augmentation이 중요:\nElastic deformation 회전, 뒤집기 그레이스케일 변환 U-Net 변형# 변형 특징 U-Net++ 중첩된 skip connection Attention U-Net Attention gate 추가 ResUNet ResNet 블록 사용 TransUNet Transformer encoder 활용 분야# 의료 영상: 세포, 종양, 장기 segmentation 위성 영상: 건물, 도로 추출 자율주행: 도로 영역 분할 생성 모델: Stable Diffusion의 구조에 영향 관련 콘텐츠# Transposed Convolution - Upsampling 원리 ResNet - Skip connection 아이디어 Mask R-CNN - Instance segmentation Segmentation 태스크 - 평가 지표 "},{"id":5,"href":"/ko/docs/architecture/transformer/vit/","title":"ViT","section":"Transformer","content":"ViT (Vision Transformer)# 개요# 논문: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) 저자: Alexey Dosovitskiy et al. (Google) 핵심 기여: 이미지를 패치로 나누어 순수 Transformer로 처리 핵심 아이디어# \u0026ldquo;이미지를 단어처럼 취급하자\u0026rdquo;\n이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.\n구조# 전체 아키텍처# Input Image (224×224×3) ↓ Split into patches (14×14 = 196 patches of 16×16) ↓ Linear projection (flatten + linear → D dim) ↓ + Positional Embedding + [CLS] token ↓ ┌─────────────────────────────┐ │ Transformer Encoder │ │ [Multi-Head Attn + FFN]×L │ └─────────────────────────────┘ ↓ [CLS] token output ↓ MLP Head → Classification핵심 컴포넌트# 컴포넌트 설명 Patch Embedding 16×16×3 → D 차원 벡터 [CLS] Token 전체 이미지 표현 학습 Position Embedding 학습 가능한 위치 인코딩 Transformer Encoder L개 레이어 Patch Embedding# # 이미지를 패치로 분할 후 선형 변환 # (B, 3, 224, 224) → (B, 196, 768) class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super().__init__() self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): # (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim) x = self.proj(x) x = x.flatten(2).transpose(1, 2) return x 모델 변형# 모델 Layers Hidden Heads Params ViT-Base 12 768 12 86M ViT-Large 24 1024 16 307M ViT-Huge 32 1280 16 632M 학습# 사전학습의 중요성# ViT는 CNN과 달리 inductive bias가 적어서 대규모 데이터가 필수:\n학습 데이터 ImageNet Accuracy ImageNet-1k (1.2M) 79.7% (ResNet보다 낮음) ImageNet-21k (14M) 84.2% JFT-300M 88.5% 학습 설정# Optimizer: Adam (β₁=0.9, β₂=0.999) Learning rate: warmup + cosine decay Augmentation: RandAugment, Mixup, CutMix Regularization: Dropout, Stochastic Depth 구현 예시# import torch import torch.nn as nn class ViT(nn.Module): def __init__( self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.1 ): super().__init__() num_patches = (img_size // patch_size) ** 2 # Patch embedding self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) # CLS token and position embedding self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) self.dropout = nn.Dropout(dropout) # Transformer encoder self.blocks = nn.ModuleList([ TransformerBlock(embed_dim, num_heads, embed_dim * mlp_ratio, dropout) for _ in range(depth) ]) self.norm = nn.LayerNorm(embed_dim) # Classification head self.head = nn.Linear(embed_dim, num_classes) def forward(self, x): B = x.shape[0] # Patch embedding x = self.patch_embed(x).flatten(2).transpose(1, 2) # Add CLS token cls_tokens = self.cls_token.expand(B, -1, -1) x = torch.cat([cls_tokens, x], dim=1) # Add position embedding x = x + self.pos_embed x = self.dropout(x) # Transformer encoder for block in self.blocks: x = block(x) x = self.norm(x) # Classification (CLS token만 사용) return self.head(x[:, 0])HuggingFace 사용# from transformers import ViTForImageClassification, ViTImageProcessor processor = ViTImageProcessor.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) model = ViTForImageClassification.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) inputs = processor(images=image, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) predicted_class = outputs.logits.argmax(-1).item() ViT의 특징# Attention 시각화# ViT는 이미지의 어떤 부분에 주목하는지 시각화 가능:\n# 마지막 레이어의 attention map attentions = model.get_last_selfattention(x) # CLS token이 어디를 보는지 확인 cls_attention = attentions[:, :, 0, 1:] # (B, heads, num_patches)장점# 전역적 context 이해 확장성 우수 (모델/데이터 scale up 가능) 다양한 태스크 transfer 용이 단점# 대규모 데이터 필요 계산량 O(n²) 작은 객체/고해상도 처리 어려움 ViT 변형# 모델 핵심 개선 DeiT Knowledge distillation으로 적은 데이터 학습 Swin Shifted window로 효율적 계산 CvT Conv 추가로 inductive bias 강화 BEiT BERT 스타일 사전학습 관련 콘텐츠# Transformer - 기반 아키텍처 ResNet - 대조되는 CNN 접근 CLIP - ViT를 image encoder로 사용 DiT - Diffusion + Transformer "},{"id":6,"href":"/ko/docs/timeline/","title":"타임라인","section":"Docs","content":"타임라인으로 배우는 Vision# 연대순으로 Computer Vision의 발전 과정을 따라가며 학습합니다.\n~2012: Classical Computer Vision# 선형대수 기하학 SIFT \u0026amp; HOG 2012-2015: CNN 시대의 시작# Convolution Backpropagation AlexNet VGG ResNet Classification 2015-2017: Detection \u0026amp; Segmentation# IoU \u0026amp; NMS YOLO Faster R-CNN U-Net Detection Segmentation 2017-2019: Attention의 등장# Attention Transformer 2020-2021: Vision Transformer \u0026amp; CLIP# ViT CLIP Contrastive Learning Self-supervised Learning 2021-2022: Diffusion 시대# Diffusion Process Stable Diffusion Generation 2023: Controllable Generation \u0026amp; SAM# ControlNet SAM 2023-2024: VLM \u0026amp; DiT# VLM DiT Vision-Language 2024-현재: 3D \u0026amp; Video Generation# NeRF 3D Gaussian Splatting 3D Vision "},{"id":7,"href":"/ko/docs/architecture/cnn/","title":"CNN","section":"아키텍처","content":"CNN (Convolutional Neural Network)# 개요# CNN은 이미지 처리에 특화된 신경망 구조로, 2012년 AlexNet의 등장 이후 컴퓨터 비전의 핵심이 되었습니다.\n핵심 구성 요소# 1. Convolution Layer# 필터(커널)를 이미지에 슬라이딩하며 특징을 추출합니다.\n$$\\text{Output}(i,j) = \\sum_{m}\\sum_{n} \\text{Input}(i+m, j+n) \\cdot \\text{Kernel}(m,n)$$\n특징:\n파라미터 공유로 효율적 지역적 패턴 학습 Translation equivariance 2. Pooling Layer# 공간 해상도를 줄이고 위치 불변성을 제공합니다.\nMax Pooling: 영역 내 최댓값 선택 Average Pooling: 영역 내 평균값 계산 3. Activation Function# 비선형성을 추가합니다.\n$$\\text{ReLU}(x) = \\max(0, x)$$\n4. Fully Connected Layer# 최종 분류를 위한 전결합 층입니다.\n전형적인 CNN 구조# Input → [Conv → ReLU → Pool] × N → Flatten → FC → Output 주요 발전 과정# 연도 모델 핵심 기여 2012 AlexNet GPU 학습, ReLU, Dropout 2014 VGG 깊은 네트워크, 3x3 필터 2015 ResNet Skip Connection, 152층 구현 예시# import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self, num_classes=10): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), ) self.classifier = nn.Sequential( nn.Flatten(), nn.Linear(128 * 8 * 8, 256), nn.ReLU(), nn.Linear(256, num_classes), ) def forward(self, x): x = self.features(x) x = self.classifier(x) return x 관련 콘텐츠# Convolution - Convolution 연산의 수학적 이해 Backpropagation - CNN 학습 원리 Classification - CNN의 대표적 응용 "},{"id":8,"href":"/ko/docs/architecture/segmentation/mask-rcnn/","title":"Mask R-CNN","section":"Segmentation","content":"Mask R-CNN# 개요# 논문: Mask R-CNN (2017) 저자: Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick (FAIR) 핵심 기여: Faster R-CNN에 mask branch를 추가하여 instance segmentation 핵심 아이디어# \u0026ldquo;Faster R-CNN + Mask Branch = Instance Segmentation\u0026rdquo;\nDetection과 segmentation을 동시에 수행하는 multi-task 모델입니다.\n구조# 전체 아키텍처# Input Image ↓ Backbone (ResNet + FPN) ↓ Region Proposal Network (RPN) ↓ RoI Align (not RoI Pooling!) ↓ ┌─────────────────────────────────────────┐ │ Three Branches │ ├─────────────┬─────────────┬─────────────┤ │ Classification │ Box Regression │ Mask Prediction │ │ (class) │ (x,y,w,h) │ (28×28 binary) │ └─────────────┴─────────────┴─────────────┘Faster R-CNN과의 차이# 구성 요소 Faster R-CNN Mask R-CNN RoI 처리 RoI Pooling RoI Align 출력 class, box class, box, mask Backbone ResNet ResNet + FPN 핵심 컴포넌트# 1. RoI Align# RoI Pooling의 양자화 문제를 해결:\nRoI Pooling 문제:\nRoI: 10.6 → 10 (반올림) 실제 위치와 오차 발생 → segmentation 품질 저하RoI Align 해결:\n양자화 없이 bilinear interpolation 정확한 위치에서 특징 추출2. Mask Branch# 각 RoI에 대해 28×28 크기의 마스크 예측:\n# Mask Head 구조 FCN (4 Conv layers) → Deconv → 28×28×num_classes특징:\n클래스별로 독립적인 마스크 예측 Binary mask (sigmoid) 최종 마스크는 box 크기로 resize 3. FPN (Feature Pyramid Network)# 다양한 스케일의 객체 검출을 위한 feature pyramid:\nP5 (작은 해상도, 큰 객체) ↑ P4 ↑ P3 ↑ P2 (큰 해상도, 작은 객체) 학습# Multi-task Loss# $$L = L_{cls} + L_{box} + L_{mask}$$\n$L_{cls}$: Classification loss (cross-entropy) $L_{box}$: Bounding box regression (smooth L1) $L_{mask}$: Binary cross-entropy (픽셀별) Mask Loss# Ground truth 클래스에 해당하는 마스크에서만 loss 계산:\n$$L_{mask} = -\\frac{1}{m^2} \\sum_{i,j} [y_{ij} \\log \\hat{y}{ij} + (1-y{ij}) \\log(1-\\hat{y}_{ij})]$$\n구현 예시# import torch import torchvision from torchvision.models.detection import maskrcnn_resnet50_fpn # 사전 학습된 모델 로드 model = maskrcnn_resnet50_fpn(pretrained=True) model.eval() # 추론 with torch.no_grad(): predictions = model([image]) # 결과 형태 pred = predictions[0] print(pred[\u0026#39;boxes\u0026#39;].shape) # (N, 4) - bounding boxes print(pred[\u0026#39;labels\u0026#39;].shape) # (N,) - class labels print(pred[\u0026#39;scores\u0026#39;].shape) # (N,) - confidence scores print(pred[\u0026#39;masks\u0026#39;].shape) # (N, 1, H, W) - instance masks커스텀 학습# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor from torchvision.models.detection.faster_rcnn import FastRCNNPredictor model = maskrcnn_resnet50_fpn(pretrained=True) num_classes = 10 # Box predictor 교체 in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # Mask predictor 교체 in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 model.roi_heads.mask_predictor = MaskRCNNPredictor( in_features_mask, hidden_layer, num_classes ) 성능# COCO Dataset# Backbone box AP mask AP ResNet-50-FPN 38.2 34.7 ResNet-101-FPN 40.0 36.1 ResNeXt-101-FPN 41.3 37.1 Mask R-CNN의 확장# Keypoint Detection# Mask branch 대신 keypoint branch 추가:\nfrom torchvision.models.detection import keypointrcnn_resnet50_fpn model = keypointrcnn_resnet50_fpn(pretrained=True) # 17개 keypoint (COCO person keypoints)Panoptic Segmentation# Instance + semantic segmentation 통합\n한계점# Two-stage라 속도 제한 (~5 FPS) 겹치는 객체 처리 어려움 작은 객체 segmentation 품질 저하 관련 콘텐츠# Faster R-CNN - 기반 모델 U-Net - Semantic segmentation SAM - Promptable segmentation ResNet - Backbone Segmentation 태스크 - 평가 지표 "},{"id":9,"href":"/ko/docs/topdown/","title":"Top-Down","section":"Docs","content":"Top-Down 학습# 문제를 정하고, 필요한 지식을 찾아갑니다.\nImage Classification# Cross-entropy Loss CNN 기초 ResNet Classification Object Detection# IoU \u0026amp; NMS Anchor Box Faster R-CNN YOLO Detection Segmentation# Transposed Convolution U-Net Mask R-CNN Segmentation Image Generation# 확률분포 VAE GAN Diffusion Stable Diffusion ControlNet DiT Generation Vision-Language (VLM)# Contrastive Learning CLIP VLM Vision-Language 3D Vision# 카메라 모델 NeRF 3D Vision 모델 배포# Quantization Deployment "},{"id":10,"href":"/ko/docs/architecture/cnn/vgg/","title":"VGG","section":"CNN","content":"VGG# 개요# 논문: Very Deep Convolutional Networks for Large-Scale Image Recognition (2014) 저자: Karen Simonyan, Andrew Zisserman (Oxford) 핵심 기여: 작은 필터(3×3)를 깊게 쌓는 것이 효과적임을 증명 핵심 아이디어# \u0026ldquo;3×3 필터를 여러 번 쌓으면 큰 필터와 같은 receptive field를 가지면서 파라미터는 더 적다\u0026rdquo;\n예시: 3×3 두 번 = 5×5 한 번의 receptive field\n방식 Receptive Field 파라미터 5×5 한 번 5×5 25C² 3×3 두 번 5×5 18C² 구조# VGG-16 아키텍처# Input (224×224×3) ↓ Block 1: [Conv3-64] × 2 → MaxPool ↓ Block 2: [Conv3-128] × 2 → MaxPool ↓ Block 3: [Conv3-256] × 3 → MaxPool ↓ Block 4: [Conv3-512] × 3 → MaxPool ↓ Block 5: [Conv3-512] × 3 → MaxPool ↓ FC: 4096 → 4096 → 1000VGG 변형# 모델 레이어 수 파라미터 VGG-11 11 133M VGG-13 13 133M VGG-16 16 138M VGG-19 19 144M 설계 원칙# 1. 작은 필터 (3×3)# 모든 Conv 레이어에서 3×3 필터 사용:\n더 많은 비선형성 (ReLU 더 많이 적용) 파라미터 효율성 더 깊은 네트워크 가능 2. 일관된 구조# 모든 Conv: 3×3, stride 1, padding 1 모든 Pool: 2×2, stride 2 채널 수: 64 → 128 → 256 → 512 → 512 3. 깊이의 중요성# 레이어를 깊게 쌓을수록 성능 향상:\nVGG-11: 10.4% top-5 error VGG-16: 7.4% top-5 error VGG-19: 7.3% top-5 error 구현 예시# import torch.nn as nn def make_layers(cfg): layers = [] in_channels = 3 for v in cfg: if v == \u0026#39;M\u0026#39;: layers.append(nn.MaxPool2d(kernel_size=2, stride=2)) else: layers.append(nn.Conv2d(in_channels, v, kernel_size=3, padding=1)) layers.append(nn.ReLU(inplace=True)) in_channels = v return nn.Sequential(*layers) # VGG-16 config cfg_vgg16 = [64, 64, \u0026#39;M\u0026#39;, 128, 128, \u0026#39;M\u0026#39;, 256, 256, 256, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;] class VGG16(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = make_layers(cfg_vgg16) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x 특징 추출기로서의 VGG# VGG의 중간 레이어는 범용적인 특징 추출기로 활용됩니다:\n# Perceptual Loss에서 VGG 특징 사용 vgg = models.vgg16(pretrained=True).features[:16] # relu3_3까지 def perceptual_loss(pred, target): pred_features = vgg(pred) target_features = vgg(target) return F.mse_loss(pred_features, target_features)활용 분야:\nStyle Transfer Perceptual Loss Feature Matching 한계점# 파라미터 과다: 138M 파라미터 (대부분 FC 레이어) 메모리 사용량: 학습 시 많은 GPU 메모리 필요 학습 어려움: 깊은 네트워크에서 gradient vanishing 이러한 한계는 ResNet의 Skip Connection으로 해결되었습니다.\n관련 콘텐츠# AlexNet - 이전 모델 ResNet - Skip Connection으로 깊이 문제 해결 CNN 기초 - Convolution의 원리 "},{"id":11,"href":"/ko/docs/architecture/detection/yolo/","title":"YOLO","section":"Detection","content":"YOLO (You Only Look Once)# 개요# 논문: You Only Look Once: Unified, Real-Time Object Detection (2016) 저자: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi 핵심 기여: Detection을 단일 회귀 문제로 재정의, 실시간 처리 가능 핵심 아이디어# \u0026ldquo;Detection을 한 번의 forward pass로 해결\u0026rdquo;\nTwo-stage detector와 달리, 이미지를 그리드로 나누고 각 그리드에서 직접 박스와 클래스를 예측합니다.\nYOLO 버전 변화# 버전 연도 핵심 개선 mAP (COCO) FPS YOLOv1 2016 첫 one-stage detector 63.4 (VOC) 45 YOLOv2 2016 Batch Norm, Anchor 78.6 (VOC) 67 YOLOv3 2018 Multi-scale, FPN 33.0 30 YOLOv4 2020 CSPNet, Mish 43.5 65 YOLOv5 2020 PyTorch, 편의성 50.7 140 YOLOv8 2023 Anchor-free, SOTA 53.9 100+ YOLOv1 구조# 동작 원리# 1. 이미지를 S×S 그리드로 분할 (S=7) 2. 각 그리드 셀이 B개의 bounding box 예측 (B=2) 3. 각 박스: (x, y, w, h, confidence) 4. 각 그리드 셀이 C개 클래스 확률 예측 (C=20) Output: S × S × (B×5 + C) = 7 × 7 × 30네트워크 구조# Input (448×448×3) ↓ 24 Conv Layers (GoogLeNet inspired) ↓ 2 FC Layers ↓ Output (7×7×30)Confidence Score# $$\\text{Confidence} = P(\\text{Object}) \\times \\text{IoU}_{pred}^{truth}$$\nLoss Function# $$L = \\lambda_{coord} L_{coord} + L_{obj} + \\lambda_{noobj} L_{noobj} + L_{class}$$\n$\\lambda_{coord} = 5$: 좌표 loss 가중치 $\\lambda_{noobj} = 0.5$: 객체 없는 셀의 loss 가중치 YOLOv3 구조# 개선점# Multi-scale Prediction: 3가지 스케일에서 예측 FPN-like Structure: 다양한 크기의 객체 검출 Anchor Boxes: 9개의 anchor (3 scales × 3 ratios) Darknet-53: ResNet-like backbone 아키텍처# Input (416×416) ↓ Darknet-53 Backbone ↓ ┌─────────────────────────────────────┐ │ FPN-like Neck │ │ 52×52 (small) + 26×26 + 13×13 (large) │ └─────────────────────────────────────┘ ↓ 3 Detection Heads ↓ Output: - 13×13×255 (대형 객체) - 26×26×255 (중형 객체) - 52×52×255 (소형 객체) 255 = 3 anchors × (4 coords + 1 obj + 80 classes) YOLOv8 구조 (최신)# 주요 특징# Anchor-free: anchor box 없이 직접 예측 Decoupled Head: classification과 regression 분리 C2f Module: CSP + 더 효율적인 특징 융합 Task-specific Heads: detection, segmentation, pose 등 모델 변형# 모델 파라미터 mAP FPS (V100) YOLOv8n 3.2M 37.3 430 YOLOv8s 11.2M 44.9 350 YOLOv8m 25.9M 50.2 220 YOLOv8l 43.7M 52.9 160 YOLOv8x 68.2M 53.9 100 구현 예시# YOLOv8 (Ultralytics)# from ultralytics import YOLO # 모델 로드 model = YOLO(\u0026#39;yolov8n.pt\u0026#39;) # nano 모델 # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 확인 for result in results: boxes = result.boxes # Bounding boxes print(boxes.xyxy) # x1, y1, x2, y2 형식 print(boxes.conf) # confidence scores print(boxes.cls) # class indices # 학습 model.train(data=\u0026#39;coco128.yaml\u0026#39;, epochs=100) # Export model.export(format=\u0026#39;onnx\u0026#39;)YOLOv5 (torch hub)# import torch # 모델 로드 model = torch.hub.load(\u0026#39;ultralytics/yolov5\u0026#39;, \u0026#39;yolov5s\u0026#39;, pretrained=True) # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 출력 results.print() results.show() # pandas DataFrame으로 결과 df = results.pandas().xyxy[0] YOLO vs Faster R-CNN# 측면 YOLO Faster R-CNN 방식 One-stage Two-stage 속도 빠름 (30-100+ FPS) 느림 (~7 FPS) 정확도 상대적으로 낮음 높음 작은 객체 어려움 상대적으로 좋음 실시간 가능 어려움 구현 단순 복잡 실제 활용# 자율주행: 실시간 객체 검출 CCTV 분석: 사람/차량 검출 산업 자동화: 제품 불량 검출 모바일: 경량화 모델 (YOLOv8n) 관련 콘텐츠# Faster R-CNN - Two-stage detector IoU \u0026amp; NMS - Detection 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표 "},{"id":12,"href":"/ko/docs/bottomup/","title":"Bottom-Up","section":"Docs","content":"Bottom-Up 학습# 기초 개념부터 차근차근 쌓아올립니다.\nLevel 1: 수학 기초# 선형대수 미적분 \u0026amp; Chain Rule 확률/통계 Level 2: 딥러닝 기초# Convolution Backpropagation Loss Functions Optimization Level 3: 기본 아키텍처# CNN 기초 AlexNet VGG ResNet Level 4: 기본 태스크# Classification Detection Segmentation Level 5: 고급 개념# Attention Transformer ViT Level 6: 생성 모델# Diffusion Process Stable Diffusion ControlNet DiT Generation Level 7: Multimodal \u0026amp; 3D# Contrastive Learning CLIP VLM Vision-Language NeRF 3D Vision "},{"id":13,"href":"/ko/docs/architecture/detection/","title":"Detection","section":"아키텍처","content":"Object Detection 모델# 이미지에서 객체의 위치(bounding box)와 클래스를 동시에 예측하는 모델들입니다.\n두 가지 접근 방식# Two-Stage Detector# Region Proposal: 객체가 있을 법한 영역 제안 Classification: 각 영역을 분류 특징: 정확도 높음, 속도 느림\nR-CNN → Fast R-CNN → Faster R-CNN One-Stage Detector# 한 번에 위치와 클래스를 예측\n특징: 속도 빠름, 실시간 가능\nYOLO SSD RetinaNet 모델 비교# 모델 유형 FPS mAP (COCO) 특징 Faster R-CNN Two-stage ~7 42.0 높은 정확도 YOLOv3 One-stage ~30 33.0 실시간 YOLOv8 One-stage ~100 53.9 SOTA DETR Transformer ~28 42.0 End-to-end 관련 콘텐츠# IoU \u0026amp; NMS - Detection의 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표, 데이터셋 "},{"id":14,"href":"/ko/docs/architecture/cnn/resnet/","title":"ResNet","section":"CNN","content":"ResNet (Residual Network)# 개요# 논문: Deep Residual Learning for Image Recognition (2015) 저자: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research) 핵심 기여: Skip Connection으로 100+ 레이어 학습 가능하게 함 해결한 문제: Degradation Problem# 네트워크가 깊어지면 성능이 오히려 떨어지는 현상:\n20-layer: 7.4% error 56-layer: 8.5% error ← 더 깊은데 성능이 나빠짐이는 과적합이 아니라 최적화의 어려움 때문입니다.\n핵심 아이디어: Residual Learning# Skip Connection (Shortcut)# 기존 학습: $$H(x) = F(x)$$\nResidual 학습: $$H(x) = F(x) + x$$\n여기서 $F(x)$는 **잔차(residual)**를 학습합니다.\n직관# 입력을 그대로 출력에 더함 네트워크는 \u0026ldquo;변화량\u0026quot;만 학습 Identity mapping이 쉬워짐 → 깊은 네트워크 학습 가능 구조# Basic Block (ResNet-18/34)# x → Conv3×3 → BN → ReLU → Conv3×3 → BN → (+x) → ReLU └─────────────────────────────────────┘ skip connectionBottleneck Block (ResNet-50/101/152)# x → Conv1×1 → BN → ReLU → Conv3×3 → BN → ReLU → Conv1×1 → BN → (+x) → ReLU └────────────────────────────────────────────────────────────┘ skip connection1×1 Conv로 채널을 줄였다가 늘려 계산량 감소.\nResNet 변형# 모델 레이어 파라미터 Top-5 Error ResNet-18 18 11.7M 10.92% ResNet-34 34 21.8M 9.46% ResNet-50 50 25.6M 7.13% ResNet-101 101 44.5M 6.44% ResNet-152 152 60.2M 6.16% 전체 아키텍처 (ResNet-50)# Input (224×224×3) ↓ Conv 7×7, 64, stride 2 ↓ MaxPool 3×3, stride 2 ↓ Stage 1: [Bottleneck(64, 256)] × 3 ↓ Stage 2: [Bottleneck(128, 512)] × 4 ↓ Stage 3: [Bottleneck(256, 1024)] × 6 ↓ Stage 4: [Bottleneck(512, 2048)] × 3 ↓ Global Average Pool ↓ FC → 1000 구현 예시# import torch.nn as nn class BasicBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) # Skip connection (차원 맞추기) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) out += identity # Skip connection out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, in_channels, mid_channels, stride=1): super().__init__() out_channels = mid_channels * self.expansion self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False) self.bn1 = nn.BatchNorm2d(mid_channels) self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, stride, 1, bias=False) self.bn2 = nn.BatchNorm2d(mid_channels) self.conv3 = nn.Conv2d(mid_channels, out_channels, 1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += identity out = self.relu(out) return out ResNet의 영향# Backbone으로 널리 사용# Object Detection: Faster R-CNN Segmentation: Mask R-CNN 대부분의 Vision 태스크에서 기본 backbone 후속 연구# ResNeXt: 그룹 컨볼루션 추가 DenseNet: 모든 레이어를 연결 SE-Net: 채널 attention 추가 EfficientNet: 최적 스케일링 왜 Skip Connection이 효과적인가?# 1. Gradient Flow 개선# $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\left(1 + \\frac{\\partial F}{\\partial x}\\right)$$\n항상 1이 더해지므로 gradient가 소실되지 않습니다.\n2. Ensemble 효과# Skip connection은 다양한 깊이의 경로를 만들어 앙상블과 유사한 효과를 냅니다.\n관련 콘텐츠# VGG - 깊이의 한계를 보여준 모델 CNN 기초 - Convolution의 원리 Detection - ResNet을 backbone으로 사용 ViT - ResNet을 대체하는 새로운 패러다임 "},{"id":15,"href":"/ko/docs/architecture/segmentation/sam/","title":"SAM","section":"Segmentation","content":"SAM (Segment Anything Model)# 개요# 논문: Segment Anything (2023) 저자: Alexander Kirillov et al. (Meta AI) 핵심 기여: 프롬프트 기반의 범용 segmentation foundation model 핵심 아이디어# \u0026ldquo;한 번 학습으로 모든 객체를 segmentation\u0026rdquo;\nPoint, box, text 등 다양한 프롬프트로 원하는 객체를 분할합니다.\n프로젝트 구성# 세 가지 요소# Task: Promptable segmentation Model: SAM 아키텍처 Data: SA-1B 데이터셋 (11M 이미지, 1.1B 마스크) 구조# 전체 아키텍처# ┌─────────────────────────────────────────┐ │ Image Encoder │ │ (ViT-H, 실행 1회) │ │ Image → Image Embeddings │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Prompt Encoder │ │ Points/Boxes/Text → Prompt Tokens │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Mask Decoder │ │ Image Emb + Prompt → Masks │ └─────────────────────────────────────────┘ ↓ Output Masks1. Image Encoder# 모델: ViT-H (Vision Transformer Huge) 입력: 1024×1024 이미지 출력: 64×64 feature embedding 특징: 이미지당 한 번만 실행 (0.15초) 2. Prompt Encoder# 다양한 프롬프트 처리:\n프롬프트 인코딩 방식 Point Positional encoding + learned embedding Box Corner points의 positional encoding Mask Convolution으로 downsampling Text CLIP text encoder (연구 중) 3. Mask Decoder# Transformer 기반 경량 디코더 양방향 attention (prompt ↔ image) 다중 마스크 출력 (모호성 해결) 프롬프트 유형# Point Prompt# # 클릭 한 번으로 객체 선택 point_coords = [[500, 375]] # x, y point_labels = [1] # 1: foreground, 0: backgroundBox Prompt# # Bounding box로 객체 범위 지정 box = [100, 100, 400, 400] # x1, y1, x2, y2Mask Prompt# # 이전 마스크를 프롬프트로 사용 (iterative refinement) mask_input = previous_mask 구현 예시# from segment_anything import sam_model_registry, SamPredictor # 모델 로드 sam = sam_model_registry[\u0026#34;vit_h\u0026#34;](checkpoint=\u0026#34;sam_vit_h.pth\u0026#34;) predictor = SamPredictor(sam) # 이미지 설정 (한 번만 실행) predictor.set_image(image) # Point prompt로 예측 masks, scores, logits = predictor.predict( point_coords=np.array([[500, 375]]), point_labels=np.array([1]), multimask_output=True, # 3개 마스크 출력 ) # Box prompt로 예측 masks, scores, logits = predictor.predict( box=np.array([100, 100, 400, 400]), multimask_output=False, )Automatic Mask Generation# from segment_anything import SamAutomaticMaskGenerator mask_generator = SamAutomaticMaskGenerator(sam) # 이미지의 모든 객체 자동 분할 masks = mask_generator.generate(image) # 각 마스크 정보 for mask in masks: print(mask[\u0026#39;segmentation\u0026#39;].shape) # binary mask print(mask[\u0026#39;area\u0026#39;]) # 마스크 면적 print(mask[\u0026#39;bbox\u0026#39;]) # bounding box print(mask[\u0026#39;predicted_iou\u0026#39;]) # 예측 IoU print(mask[\u0026#39;stability_score\u0026#39;]) # 안정성 점수 모델 변형# 모델 Encoder 파라미터 속도 SAM ViT-H ViT-Huge 636M 0.15s/image SAM ViT-L ViT-Large 308M 0.1s/image SAM ViT-B ViT-Base 91M 0.06s/image 경량화 버전# MobileSAM: 모바일용 경량 모델 FastSAM: YOLO 기반 실시간 버전 EfficientSAM: 효율적인 학습 방법 SA-1B 데이터셋# 데이터 엔진# 1. Manual Phase: 전문가가 마스크 생성 2. Semi-Automatic: SAM 제안 + 사람 수정 3. Fully Automatic: SAM 자동 생성 + 검증규모# 11M 이미지 1.1B 마스크 평균 이미지당 100개 마스크 활용# Zero-shot Transfer# 학습하지 않은 도메인에서도 동작:\n의료 영상 위성 영상 현미경 이미지 응용 분야# 이미지 편집: 객체 선택 및 제거 Annotation 도구: 라벨링 자동화 AR/VR: 실시간 객체 분할 비디오 분석: 프레임별 segmentation SAM 2 (2024)# SAM의 비디오 확장:\n비디오 전체에서 객체 추적 시간적 일관성 유지 메모리 효율적 처리 한계점# 큰 모델 크기 (ViT-H) 세밀한 경계에서 부정확 Text prompt 지원 제한적 Real-time 어려움 관련 콘텐츠# ViT - Image encoder 기반 Mask R-CNN - Instance segmentation CLIP - Text-image 연결 Segmentation 태스크 - 평가 지표 "},{"id":16,"href":"/ko/docs/architecture/segmentation/","title":"Segmentation","section":"아키텍처","content":"Segmentation 모델# 이미지의 각 픽셀에 레이블을 할당하는 모델들입니다.\nSegmentation 유형# Semantic Segmentation# 각 픽셀을 클래스로 분류 (객체 구분 X)\nInput: 고양이 2마리 이미지 Output: 모든 고양이 픽셀 → \u0026#34;고양이\u0026#34; 클래스Instance Segmentation# 각 객체 인스턴스를 구분\nInput: 고양이 2마리 이미지 Output: 고양이1 픽셀, 고양이2 픽셀 (분리)Panoptic Segmentation# Semantic + Instance 통합\nOutput: 배경(semantic) + 각 객체(instance) 주요 모델# 모델 유형 연도 특징 U-Net Semantic 2015 Encoder-Decoder, 의료영상 Mask R-CNN Instance 2017 Faster R-CNN + Mask DeepLab Semantic 2017 Atrous Convolution SAM Promptable 2023 Foundation Model 관련 콘텐츠# Transposed Convolution - Upsampling 기법 Segmentation 태스크 - 평가 지표, 데이터셋 "},{"id":17,"href":"/ko/docs/architecture/transformer/","title":"Transformer","section":"아키텍처","content":"Transformer# 개요# 논문: Attention Is All You Need (2017) 저자: Vaswani et al. (Google) 핵심 기여: Self-attention으로 RNN 없이 시퀀스 처리 원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.\n핵심 아이디어: Self-Attention# 모든 위치 간의 관계를 직접 모델링:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nQ (Query): 현재 위치에서 찾고 싶은 것 K (Key): 다른 위치들의 특성 V (Value): 실제 정보 CNN과 비교# 측면 CNN Transformer 수용 영역 지역적 (kernel size) 전역적 (모든 위치) 관계 모델링 암시적 명시적 (attention) 위치 정보 구조에 내재 Positional encoding 필요 계산량 O(n) O(n²) 구조# Encoder-Decoder 구조# ┌─────────────────────┐ ┌─────────────────────┐ │ Encoder │ │ Decoder │ ├─────────────────────┤ ├─────────────────────┤ │ Multi-Head Attn │ │ Masked Multi-Head │ │ ↓ │ │ ↓ │ │ Add \u0026amp; Norm │────→│ Cross Attention │ │ ↓ │ │ ↓ │ │ Feed Forward │ │ Add \u0026amp; Norm │ │ ↓ │ │ ↓ │ │ Add \u0026amp; Norm │ │ Feed Forward │ └─────────────────────┘ │ ↓ │ × N │ Add \u0026amp; Norm │ └─────────────────────┘ × NMulti-Head Attention# 여러 관점에서 attention 수행:\n# 8개 head, 각 head는 64차원 MultiHead(Q, K, V) = Concat(head_1, ..., head_8) @ W_O where head_i = Attention(Q @ W_Q_i, K @ W_K_i, V @ W_V_i)Positional Encoding# 위치 정보 주입:\n$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$ $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\nVision에서의 Transformer# 핵심 모델들# 모델 연도 입력 처리 특징 ViT 2020 Image → Patches 순수 Transformer DeiT 2021 Distillation 효율적 학습 Swin 2021 Shifted Windows 계층적 구조 DiT 2023 Diffusion + Transformer 이미지 생성 구현 예시# import torch import torch.nn as nn import math class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() self.num_heads = num_heads self.d_k = d_model // num_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # Linear projections Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) attn = torch.softmax(scores, dim=-1) context = torch.matmul(attn, V) # Concat and project context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) return self.W_o(context) class TransformerBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout=0.1): super().__init__() self.attention = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.ffn = nn.Sequential( nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model), ) self.dropout = nn.Dropout(dropout) def forward(self, x): # Self-attention with residual attn_out = self.attention(x, x, x) x = self.norm1(x + self.dropout(attn_out)) # FFN with residual ffn_out = self.ffn(x) x = self.norm2(x + self.dropout(ffn_out)) return x Transformer의 영향# Vision 분야# Image Classification: ViT, DeiT, Swin Object Detection: DETR, Deformable DETR Segmentation: SAM, SegFormer Generation: DiT 장점# 전역적 context 이해 확장성 (scaling law) 사전학습 효과적 단점# O(n²) 계산량 많은 데이터 필요 위치 정보 취약 관련 콘텐츠# Attention - Attention 수식 상세 ViT - Vision Transformer DiT - Diffusion Transformer CNN 기초 - 대조되는 접근 방식 "},{"id":18,"href":"/ko/docs/math/","title":"수학","section":"Docs","content":"수학 기초# Vision 엔지니어에게 필요한 수학 지식입니다.\n"},{"id":19,"href":"/ko/docs/architecture/","title":"아키텍처","section":"Docs","content":"아키텍처# Computer Vision 모델 및 네트워크 구조입니다.\n카테고리# Classical CV - SIFT, HOG 등 전통적 특징 추출 CNN - Convolutional Neural Network 계열 Detection - 객체 탐지 모델 Segmentation - 세그멘테이션 모델 Transformer - Transformer 기반 모델 Generative - 생성 모델 Multimodal - 멀티모달 모델 3D - 3D 비전 모델 "},{"id":20,"href":"/ko/docs/task/","title":"태스크","section":"Docs","content":"태스크# 문제 정의, 평가지표, 데이터셋입니다.\n"},{"id":21,"href":"/ko/docs/etc/","title":"기타 자료","section":"Docs","content":"논문 \u0026amp; 자료# 필수 논문과 학습 자료 모음입니다.\n"}]