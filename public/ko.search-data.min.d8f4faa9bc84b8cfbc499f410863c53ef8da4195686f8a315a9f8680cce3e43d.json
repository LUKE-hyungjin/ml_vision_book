[{"id":0,"href":"/ko/docs/architecture/cnn/alexnet/","title":"AlexNet","section":"CNN","content":"AlexNet# 개요# 논문: ImageNet Classification with Deep Convolutional Neural Networks (2012) 저자: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton 핵심 기여: 딥러닝 시대의 시작을 알린 모델. ImageNet 2012에서 압도적 성능으로 우승 왜 중요한가?# AlexNet은 ImageNet에서 top-5 error를 **26% → 16%**로 크게 낮추며 딥러닝의 가능성을 증명했습니다. 이후 모든 컴퓨터 비전 연구가 CNN 기반으로 전환되는 계기가 되었습니다.\n구조# 전체 아키텍처# Input (224×224×3) ↓ Conv1: 96 filters, 11×11, stride 4 → ReLU → LRN → MaxPool ↓ Conv2: 256 filters, 5×5 → ReLU → LRN → MaxPool ↓ Conv3: 384 filters, 3×3 → ReLU ↓ Conv4: 384 filters, 3×3 → ReLU ↓ Conv5: 256 filters, 3×3 → ReLU → MaxPool ↓ FC6: 4096 → ReLU → Dropout ↓ FC7: 4096 → ReLU → Dropout ↓ FC8: 1000 (softmax)주요 특징# 요소 설명 ReLU Sigmoid 대신 사용하여 학습 속도 6배 향상 GPU 학습 2개의 GTX 580 GPU로 병렬 학습 Dropout FC 레이어에 0.5 비율로 적용하여 과적합 방지 LRN Local Response Normalization (현재는 사용 안 함) Data Augmentation 랜덤 크롭, 수평 뒤집기, 색상 변환 핵심 기술 상세# 1. ReLU (Rectified Linear Unit)# $$f(x) = \\max(0, x)$$\n장점:\nSigmoid/tanh보다 계산이 단순 Gradient vanishing 문제 완화 학습 속도 대폭 향상 2. Dropout# 학습 시 뉴런을 랜덤하게 비활성화하여 과적합 방지:\n$$\\hat{y} = \\frac{1}{1-p} \\cdot y \\cdot \\text{mask}$$\n3. Data Augmentation# 원본 256×256 이미지에서:\n224×224 랜덤 크롭 수평 뒤집기 PCA 기반 색상 변환 파라미터 수# 레이어 파라미터 Conv 레이어 ~2.3M FC 레이어 ~58.6M 총합 ~60M 대부분의 파라미터가 FC 레이어에 집중되어 있습니다.\n구현 예시# import torch.nn as nn class AlexNet(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.classifier = nn.Sequential( nn.Dropout(0.5), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x 한계점# 큰 필터 크기 (11×11, 5×5) → 비효율적 LRN은 효과 미미 → 이후 사용 안 함 FC 레이어에 파라미터 집중 → 메모리 비효율 이러한 한계는 VGG와 ResNet에서 개선되었습니다.\n관련 콘텐츠# CNN 기초 - CNN의 기본 구조 VGG - 더 깊은 네트워크 ResNet - Skip Connection Classification - 이미지 분류 태스크 "},{"id":1,"href":"/ko/docs/architecture/classical/","title":"Classical CV","section":"아키텍처","content":"Classical Computer Vision# 딥러닝 이전의 전통적인 컴퓨터 비전 기법들입니다.\n특징# 수작업으로 설계된 특징 추출기 (Hand-crafted features) 수학적으로 해석 가능한 알고리즘 적은 데이터로도 동작 계산 비용이 상대적으로 낮음 주요 기법# SIFT \u0026amp; HOG - 대표적인 특징 추출 기법 "},{"id":2,"href":"/ko/docs/architecture/multimodal/clip/","title":"CLIP","section":"Multimodal","content":"CLIP (Contrastive Language-Image Pre-training)# 개요# 논문: Learning Transferable Visual Models From Natural Language Supervision (2021) 저자: Alec Radford et al. (OpenAI) 핵심 기여: 이미지와 텍스트를 같은 공간에 임베딩하여 zero-shot 분류 핵심 아이디어# \u0026ldquo;이미지와 텍스트 쌍을 대조 학습하여 공유 임베딩 공간 구축\u0026rdquo;\n4억 개의 이미지-텍스트 쌍으로 학습하여, 한 번도 본 적 없는 클래스도 분류할 수 있습니다.\n구조# 전체 아키텍처# ┌─────────────────┐ Image ────→ │ Image Encoder │ ────→ Image Embedding (512-d) │ (ViT or ResNet)│ ↓ └─────────────────┘ ↓ (cosine similarity) ↓ ┌─────────────────┐ ↓ Text ────→ │ Text Encoder │ ────→ Text Embedding (512-d) │ (Transformer) │ └─────────────────┘인코더 옵션# Image Encoder Text Encoder ViT-B/32, ViT-B/16, ViT-L/14 Transformer (12-layer) ResNet-50, ResNet-101 Contrastive Learning# 학습 방식# 배치 내 N개의 이미지-텍스트 쌍:\nText1 Text2 Text3 ... TextN Image1 [✓] [✗] [✗] [✗] Image2 [✗] [✓] [✗] [✗] Image3 [✗] [✗] [✓] [✗] ... ImageN [✗] [✗] [✗] [✓] 대각선: 매칭 쌍 (positive) → 가깝게 비대각선: 비매칭 쌍 (negative) → 멀게 손실 함수# $$L = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(I_i, T_i)/\\tau)}{\\sum_{j=1}^{N}\\exp(\\text{sim}(I_i, T_j)/\\tau)}$$\n$\\text{sim}$: cosine similarity $\\tau$: temperature (학습 가능) 대칭적 학습# Image→Text와 Text→Image 양방향으로 학습:\n$$L_{total} = \\frac{1}{2}(L_{I \\to T} + L_{T \\to I})$$\nZero-shot Classification# 학습 없이 새로운 클래스 분류:\n# 1. 클래스 이름을 텍스트 임베딩으로 classes = [\u0026#34;a photo of a dog\u0026#34;, \u0026#34;a photo of a cat\u0026#34;, ...] text_embeddings = model.encode_text(classes) # 2. 이미지 임베딩 image_embedding = model.encode_image(image) # 3. 가장 유사한 클래스 선택 similarity = image_embedding @ text_embeddings.T prediction = similarity.argmax()Prompt Engineering# 분류 성능은 프롬프트에 따라 달라집니다:\n# 기본 \u0026#34;dog\u0026#34; # 개선 \u0026#34;a photo of a dog\u0026#34; # 더 개선 (앙상블) templates = [ \u0026#34;a photo of a {}\u0026#34;, \u0026#34;a blurry photo of a {}\u0026#34;, \u0026#34;a sculpture of a {}\u0026#34;, ... ] 구현 예시# 기본 사용# import torch import clip from PIL import Image # 모델 로드 device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model, preprocess = clip.load(\u0026#34;ViT-B/32\u0026#34;, device=device) # 이미지 전처리 image = preprocess(Image.open(\u0026#34;image.jpg\u0026#34;)).unsqueeze(0).to(device) # 텍스트 토큰화 text = clip.tokenize([\u0026#34;a dog\u0026#34;, \u0026#34;a cat\u0026#34;, \u0026#34;a bird\u0026#34;]).to(device) # 유사도 계산 with torch.no_grad(): image_features = model.encode_image(image) text_features = model.encode_text(text) # 정규화 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) # 유사도 similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1) print(similarity) # [0.95, 0.03, 0.02]OpenCLIP (open source)# import open_clip import torch model, _, preprocess = open_clip.create_model_and_transforms( \u0026#39;ViT-B-32\u0026#39;, pretrained=\u0026#39;laion2b_s34b_b79k\u0026#39; ) tokenizer = open_clip.get_tokenizer(\u0026#39;ViT-B-32\u0026#39;) # 사용법 동일 성능# Zero-shot ImageNet# Model Top-1 Accuracy CLIP ViT-B/32 63.2% CLIP ViT-B/16 68.3% CLIP ViT-L/14 75.5% CLIP ViT-L/14@336 76.2% 지도 학습 없이도 상당한 성능을 달성합니다.\nCLIP의 활용# 1. Image-Text Retrieval# # 텍스트로 이미지 검색 query_embedding = model.encode_text(\u0026#34;a red car\u0026#34;) similarities = query_embedding @ image_database.T top_images = similarities.argsort(descending=True)[:10]2. Stable Diffusion의 Text Encoder# Text Prompt → CLIP Text Encoder → Conditioning3. 이미지 유사도 검색# img1_feat = model.encode_image(img1) img2_feat = model.encode_image(img2) similarity = (img1_feat @ img2_feat.T).item()4. CLIP Score (이미지-텍스트 정합성)# def clip_score(image, text): img_feat = model.encode_image(image) txt_feat = model.encode_text(text) return (img_feat @ txt_feat.T).item() 한계점# 세밀한 이해 부족: 개수, 공간 관계, 텍스트 인식 편향: 학습 데이터의 편향 반영 일부 도메인 약함: 의료, 위성 등 특수 도메인 관련 콘텐츠# Contrastive Learning - 수학적 기초 ViT - Image encoder VLM - CLIP 발전형 Stable Diffusion - CLIP 활용 "},{"id":3,"href":"/ko/docs/architecture/detection/faster-rcnn/","title":"Faster R-CNN","section":"Detection","content":"Faster R-CNN# 개요# 논문: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015) 저자: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun 핵심 기여: Region Proposal Network(RPN)으로 end-to-end 학습 가능한 detector R-CNN 계보# R-CNN (2014) ↓ ROI Pooling 도입 Fast R-CNN (2015) ↓ RPN 도입 Faster R-CNN (2015) 모델 Region Proposal 속도 R-CNN Selective Search ~47s/image Fast R-CNN Selective Search ~2s/image Faster R-CNN RPN (학습됨) ~0.2s/image 구조# 전체 아키텍처# Input Image ↓ Backbone (ResNet/VGG) → Feature Map ↓ ┌───────────────────────────────────┐ │ Region Proposal Network │ │ Feature Map → Anchors → RoIs │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ RoI Pooling/Align │ │ 다양한 크기 RoI → 고정 크기 특징 │ └───────────────────────────────────┘ ↓ ┌───────────────────────────────────┐ │ Detection Head (R-CNN Head) │ │ Classification + Box Regression │ └───────────────────────────────────┘ ↓ Output: [class, x, y, w, h] 핵심 컴포넌트# 1. Backbone# 이미지에서 특징 맵 추출:\nVGG-16: 원 논문 ResNet-50/101: 현재 주로 사용 FPN: Feature Pyramid Network 추가 시 성능 향상 2. Region Proposal Network (RPN)# Feature map 위에서 객체 후보 영역 생성:\nFeature Map (H×W×C) ↓ 3×3 Conv ↓ ┌─────────────────────────────────┐ │ 1×1 Conv (cls) 1×1 Conv (reg) │ │ 2k scores 4k coordinates │ └─────────────────────────────────┘Anchor Box:\n각 위치에서 k개의 앵커 (기본: 9개) 3가지 크기 × 3가지 비율 = 9 anchors 크기: 128², 256², 512² 비율: 1:1, 1:2, 2:1 3. RoI Pooling / RoI Align# 다양한 크기의 RoI를 고정 크기(예: 7×7)로 변환:\nRoI Pooling: 양자화로 인한 오차 발생 RoI Align: 양자화 없이 bilinear interpolation 사용 (Mask R-CNN에서 제안)\n4. Detection Head# 각 RoI에 대해:\nClassification: softmax로 클래스 예측 Box Regression: bounding box 좌표 미세 조정 학습# Multi-task Loss# $$L = L_{cls} + \\lambda L_{reg}$$\nClassification Loss (Cross-entropy): $$L_{cls} = -\\log p_{c^*}$$\nRegression Loss (Smooth L1): $$L_{reg} = \\sum_{i \\in {x,y,w,h}} \\text{smooth}_{L_1}(t_i - t_i^*)$$\nBox Parameterization# $$t_x = (x - x_a) / w_a, \\quad t_y = (y - y_a) / h_a$$ $$t_w = \\log(w / w_a), \\quad t_h = \\log(h / h_a)$$\nTraining Strategy# RPN과 R-CNN Head를 번갈아 학습 (원 논문) 또는 end-to-end joint training (현재 주로 사용) 구현 예시# import torch import torchvision from torchvision.models.detection import fasterrcnn_resnet50_fpn # 사전 학습된 모델 로드 model = fasterrcnn_resnet50_fpn(pretrained=True) model.eval() # 추론 with torch.no_grad(): predictions = model([image]) # 결과 형태 # predictions[0][\u0026#39;boxes\u0026#39;]: (N, 4) - bounding boxes # predictions[0][\u0026#39;labels\u0026#39;]: (N,) - class labels # predictions[0][\u0026#39;scores\u0026#39;]: (N,) - confidence scores커스텀 클래스로 학습# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # backbone 고정, head만 교체 model = fasterrcnn_resnet50_fpn(pretrained=True) num_classes = 10 # 배경 포함 # Box predictor 교체 in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 성능# COCO Dataset# Backbone mAP mAP@50 VGG-16 21.9 42.7 ResNet-50 35.7 57.0 ResNet-101 37.4 58.8 ResNet-50-FPN 37.0 58.5 한계점# Two-stage 방식으로 속도 제한 작은 객체 검출 어려움 (FPN으로 완화) Anchor 설계에 도메인 지식 필요 관련 콘텐츠# YOLO - One-stage detector Mask R-CNN - Faster R-CNN + segmentation ResNet - 주로 사용되는 backbone IoU \u0026amp; NMS - Detection 핵심 개념 Detection 태스크 - 평가 지표 "},{"id":4,"href":"/ko/docs/architecture/3d/nerf/","title":"NeRF","section":"3D Vision","content":"NeRF (Neural Radiance Fields)# 개요# 논문: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020) 저자: Ben Mildenhall et al. (UC Berkeley, Google) 핵심 기여: 신경망으로 3D 장면을 암시적으로 표현 핵심 아이디어# \u0026ldquo;MLP로 3D 공간의 색상과 밀도를 학습\u0026rdquo;\n여러 시점의 이미지만으로 3D 장면을 완전히 재구성합니다.\n입출력# 입력# 위치: $(x, y, z)$ - 3D 공간의 점 방향: $(\\theta, \\phi)$ - 시선 방향 출력# 색상: $(r, g, b)$ - RGB 값 밀도: $\\sigma$ - 불투명도 (x, y, z, θ, φ) → MLP → (r, g, b, σ) 구조# 전체 아키텍처# ┌─────────────────────────────────────────────────────────┐ │ NeRF │ │ │ │ Position (x,y,z) ──→ Positional ──→ ┌─────────────┐ │ │ Encoding │ │ │ │ ↓ │ MLP │ │ │ γ(x,y,z) ──→ │ (8 layers)│──→ σ (density) │ │ 256 dim │ │ │ Direction (θ,φ) ──→ Positional ──→ │ │ │ │ Encoding └──────┬──────┘ │ │ ↓ ↓ │ │ γ(θ,φ) ──→ [Additional MLP]──→ (r,g,b) color │ │ └─────────────────────────────────────────────────────────┘Positional Encoding# 고주파 정보를 학습하기 위해 위치를 고차원으로 매핑:\n$$\\gamma(p) = (\\sin(2^0\\pi p), \\cos(2^0\\pi p), \u0026hellip;, \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p))$$\n위치: $L=10$ (60차원) 방향: $L=4$ (24차원) 왜 Positional Encoding이 필요한가?# MLP는 저주파 함수에 편향되어 있어서, 고주파 디테일(텍스처, 엣지)을 학습하기 어렵습니다.\nVolume Rendering# 광선 추적# 카메라에서 픽셀을 통해 광선을 쏘고, 광선 위의 점들을 샘플링:\n$$r(t) = o + td$$\n$o$: 카메라 원점 $d$: 광선 방향 $t$: 광선 위의 거리 색상 계산# 광선 위의 모든 점에서 색상과 밀도를 적분:\n$$C(r) = \\int_{t_n}^{t_f} T(t) \\sigma(r(t)) c(r(t), d) dt$$\n여기서 투과율:\n$$T(t) = \\exp\\left(-\\int_{t_n}^{t} \\sigma(r(s)) ds\\right)$$\n이산화 (실제 구현)# $$\\hat{C}(r) = \\sum_{i=1}^{N} T_i (1 - \\exp(-\\sigma_i \\delta_i)) c_i$$\n$$T_i = \\exp\\left(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right)$$\nHierarchical Sampling# 2단계 샘플링# 효율적인 렌더링을 위해 두 개의 네트워크 사용:\n1. Coarse Network: 균등 샘플링으로 대략적인 밀도 파악 2. Fine Network: 밀도 높은 영역에 집중 샘플링# Coarse: 64 samples (균등) t_coarse = torch.linspace(near, far, 64) weights = render_weights(coarse_network, t_coarse) # Fine: 64 + 128 samples (가중 샘플링) t_fine = sample_pdf(t_coarse, weights, 128) t_all = torch.sort(torch.cat([t_coarse, t_fine]))[0] color = render(fine_network, t_all) 학습# 손실 함수# 렌더링된 색상과 실제 이미지 픽셀의 차이:\n$$L = \\sum_{r \\in R} \\left[ | \\hat{C}_c(r) - C(r) |^2 + | \\hat{C}_f(r) - C(r) |^2 \\right]$$\n학습 데이터# 다양한 시점의 이미지 (보통 100장 이상) 각 이미지의 카메라 포즈 (COLMAP으로 추정) 학습 시간# 단일 장면: 1-2일 (V100 GPU 기준) 매우 느린 것이 단점 구현 예시# 기본 MLP# import torch import torch.nn as nn class NeRF(nn.Module): def __init__(self, D=8, W=256, input_ch=63, input_ch_views=27): super().__init__() self.D = D self.W = W self.skips = [4] # skip connection at layer 4 # Position encoding layers self.pts_linears = nn.ModuleList( [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)] ) # Density output self.sigma_linear = nn.Linear(W, 1) # Color output self.feature_linear = nn.Linear(W, W) self.views_linear = nn.Linear(W + input_ch_views, W // 2) self.rgb_linear = nn.Linear(W // 2, 3) def forward(self, x, d): # x: position, d: direction h = x for i, layer in enumerate(self.pts_linears): h = torch.relu(layer(h)) if i in self.skips: h = torch.cat([x, h], dim=-1) sigma = self.sigma_linear(h) feature = self.feature_linear(h) h = torch.cat([feature, d], dim=-1) h = torch.relu(self.views_linear(h)) rgb = torch.sigmoid(self.rgb_linear(h)) return rgb, sigmaPositional Encoding# def positional_encoding(x, L): \u0026#34;\u0026#34;\u0026#34; x: (..., D) returns: (..., D * 2 * L) \u0026#34;\u0026#34;\u0026#34; freqs = 2.0 ** torch.arange(L, device=x.device) x_freq = x[..., None] * freqs # (..., D, L) encoded = torch.cat([ torch.sin(x_freq), torch.cos(x_freq) ], dim=-1) # (..., D, 2L) return encoded.flatten(-2) # (..., D * 2L)Volume Rendering# def render_rays(network, rays_o, rays_d, near, far, N_samples): # Sample points along ray t = torch.linspace(near, far, N_samples) pts = rays_o[..., None, :] + rays_d[..., None, :] * t[..., :, None] # Get colors and densities pts_flat = pts.reshape(-1, 3) dirs_flat = rays_d[..., None, :].expand_as(pts).reshape(-1, 3) rgb, sigma = network( positional_encoding(pts_flat, L=10), positional_encoding(dirs_flat, L=4) ) rgb = rgb.reshape(*pts.shape[:-1], 3) sigma = sigma.reshape(*pts.shape[:-1]) # Volume rendering delta = t[..., 1:] - t[..., :-1] alpha = 1 - torch.exp(-sigma[..., :-1] * delta) T = torch.cumprod(1 - alpha + 1e-10, dim=-1) T = torch.cat([torch.ones_like(T[..., :1]), T[..., :-1]], dim=-1) weights = alpha * T rgb_map = (weights[..., None] * rgb[..., :-1, :]).sum(dim=-2) return rgb_map NeRF 변형들# 속도 개선# 모델 특징 속도 향상 Instant-NGP Hash encoding 1000x Plenoxels Voxel grid 100x TensoRF Tensor decomposition 100x 기능 확장# 모델 기능 Mip-NeRF 안티앨리어싱 NeRF-W 야외 장면, 조명 변화 D-NeRF 동적 장면 NeRF++ 무한 장면 Instant-NGP# 핵심 아이디어# 다해상도 해시 테이블로 위치 인코딩 대체:\nPosition → Multi-resolution Hash → Small MLP → (rgb, σ)장점# 학습: 수 분 렌더링: 실시간 품질 유지 한계점# 학습 시간: 장면당 수 시간 ~ 수 일 렌더링 속도: 실시간 어려움 정적 장면: 동적 장면 처리 어려움 메모리: 고해상도 시 많은 샘플 필요 관련 콘텐츠# 3D Gaussian Splatting - 실시간 대안 MLP - 기본 네트워크 ViT - Vision 모델 비교 "},{"id":5,"href":"/ko/docs/engineering/deployment/onnx/","title":"ONNX","section":"배포","content":"ONNX (Open Neural Network Exchange)# 개요# ONNX는 딥러닝 모델의 표준 교환 포맷입니다.\nPyTorch ─┐ TensorFlow ─┼──→ ONNX ──→ TensorRT / OpenVINO / CoreML / ... Keras ─┘ PyTorch → ONNX 변환# 기본 변환# import torch import torch.onnx model = MyModel() model.load_state_dict(torch.load(\u0026#39;model.pt\u0026#39;)) model.eval() # 더미 입력 (모델 구조 추론용) dummy_input = torch.randn(1, 3, 224, 224) # ONNX 변환 torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, input_names=[\u0026#39;input\u0026#39;], output_names=[\u0026#39;output\u0026#39;], dynamic_axes={ \u0026#39;input\u0026#39;: {0: \u0026#39;batch_size\u0026#39;}, \u0026#39;output\u0026#39;: {0: \u0026#39;batch_size\u0026#39;} }, opset_version=17, )주요 파라미터# 파라미터 설명 input_names 입력 텐서 이름 output_names 출력 텐서 이름 dynamic_axes 동적 차원 (배치 크기 등) opset_version ONNX 연산자 버전 ONNX 검증# 구조 확인# import onnx # 모델 로드 및 검증 model = onnx.load(\u0026#34;model.onnx\u0026#34;) onnx.checker.check_model(model) # 그래프 정보 print(onnx.helper.printable_graph(model.graph))Netron으로 시각화# pip install netron netron model.onnx ONNX Runtime 추론# 기본 추론# import onnxruntime as ort import numpy as np # 세션 생성 session = ort.InferenceSession( \u0026#34;model.onnx\u0026#34;, providers=[\u0026#39;CUDAExecutionProvider\u0026#39;, \u0026#39;CPUExecutionProvider\u0026#39;] ) # 입력 정보 input_name = session.get_inputs()[0].name output_name = session.get_outputs()[0].name # 추론 input_data = np.random.randn(1, 3, 224, 224).astype(np.float32) result = session.run([output_name], {input_name: input_data})성능 최적화# # 세션 옵션 sess_options = ort.SessionOptions() sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL sess_options.intra_op_num_threads = 4 # GPU 옵션 providers = [ (\u0026#39;CUDAExecutionProvider\u0026#39;, { \u0026#39;device_id\u0026#39;: 0, \u0026#39;arena_extend_strategy\u0026#39;: \u0026#39;kNextPowerOfTwo\u0026#39;, }), \u0026#39;CPUExecutionProvider\u0026#39; ] session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;, sess_options, providers=providers) 동적 입력 처리# 가변 배치 크기# torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, dynamic_axes={ \u0026#39;input\u0026#39;: {0: \u0026#39;batch\u0026#39;}, \u0026#39;output\u0026#39;: {0: \u0026#39;batch\u0026#39;} } )가변 이미지 크기# torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, dynamic_axes={ \u0026#39;input\u0026#39;: {0: \u0026#39;batch\u0026#39;, 2: \u0026#39;height\u0026#39;, 3: \u0026#39;width\u0026#39;}, \u0026#39;output\u0026#39;: {0: \u0026#39;batch\u0026#39;} } ) 일반적인 문제 해결# 지원되지 않는 연산자# # 커스텀 연산자 등록 from torch.onnx import register_custom_op_symbolic def my_custom_op(g, input): return g.op(\u0026#34;CustomOp\u0026#34;, input) register_custom_op_symbolic(\u0026#39;my_custom_op\u0026#39;, my_custom_op, opset_version=11)동적 제어 흐름# # torch.jit.script 사용 @torch.jit.script def my_function(x): if x.sum() \u0026gt; 0: return x * 2 return x # 또는 조건부 로직 제거값 불일치# # PyTorch와 ONNX Runtime 출력 비교 import numpy as np torch_output = model(input_tensor).detach().numpy() onnx_output = session.run([output_name], {input_name: input_data})[0] np.testing.assert_allclose(torch_output, onnx_output, rtol=1e-3, atol=1e-5) ONNX 모델 최적화# onnxoptimizer# import onnx from onnxoptimizer import optimize model = onnx.load(\u0026#34;model.onnx\u0026#34;) optimized_model = optimize(model, [ \u0026#39;eliminate_deadend\u0026#39;, \u0026#39;eliminate_identity\u0026#39;, \u0026#39;fuse_consecutive_transposes\u0026#39;, \u0026#39;fuse_bn_into_conv\u0026#39;, ]) onnx.save(optimized_model, \u0026#34;model_optimized.onnx\u0026#34;)onnx-simplifier# pip install onnx-simplifier onnxsim model.onnx model_simplified.onnximport onnx from onnxsim import simplify model = onnx.load(\u0026#34;model.onnx\u0026#34;) simplified_model, check = simplify(model) onnx.save(simplified_model, \u0026#34;model_simplified.onnx\u0026#34;) 벤치마크# import time import numpy as np # 워밍업 for _ in range(10): session.run([output_name], {input_name: input_data}) # 측정 n_runs = 100 start = time.time() for _ in range(n_runs): session.run([output_name], {input_name: input_data}) elapsed = time.time() - start print(f\u0026#34;Average latency: {elapsed / n_runs * 1000:.2f} ms\u0026#34;) print(f\u0026#34;Throughput: {n_runs / elapsed:.2f} FPS\u0026#34;) 관련 콘텐츠# TensorRT - GPU 최적화 모델 서빙 - 추론 서버 최적화 - 양자화 "},{"id":6,"href":"/ko/docs/architecture/classical/sift-hog/","title":"SIFT \u0026 HOG","section":"Classical CV","content":"SIFT \u0026amp; HOG# 개요# 딥러닝 이전 시대의 대표적인 특징 추출 기법입니다.\n기법 연도 핵심 아이디어 SIFT 2004 스케일 불변 키포인트 검출 HOG 2005 그래디언트 방향 히스토그램 SIFT (Scale-Invariant Feature Transform)# 핵심 아이디어# 이미지의 크기나 회전에 관계없이 동일한 특징점을 찾아내는 알고리즘입니다.\n동작 과정# Scale-space 생성: Gaussian blur를 다양한 스케일로 적용 DoG (Difference of Gaussian): 인접한 스케일 간 차이 계산 키포인트 검출: DoG에서 극값(extrema) 찾기 방향 할당: 키포인트 주변의 그래디언트 방향 계산 디스크립터 생성: 128차원 벡터로 특징 표현 수식# Gaussian blur: $$L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)$$\nDifference of Gaussian: $$D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)$$\n특징# 장점: 스케일, 회전, 조명 변화에 강건 단점: 계산 비용이 높음, 실시간 처리 어려움 HOG (Histogram of Oriented Gradients)# 핵심 아이디어# 이미지를 셀 단위로 나누고, 각 셀에서 그래디언트 방향의 히스토그램을 계산합니다.\n동작 과정# 그래디언트 계산: 각 픽셀에서 x, y 방향 그래디언트 계산 셀 분할: 이미지를 8x8 픽셀 셀로 분할 히스토그램 생성: 각 셀에서 9개 방향(0°~180°)의 히스토그램 계산 블록 정규화: 2x2 셀을 하나의 블록으로 묶어 정규화 특징 벡터 생성: 모든 블록의 히스토그램을 연결 수식# 그래디언트 크기와 방향: $$G = \\sqrt{G_x^2 + G_y^2}$$ $$\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n특징# 장점: 조명 변화에 강건, 계산이 상대적으로 빠름 단점: 스케일 변화에 민감 주요 용도: 보행자 검출 (Dalal \u0026amp; Triggs, 2005) 구현 예시# import cv2 # SIFT sift = cv2.SIFT_create() keypoints, descriptors = sift.detectAndCompute(image, None) # HOG hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) boxes, weights = hog.detectMultiScale(image) 딥러닝과의 비교# 측면 SIFT/HOG CNN 특징 설계 수작업 학습 기반 데이터 요구량 적음 많음 성능 제한적 우수 해석 가능성 높음 낮음 일반화 제한적 우수 현재는 대부분의 태스크에서 딥러닝이 SIFT/HOG를 대체했지만, 특징 매칭이나 엣지 케이스에서는 여전히 사용됩니다.\n관련 콘텐츠# 선형대수 - 그래디언트 계산의 기초 CNN 기초 - SIFT/HOG를 대체한 딥러닝 기법 "},{"id":7,"href":"/ko/docs/architecture/segmentation/unet/","title":"U-Net","section":"Segmentation","content":"U-Net# 개요# 논문: U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) 저자: Olaf Ronneberger, Philipp Fischer, Thomas Brox 핵심 기여: Encoder-Decoder + Skip Connection으로 정밀한 segmentation 핵심 아이디어# \u0026ldquo;수축 경로(encoder)로 context를 잡고, 확장 경로(decoder)로 정밀한 localization\u0026rdquo;\n의료 영상처럼 데이터가 적은 상황에서도 효과적으로 동작합니다.\n구조# 전체 아키텍처# Input (572×572) ↓ ┌───────────────────────────────────────────────────────────┐ │ Contracting Path │ │ [Conv3×3 → ReLU → Conv3×3 → ReLU → MaxPool] × 4 │ │ 64 → 128 → 256 → 512 → 1024 │ └───────────────────────────────────────────────────────────┘ ↓ ┌───────────────────────────────────────────────────────────┐ │ Expanding Path │ │ [UpConv2×2 → Concat(skip) → Conv3×3 → ReLU × 2] × 4 │ │ 1024 → 512 → 256 → 128 → 64 │ └───────────────────────────────────────────────────────────┘ ↓ Conv 1×1 → Output (388×388×num_classes)U자 형태# Input Output ↓ ↑ [Conv]─────────────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]───────────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]─────────────────────────→[UpConv+Concat] ↓ ↑ [Conv]──────────────────→[UpConv+Concat] ↓ ↑ [Bottleneck] 핵심 컴포넌트# 1. Contracting Path (Encoder)# 3×3 Conv (unpadded) + ReLU × 2 2×2 Max Pooling (stride 2) 채널 수 2배씩 증가 2. Expanding Path (Decoder)# 2×2 Up-convolution (채널 절반) Skip connection으로 encoder 특징 concat 3×3 Conv + ReLU × 2 3. Skip Connection# Encoder의 고해상도 특징을 decoder로 전달:\n위치 정보 보존 정밀한 경계 복원 구현 예시# import torch import torch.nn as nn class DoubleConv(nn.Module): def __init__(self, in_ch, out_ch): super().__init__() self.conv = nn.Sequential( nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True) ) def forward(self, x): return self.conv(x) class UNet(nn.Module): def __init__(self, in_channels=3, out_channels=1): super().__init__() # Encoder self.enc1 = DoubleConv(in_channels, 64) self.enc2 = DoubleConv(64, 128) self.enc3 = DoubleConv(128, 256) self.enc4 = DoubleConv(256, 512) self.pool = nn.MaxPool2d(2) # Bottleneck self.bottleneck = DoubleConv(512, 1024) # Decoder self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2) self.dec4 = DoubleConv(1024, 512) self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2) self.dec3 = DoubleConv(512, 256) self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2) self.dec2 = DoubleConv(256, 128) self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2) self.dec1 = DoubleConv(128, 64) # Output self.out = nn.Conv2d(64, out_channels, 1) def forward(self, x): # Encoder e1 = self.enc1(x) e2 = self.enc2(self.pool(e1)) e3 = self.enc3(self.pool(e2)) e4 = self.enc4(self.pool(e3)) # Bottleneck b = self.bottleneck(self.pool(e4)) # Decoder + Skip connections d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1)) d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1)) d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1)) d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1)) return self.out(d1) 학습# Loss Function# Binary Segmentation: $$L = \\text{BCE}(y, \\hat{y}) + \\text{Dice Loss}$$\nMulti-class: $$L = \\text{CrossEntropy}(y, \\hat{y})$$\nData Augmentation# 의료 영상은 데이터가 적으므로 augmentation이 중요:\nElastic deformation 회전, 뒤집기 그레이스케일 변환 U-Net 변형# 변형 특징 U-Net++ 중첩된 skip connection Attention U-Net Attention gate 추가 ResUNet ResNet 블록 사용 TransUNet Transformer encoder 활용 분야# 의료 영상: 세포, 종양, 장기 segmentation 위성 영상: 건물, 도로 추출 자율주행: 도로 영역 분할 생성 모델: Stable Diffusion의 구조에 영향 관련 콘텐츠# Transposed Convolution - Upsampling 원리 ResNet - Skip connection 아이디어 Mask R-CNN - Instance segmentation Segmentation 태스크 - 평가 지표 "},{"id":8,"href":"/ko/docs/architecture/generative/vae/","title":"VAE","section":"Generative","content":"VAE (Variational Autoencoder)# 개요# 논문: Auto-Encoding Variational Bayes (2013) 저자: Diederik P. Kingma, Max Welling 핵심 기여: 확률적 잠재 공간에서 생성 가능한 오토인코더 핵심 아이디어# \u0026ldquo;데이터를 압축하는 동시에 잠재 공간을 정규화하여 생성 가능하게\u0026rdquo;\n일반 Autoencoder는 데이터 압축만 하지만, VAE는 잠재 공간이 연속적이고 의미있는 구조를 갖도록 학습합니다.\nAutoencoder vs VAE# 일반 Autoencoder# x → Encoder → z (deterministic) → Decoder → x̂ z가 불연속적 학습 데이터 외의 z에서 생성 불가 VAE# x → Encoder → (μ, σ) → z ~ N(μ, σ²) → Decoder → x̂ z가 확률 분포 잠재 공간이 연속적 → 생성 가능 구조# 전체 아키텍처# Input x ↓ ┌─────────────────────────┐ │ Encoder │ │ x → hidden → (μ, σ) │ └─────────────────────────┘ ↓ ┌─────────────────────────┐ │ Reparameterization │ │ z = μ + σ * ε │ │ where ε ~ N(0, 1) │ └─────────────────────────┘ ↓ ┌─────────────────────────┐ │ Decoder │ │ z → hidden → x̂ │ └─────────────────────────┘ ↓ Output x̂Reparameterization Trick# Gradient가 샘플링을 통과하도록:\n$$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n이렇게 하면 $\\mu$, $\\sigma$에 대해 역전파 가능합니다.\n손실 함수# ELBO (Evidence Lower Bound)# $$\\mathcal{L} = -\\mathbb{E}{q(z|x)}[\\log p(x|z)] + D{KL}(q(z|x) | p(z))$$\n두 가지 항# 1. Reconstruction Loss: $$\\mathcal{L}_{recon} = |x - \\hat{x}|^2 \\quad \\text{or} \\quad \\text{BCE}(x, \\hat{x})$$\n2. KL Divergence: $$\\mathcal{L}{KL} = -\\frac{1}{2}\\sum{j=1}^{J}(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)$$\n잠재 분포 $q(z|x)$를 표준 정규분포 $p(z) = \\mathcal{N}(0, I)$에 가깝게.\n전체 손실# $$\\mathcal{L} = \\mathcal{L}{recon} + \\beta \\cdot \\mathcal{L}{KL}$$\n$\\beta = 1$: 표준 VAE $\\beta \u0026gt; 1$: β-VAE (더 disentangled) 구현 예시# import torch import torch.nn as nn import torch.nn.functional as F class VAE(nn.Module): def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20): super().__init__() # Encoder self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc_mu = nn.Linear(hidden_dim, latent_dim) self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # Decoder self.fc3 = nn.Linear(latent_dim, hidden_dim) self.fc4 = nn.Linear(hidden_dim, input_dim) def encode(self, x): h = F.relu(self.fc1(x)) mu = self.fc_mu(h) logvar = self.fc_logvar(h) return mu, logvar def reparameterize(self, mu, logvar): std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def decode(self, z): h = F.relu(self.fc3(z)) return torch.sigmoid(self.fc4(h)) def forward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) x_recon = self.decode(z) return x_recon, mu, logvar def vae_loss(x_recon, x, mu, logvar): # Reconstruction loss recon_loss = F.binary_cross_entropy(x_recon, x, reduction=\u0026#39;sum\u0026#39;) # KL divergence kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return recon_loss + kl_loss 생성# 학습 후 새로운 이미지 생성:\n# 표준 정규분포에서 샘플링 z = torch.randn(batch_size, latent_dim) # Decoder로 이미지 생성 generated = model.decode(z)잠재 공간 탐색# # 두 이미지 사이 보간 z1, z2 = model.encode(img1), model.encode(img2) for alpha in np.linspace(0, 1, 10): z_interp = alpha * z1 + (1 - alpha) * z2 img_interp = model.decode(z_interp) VAE 변형# 변형 핵심 아이디어 β-VAE KL 가중치 증가 → disentanglement VQ-VAE 이산 잠재 공간 CVAE 조건부 생성 NVAE 계층적 구조, 고해상도 한계점# 생성된 이미지가 흐릿함 (pixel-wise loss 한계) 고해상도 생성 어려움 GAN 대비 선명도 부족 이러한 한계 때문에 현재는 Diffusion 모델이 주류가 되었습니다.\nVAE의 역할# 현재 VAE 자체보다 VAE Encoder가 중요하게 사용됩니다:\nStable Diffusion: VAE로 latent space 압축 VQ-VAE → DALL-E: 이산 토큰 생성 관련 콘텐츠# 확률분포 - VAE의 수학적 기초 GAN - 적대적 생성 모델 Stable Diffusion - VAE를 encoder로 사용 Generation 태스크 - 평가 지표 "},{"id":9,"href":"/ko/docs/architecture/transformer/vit/","title":"ViT","section":"Transformer","content":"ViT (Vision Transformer)# 개요# 논문: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) 저자: Alexey Dosovitskiy et al. (Google) 핵심 기여: 이미지를 패치로 나누어 순수 Transformer로 처리 핵심 아이디어# \u0026ldquo;이미지를 단어처럼 취급하자\u0026rdquo;\n이미지를 16×16 패치로 나누고, 각 패치를 토큰처럼 Transformer에 입력합니다.\n구조# 전체 아키텍처# Input Image (224×224×3) ↓ Split into patches (14×14 = 196 patches of 16×16) ↓ Linear projection (flatten + linear → D dim) ↓ + Positional Embedding + [CLS] token ↓ ┌─────────────────────────────┐ │ Transformer Encoder │ │ [Multi-Head Attn + FFN]×L │ └─────────────────────────────┘ ↓ [CLS] token output ↓ MLP Head → Classification핵심 컴포넌트# 컴포넌트 설명 Patch Embedding 16×16×3 → D 차원 벡터 [CLS] Token 전체 이미지 표현 학습 Position Embedding 학습 가능한 위치 인코딩 Transformer Encoder L개 레이어 Patch Embedding# # 이미지를 패치로 분할 후 선형 변환 # (B, 3, 224, 224) → (B, 196, 768) class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super().__init__() self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): # (B, C, H, W) → (B, embed_dim, H/P, W/P) → (B, num_patches, embed_dim) x = self.proj(x) x = x.flatten(2).transpose(1, 2) return x 모델 변형# 모델 Layers Hidden Heads Params ViT-Base 12 768 12 86M ViT-Large 24 1024 16 307M ViT-Huge 32 1280 16 632M 학습# 사전학습의 중요성# ViT는 CNN과 달리 inductive bias가 적어서 대규모 데이터가 필수:\n학습 데이터 ImageNet Accuracy ImageNet-1k (1.2M) 79.7% (ResNet보다 낮음) ImageNet-21k (14M) 84.2% JFT-300M 88.5% 학습 설정# Optimizer: Adam (β₁=0.9, β₂=0.999) Learning rate: warmup + cosine decay Augmentation: RandAugment, Mixup, CutMix Regularization: Dropout, Stochastic Depth 구현 예시# import torch import torch.nn as nn class ViT(nn.Module): def __init__( self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.1 ): super().__init__() num_patches = (img_size // patch_size) ** 2 # Patch embedding self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) # CLS token and position embedding self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) self.dropout = nn.Dropout(dropout) # Transformer encoder self.blocks = nn.ModuleList([ TransformerBlock(embed_dim, num_heads, embed_dim * mlp_ratio, dropout) for _ in range(depth) ]) self.norm = nn.LayerNorm(embed_dim) # Classification head self.head = nn.Linear(embed_dim, num_classes) def forward(self, x): B = x.shape[0] # Patch embedding x = self.patch_embed(x).flatten(2).transpose(1, 2) # Add CLS token cls_tokens = self.cls_token.expand(B, -1, -1) x = torch.cat([cls_tokens, x], dim=1) # Add position embedding x = x + self.pos_embed x = self.dropout(x) # Transformer encoder for block in self.blocks: x = block(x) x = self.norm(x) # Classification (CLS token만 사용) return self.head(x[:, 0])HuggingFace 사용# from transformers import ViTForImageClassification, ViTImageProcessor processor = ViTImageProcessor.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) model = ViTForImageClassification.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) inputs = processor(images=image, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) predicted_class = outputs.logits.argmax(-1).item() ViT의 특징# Attention 시각화# ViT는 이미지의 어떤 부분에 주목하는지 시각화 가능:\n# 마지막 레이어의 attention map attentions = model.get_last_selfattention(x) # CLS token이 어디를 보는지 확인 cls_attention = attentions[:, :, 0, 1:] # (B, heads, num_patches)장점# 전역적 context 이해 확장성 우수 (모델/데이터 scale up 가능) 다양한 태스크 transfer 용이 단점# 대규모 데이터 필요 계산량 O(n²) 작은 객체/고해상도 처리 어려움 ViT 변형# 모델 핵심 개선 DeiT Knowledge distillation으로 적은 데이터 학습 Swin Shifted window로 효율적 계산 CvT Conv 추가로 inductive bias 강화 BEiT BERT 스타일 사전학습 관련 콘텐츠# Transformer - 기반 아키텍처 ResNet - 대조되는 CNN 접근 CLIP - ViT를 image encoder로 사용 DiT - Diffusion + Transformer "},{"id":10,"href":"/ko/docs/engineering/data/","title":"데이터","section":"실무 기술","content":"데이터 엔지니어링# 데이터 수집, 처리, 증강에 관한 실무 기술입니다.\n주요 내용# 주제 설명 데이터 증강 이미지 변환, Cutout, MixUp, AutoAugment 데이터 포맷 COCO, Pascal VOC, YOLO 포맷 데이터 파이프라인 DataLoader, 전처리 최적화 레이블링 도구, 품질 관리, 자동화 데이터의 중요성# \u0026ldquo;Garbage in, garbage out\u0026rdquo;\n아무리 좋은 모델도 데이터 품질이 나쁘면 성능이 제한됩니다.\n데이터 vs 모델# 상황 우선순위 데이터 부족 증강, 전이학습 데이터 품질 낮음 정제, 레이블링 개선 데이터 불균형 샘플링, 가중치 조정 데이터 충분 모델 개선 "},{"id":11,"href":"/ko/docs/engineering/data/augmentation/","title":"데이터 증강","section":"데이터","content":"데이터 증강 (Data Augmentation)# 개요# 데이터 증강은 기존 데이터에 변환을 적용해 학습 데이터를 늘리는 기법입니다.\n기본 증강 기법# 기하학적 변환# import albumentations as A transform = A.Compose([ A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.Rotate(limit=30, p=0.5), A.RandomScale(scale_limit=0.2, p=0.5), A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5), ]) 기법 설명 주의사항 Flip 좌우/상하 반전 방향 의미 있는 태스크 주의 Rotate 회전 Detection bbox 변환 필요 Scale 크기 조정 객체 크기 분포 고려 Crop 랜덤 크롭 객체 잘림 확인 색상 변환# transform = A.Compose([ A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5), A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5), A.GaussNoise(var_limit=(10, 50), p=0.3), A.GaussianBlur(blur_limit=7, p=0.3), ]) 고급 증강 기법# Cutout / Random Erasing# 이미지의 일부를 가려서 robustness 향상:\ntransform = A.Compose([ A.CoarseDropout( max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5 ), ])MixUp# 두 이미지와 레이블을 혼합:\ndef mixup(x1, y1, x2, y2, alpha=0.2): lam = np.random.beta(alpha, alpha) x = lam * x1 + (1 - lam) * x2 y = lam * y1 + (1 - lam) * y2 return x, yCutMix# 한 이미지의 일부를 다른 이미지로 대체:\ndef cutmix(x1, y1, x2, y2, alpha=1.0): lam = np.random.beta(alpha, alpha) # 박스 좌표 계산 H, W = x1.shape[1:3] cut_ratio = np.sqrt(1 - lam) cut_h, cut_w = int(H * cut_ratio), int(W * cut_ratio) cx, cy = np.random.randint(W), np.random.randint(H) x1_box = np.clip(cx - cut_w // 2, 0, W) x2_box = np.clip(cx + cut_w // 2, 0, W) y1_box = np.clip(cy - cut_h // 2, 0, H) y2_box = np.clip(cy + cut_h // 2, 0, H) # 이미지 합성 x = x1.clone() x[:, y1_box:y2_box, x1_box:x2_box] = x2[:, y1_box:y2_box, x1_box:x2_box] # 레이블 비율 조정 lam = 1 - (y2_box - y1_box) * (x2_box - x1_box) / (H * W) y = lam * y1 + (1 - lam) * y2 return x, yMosaic (YOLO)# 4개의 이미지를 하나로 합침:\n┌─────┬─────┐ │ img1│ img2│ ├─────┼─────┤ │ img3│ img4│ └─────┴─────┘ AutoAugment# 개념# 최적의 증강 정책을 자동으로 탐색:\nfrom torchvision.transforms import autoaugment transform = autoaugment.AutoAugment( policy=autoaugment.AutoAugmentPolicy.IMAGENET )변형들# 방법 특징 AutoAugment 강화학습으로 탐색 RandAugment 단순화된 무작위 선택 TrivialAugment 더 단순화 RandAugment# from torchvision.transforms import RandAugment transform = RandAugment( num_ops=2, # 적용할 변환 개수 magnitude=9, # 변환 강도 (0-30) ) Detection용 증강# Detection에서는 bbox도 함께 변환해야 합니다:\nimport albumentations as A transform = A.Compose([ A.HorizontalFlip(p=0.5), A.RandomScale(scale_limit=0.2, p=0.5), A.RandomCrop(width=640, height=640, p=0.5), ], bbox_params=A.BboxParams( format=\u0026#39;pascal_voc\u0026#39;, # [x_min, y_min, x_max, y_max] min_area=100, # 너무 작은 bbox 제거 min_visibility=0.3, # 잘린 bbox 최소 비율 )) # 사용 transformed = transform( image=image, bboxes=bboxes, class_labels=labels ) Segmentation용 증강# 마스크도 동일하게 변환:\ntransform = A.Compose([ A.HorizontalFlip(p=0.5), A.Rotate(limit=30, p=0.5), A.ElasticTransform(p=0.3), # 의료영상에 효과적 ]) # 사용 transformed = transform(image=image, mask=mask) 도메인별 증강# 의료 영상# transform = A.Compose([ A.ElasticTransform(alpha=120, sigma=6, p=0.3), A.GridDistortion(p=0.3), A.CLAHE(clip_limit=4.0, p=0.5), # 대비 향상 ])위성/항공 영상# transform = A.Compose([ A.RandomRotate90(p=0.5), A.Transpose(p=0.5), A.RandomBrightnessContrast(p=0.5), ])야간/저조도# transform = A.Compose([ A.RandomGamma(gamma_limit=(80, 120), p=0.5), A.GaussNoise(var_limit=(10, 100), p=0.5), ]) 증강 전략# Train vs Validation# # Train: 강한 증강 train_transform = A.Compose([ A.RandomResizedCrop(224, 224), A.HorizontalFlip(p=0.5), A.RandAugment(num_ops=2, magnitude=9), A.Normalize(), ]) # Validation: 증강 없음 또는 최소화 val_transform = A.Compose([ A.Resize(256, 256), A.CenterCrop(224, 224), A.Normalize(), ])TTA (Test-Time Augmentation)# 추론 시 여러 증강 적용 후 평균:\ndef tta_predict(model, image, transforms): predictions = [] for transform in transforms: augmented = transform(image=image)[\u0026#39;image\u0026#39;] pred = model(augmented) predictions.append(pred) return np.mean(predictions, axis=0) tta_transforms = [ A.Compose([]), # 원본 A.Compose([A.HorizontalFlip(p=1.0)]), A.Compose([A.VerticalFlip(p=1.0)]), ] 주의사항# 과도한 증강: 오히려 성능 저하 가능 태스크 특성 고려: 좌우 방향이 중요한 경우 Flip 주의 검증 세트: 증강 없이 평가해야 공정한 비교 계산 비용: 실시간 증강 vs 사전 증강 트레이드오프 관련 콘텐츠# 데이터 파이프라인 - DataLoader 최적화 Classification - 분류 태스크 Detection - 객체 탐지 태스크 "},{"id":12,"href":"/ko/docs/engineering/hardware/camera/","title":"카메라","section":"하드웨어","content":"카메라# 개요# Vision 시스템의 핵심 구성 요소인 카메라에 대해 알아봅니다.\n이미지 센서# CCD vs CMOS# 특성 CCD CMOS 노이즈 낮음 보통~낮음 속도 느림 빠름 전력 높음 낮음 비용 비쌈 저렴 현재 추세 감소 대세 센서 크기# ┌─────────────────────┐ │ Full Frame │ │ (36 x 24 mm) │ │ ┌───────────────┐ │ │ │ APS-C │ │ │ │ (23 x 15 mm) │ │ │ │ ┌─────────┐ │ │ │ │ │ 1 inch │ │ │ │ │ │(13x9mm) │ │ │ │ │ └─────────┘ │ │ │ └───────────────┘ │ └─────────────────────┘센서가 클수록:\n더 많은 빛 수집 (저조도 성능) 더 얕은 심도 (배경 흐림) 높은 비용 해상도 선택# 필요 해상도 = (검사 영역 크기) / (최소 검출 크기) × 여유 계수 예: 100mm 영역에서 0.1mm 결함 검출 → 100 / 0.1 × 2 = 2000 pixels → 최소 2MP 카메라 필요 렌즈# 초점 거리 (Focal Length)# # 필요한 초점 거리 계산 def calculate_focal_length(sensor_size, object_size, working_distance): \u0026#34;\u0026#34;\u0026#34; sensor_size: 센서 크기 (mm) object_size: 촬영 영역 크기 (mm) working_distance: 카메라-피사체 거리 (mm) \u0026#34;\u0026#34;\u0026#34; focal_length = (sensor_size * working_distance) / object_size return focal_length # 예: 1/2\u0026#34; 센서, 100mm 영역, 200mm 거리 fl = calculate_focal_length(6.4, 100, 200) # ≈ 12.8mmF-Number (조리개)# 밝기: 낮은 F = 밝음 (F1.4 \u0026gt; F2.8 \u0026gt; F5.6) 심도: 높은 F = 깊은 심도 (더 넓은 범위가 선명) F-Number 밝기 심도 용도 F1.4-2.8 밝음 얕음 저조도, 고속 F4-5.6 보통 보통 일반 F8-16 어두움 깊음 검사, 정밀 측정 렌즈 종류# 종류 특징 용도 일반 렌즈 원근감 있음, 저렴 일반 촬영 텔레센트릭 원근감 없음, 정확한 측정 정밀 검사, 측정 매크로 근접 촬영, 배율 1:1 이상 미세 결함 검사 광각 넓은 시야각 넓은 영역 감시 카메라 인터페이스# 인터페이스 속도 거리 특징 USB 3.0 5 Gbps 5m 간편, 저비용 GigE 1 Gbps 100m 긴 거리, 다중 연결 10GigE 10 Gbps 100m 고속, 고해상도 Camera Link 6.8 Gbps 10m 고속, 산업용 CoaXPress 12.5 Gbps 40m 초고속, 산업용 GigE Vision# # aravis 라이브러리 사용 예 import gi gi.require_version(\u0026#39;Aravis\u0026#39;, \u0026#39;0.8\u0026#39;) from gi.repository import Aravis # 카메라 연결 camera = Aravis.Camera.new(None) # 첫 번째 발견된 카메라 # 설정 camera.set_region(0, 0, 1920, 1080) camera.set_frame_rate(30) camera.set_exposure_time(10000) # μs # 스트림 시작 stream = camera.create_stream() camera.start_acquisition() # 이미지 획득 buffer = stream.pop_buffer() if buffer: data = buffer.get_data() stream.push_buffer(buffer) camera.stop_acquisition() 글로벌 vs 롤링 셔터# 글로벌 셔터# 모든 픽셀 동시 노출:\n시간 → Row 1: [====노출====] Row 2: [====노출====] Row 3: [====노출====] ↑ ↑ 시작 종료 장점: 움직이는 물체 왜곡 없음 용도: 고속 이동 물체, 산업 검사 롤링 셔터# 행별로 순차 노출:\n시간 → Row 1: [====노출====] Row 2: [====노출====] Row 3: [====노출====] 단점: 움직이는 물체 왜곡 (젤리 효과) 장점: 저비용, 고해상도 산업용 카메라 브랜드# 브랜드 특징 Basler 폭넓은 라인업, 합리적 가격 FLIR 열화상, 과학용 Allied Vision 고성능, 다양한 인터페이스 IDS USB3 Vision, 소프트웨어 우수 JAI 다중 센서, 라인 스캔 Teledyne DALSA 라인 스캔 전문 라인 스캔 카메라# 원리# ┌─────────────────────────────┐ │ ← 물체 이동 방향 │ │ │ │ ■■■■■■■■■■ ← 1D 센서 │ │ │ └─────────────────────────────┘ 1D 센서가 이동하는 물체를 스캔 연속 공정에서 무한 길이 이미지 획득 용도# 웹 검사 (필름, 직물, 종이) 컨베이어 벨트 검사 인쇄물 검사 설정 계산# def calculate_line_rate(conveyor_speed_m_s, pixel_size_um, magnification): \u0026#34;\u0026#34;\u0026#34; 라인 스캔 속도 계산 conveyor_speed_m_s: 컨베이어 속도 (m/s) pixel_size_um: 픽셀 크기 (μm) magnification: 광학 배율 \u0026#34;\u0026#34;\u0026#34; object_pixel_size_m = (pixel_size_um / magnification) * 1e-6 line_rate = conveyor_speed_m_s / object_pixel_size_m return line_rate # lines per second # 예: 1 m/s, 5μm 픽셀, 0.5 배율 rate = calculate_line_rate(1.0, 5, 0.5) # = 100,000 lines/s 3D 카메라# 기술 원리 특징 스테레오 두 카메라 시차 저비용, 텍스처 필요 구조광 패턴 투사 정확, 실내용 ToF 빛 비행시간 빠름, 중거리 LiDAR 레이저 스캔 장거리, 정확 Intel RealSense (스테레오 + 구조광)# import pyrealsense2 as rs pipeline = rs.pipeline() config = rs.config() config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30) pipeline.start(config) frames = pipeline.wait_for_frames() depth_frame = frames.get_depth_frame() color_frame = frames.get_color_frame() # Point Cloud 생성 pc = rs.pointcloud() points = pc.calculate(depth_frame) 관련 콘텐츠# 조명 - 조명 설계 엣지 디바이스 - 처리 장치 기하학 - 카메라 모델 "},{"id":13,"href":"/ko/docs/timeline/","title":"타임라인","section":"Docs","content":"타임라인으로 배우는 Vision# 연대순으로 Computer Vision의 발전 과정을 따라가며 학습합니다.\n~2012: Classical Computer Vision# 선형대수 기하학 SIFT \u0026amp; HOG 2012-2015: CNN 시대의 시작# Convolution Backpropagation AlexNet VGG ResNet Classification 2015-2017: Detection \u0026amp; Segmentation# IoU \u0026amp; NMS YOLO Faster R-CNN U-Net Detection Segmentation 2017-2019: Attention의 등장# Attention Transformer 2020-2021: Vision Transformer \u0026amp; CLIP# ViT CLIP Contrastive Learning Self-supervised Learning 2021-2022: Diffusion 시대# Diffusion Process Stable Diffusion Generation 2023: Controllable Generation \u0026amp; SAM# ControlNet SAM 2023-2024: VLM \u0026amp; DiT# VLM DiT Vision-Language 2024-현재: 3D \u0026amp; Video Generation# NeRF 3D Gaussian Splatting 3D Vision "},{"id":14,"href":"/ko/docs/architecture/3d/3dgs/","title":"3D Gaussian Splatting","section":"3D Vision","content":"3D Gaussian Splatting# 개요# 논문: 3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023) 저자: Bernhard Kerbl et al. (Inria, Max Planck Institute) 핵심 기여: 실시간 렌더링이 가능한 명시적 3D 표현 핵심 아이디어# \u0026ldquo;3D 점들을 Gaussian으로 표현하여 빠르게 렌더링\u0026rdquo;\nNeRF와 달리 명시적인 점 기반 표현으로 실시간 렌더링을 달성합니다.\nNeRF vs 3D Gaussian Splatting# 측면 NeRF 3DGS 표현 암시적 (MLP) 명시적 (점 집합) 렌더링 Ray marching Rasterization 속도 느림 (분 단위) 실시간 (100+ FPS) 학습 느림 (시간~일) 빠름 (분~시간) 편집 어려움 용이 구조# 3D Gaussian 표현# 각 점은 다음 속성을 가짐:\n┌─────────────────────────────────────────┐ │ 3D Gaussian │ │ │ │ 위치 (μ): (x, y, z) │ │ 공분산 (Σ): 3×3 행렬 │ │ 색상 (c): SH coefficients │ │ 불투명도 (α): scalar │ │ │ └─────────────────────────────────────────┘수학적 정의# 3D Gaussian 분포:\n$$G(x) = \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$$\n공분산 행렬 분해# 양의 준정부호를 보장하기 위해:\n$$\\Sigma = RSS^TR^T$$\n$R$: 회전 행렬 (quaternion으로 표현) $S$: 스케일 행렬 (대각 행렬) # 학습 가능한 파라미터 position = torch.zeros(N, 3) # 위치 scale = torch.ones(N, 3) # 스케일 rotation = torch.zeros(N, 4) # 쿼터니언 opacity = torch.zeros(N, 1) # 불투명도 sh_coeffs = torch.zeros(N, 16, 3) # 색상 (SH) Spherical Harmonics (SH)# 왜 SH를 사용하는가?# 시점에 따른 색상 변화(반사, 하이라이트)를 표현:\n정면에서 본 색: 밝은 빨강 측면에서 본 색: 어두운 빨강SH 계수# 0차부터 3차까지 총 16개 계수:\n$$c(d) = \\sum_{l=0}^{3} \\sum_{m=-l}^{l} c_{lm} Y_{lm}(d)$$\n렌더링 파이프라인# Splatting 과정# ┌─────────────────────────────────────────────────────────┐ │ Gaussian Splatting Pipeline │ │ │ │ 1. 3D Gaussians ──→ Project to 2D │ │ ↓ │ │ 2. 2D Gaussians ──→ Sort by depth │ │ ↓ │ │ 3. Sorted Gaussians ──→ Alpha blending (front-to-back)│ │ ↓ │ │ 4. Final Image │ │ │ └─────────────────────────────────────────────────────────┘2D 투영# 3D Gaussian을 2D로 투영:\n$$\\Sigma_{2D} = JW\\Sigma W^TJ^T$$\n$W$: 뷰 변환 행렬 $J$: 야코비안 (어파인 근사) Alpha Blending# 깊이 순서대로 색상 합성:\n$$C = \\sum_{i=1}^{N} c_i \\alpha_i \\prod_{j=1}^{i-1}(1-\\alpha_j)$$\n학습# Adaptive Density Control# 학습 중 Gaussian 개수를 동적으로 조절:\n┌─────────────────────────────────────────┐ │ Adaptive Density Control │ │ │ │ 큰 gradient + 작은 scale → Clone │ │ 큰 gradient + 큰 scale → Split │ │ 낮은 opacity → Remove │ │ │ └─────────────────────────────────────────┘손실 함수# $$L = (1-\\lambda)L_1 + \\lambda L_{D-SSIM}$$\n$L_1$: 픽셀 단위 손실 $L_{D-SSIM}$: 구조적 유사도 손실 학습 과정# for iteration in range(30000): # 1. 렌더링 rendered = render(gaussians, camera) # 2. 손실 계산 loss = l1_loss(rendered, gt_image) + ssim_loss(rendered, gt_image) # 3. 역전파 loss.backward() optimizer.step() # 4. Adaptive control (매 100 iteration) if iteration % 100 == 0: densify_and_prune(gaussians) 구현 예시# 데이터 구조# class GaussianModel: def __init__(self): # 위치 self._xyz = torch.empty(0) # 색상 (SH coefficients) self._features_dc = torch.empty(0) self._features_rest = torch.empty(0) # 스케일 self._scaling = torch.empty(0) # 회전 (quaternion) self._rotation = torch.empty(0) # 불투명도 self._opacity = torch.empty(0) @property def get_covariance(self): \u0026#34;\u0026#34;\u0026#34;공분산 행렬 계산\u0026#34;\u0026#34;\u0026#34; S = self.scaling_activation(self._scaling) R = self.rotation_activation(self._rotation) L = build_scaling_rotation(S, R) return L @ L.transpose(1, 2)초기화 (from Point Cloud)# def create_from_pcd(self, pcd): \u0026#34;\u0026#34;\u0026#34;SfM 점군으로부터 초기화\u0026#34;\u0026#34;\u0026#34; points = torch.tensor(pcd.points) colors = torch.tensor(pcd.colors) self._xyz = nn.Parameter(points) # 색상을 SH로 변환 fused_color = RGB2SH(colors) self._features_dc = nn.Parameter(fused_color) # 초기 스케일: 인접 점까지 거리 dist = torch.cdist(points, points) dist[dist == 0] = float(\u0026#39;inf\u0026#39;) init_scale = torch.log(dist.min(dim=1)[0]) self._scaling = nn.Parameter(init_scale.repeat(1, 3)) # 초기 회전: identity self._rotation = nn.Parameter(torch.zeros(len(points), 4)) self._rotation[:, 0] = 1 # w=1 for identity quaternion # 초기 불투명도 self._opacity = nn.Parameter(inverse_sigmoid(0.1 * torch.ones(len(points), 1)))CUDA 렌더러# def render(viewpoint_camera, gaussians, bg_color): \u0026#34;\u0026#34;\u0026#34;CUDA 기반 고속 렌더링\u0026#34;\u0026#34;\u0026#34; # 2D 투영 screenspace_points = project_to_screen(gaussians, viewpoint_camera) # 타일 기반 래스터라이제이션 rendered_image, radii = rasterize_gaussians( means3D=gaussians.get_xyz, means2D=screenspace_points, shs=gaussians.get_features, colors_precomp=None, opacities=gaussians.get_opacity, scales=gaussians.get_scaling, rotations=gaussians.get_rotation, cov3D_precomp=None, viewmatrix=viewpoint_camera.world_view_transform, projmatrix=viewpoint_camera.full_proj_transform, sh_degree=gaussians.active_sh_degree, campos=viewpoint_camera.camera_center, prefiltered=False, bg=bg_color ) return rendered_image 성능# 품질 비교 (PSNR)# 데이터셋 NeRF Instant-NGP 3DGS Synthetic 31.0 33.2 33.3 Real-World 26.8 29.5 29.4 속도# 방법 학습 시간 렌더링 FPS NeRF 수 일 0.05 Instant-NGP 5분 10 3DGS 30분 100+ 장점과 단점# 장점# 실시간 렌더링 (100+ FPS) 빠른 학습 (30분) 명시적 표현으로 편집 용이 고품질 결과 단점# 메모리 사용량 (수백만 점) 초기 점군 품질 의존 복잡한 기하학에서 아티팩트 응용# 1. VR/AR# 실시간 렌더링으로 몰입형 경험\n2. 게임# 사실적인 환경 재구성\n3. 디지털 트윈# 실제 공간의 3D 복제\n4. 영화/VFX# 빠른 프리비주얼라이제이션\n후속 연구# 모델 개선점 4D-GS 동적 장면 GSGEN 텍스트→3D 생성 GaussianAvatar 사람 모델링 Compact-3DGS 압축 Mip-Splatting 안티앨리어싱 관련 콘텐츠# NeRF - 암시적 표현 Transformer - 최신 3D 모델에서 활용 "},{"id":15,"href":"/ko/docs/architecture/cnn/","title":"CNN","section":"아키텍처","content":"CNN (Convolutional Neural Network)# 개요# CNN은 이미지 처리에 특화된 신경망 구조로, 2012년 AlexNet의 등장 이후 컴퓨터 비전의 핵심이 되었습니다.\n핵심 구성 요소# 1. Convolution Layer# 필터(커널)를 이미지에 슬라이딩하며 특징을 추출합니다.\n$$\\text{Output}(i,j) = \\sum_{m}\\sum_{n} \\text{Input}(i+m, j+n) \\cdot \\text{Kernel}(m,n)$$\n특징:\n파라미터 공유로 효율적 지역적 패턴 학습 Translation equivariance 2. Pooling Layer# 공간 해상도를 줄이고 위치 불변성을 제공합니다.\nMax Pooling: 영역 내 최댓값 선택 Average Pooling: 영역 내 평균값 계산 3. Activation Function# 비선형성을 추가합니다.\n$$\\text{ReLU}(x) = \\max(0, x)$$\n4. Fully Connected Layer# 최종 분류를 위한 전결합 층입니다.\n전형적인 CNN 구조# Input → [Conv → ReLU → Pool] × N → Flatten → FC → Output 주요 발전 과정# 연도 모델 핵심 기여 2012 AlexNet GPU 학습, ReLU, Dropout 2014 VGG 깊은 네트워크, 3x3 필터 2015 ResNet Skip Connection, 152층 구현 예시# import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self, num_classes=10): super().__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), ) self.classifier = nn.Sequential( nn.Flatten(), nn.Linear(128 * 8 * 8, 256), nn.ReLU(), nn.Linear(256, num_classes), ) def forward(self, x): x = self.features(x) x = self.classifier(x) return x 관련 콘텐츠# Convolution - Convolution 연산의 수학적 이해 Backpropagation - CNN 학습 원리 Classification - CNN의 대표적 응용 "},{"id":16,"href":"/ko/docs/architecture/transformer/dit/","title":"DiT","section":"Transformer","content":"DiT (Diffusion Transformer)# 개요# 논문: Scalable Diffusion Models with Transformers (2023) 저자: William Peebles, Saining Xie (Meta AI, NYU) 핵심 기여: Diffusion 모델의 backbone을 U-Net에서 Transformer로 교체 핵심 아이디어# \u0026ldquo;U-Net 대신 Transformer를 쓰면 scaling이 더 잘 된다\u0026rdquo;\n기존 Diffusion 모델(Stable Diffusion 등)은 U-Net을 사용하지만, DiT는 Transformer를 사용하여 더 나은 scaling 특성을 보여줍니다.\n배경# 기존 Diffusion 모델의 backbone# Stable Diffusion: Latent → U-Net → Denoised Latent U-Net은 CNN 기반 Skip connection으로 세부 정보 보존 하지만 모델 크기 증가 시 효율 저하 DiT의 접근# Latent → Patchify → Transformer → Unpatchify → Denoised Latent ViT 스타일로 latent를 패치화 Transformer로 처리 모델 크기에 따라 일관된 성능 향상 구조# 전체 아키텍처# Input: Noisy Latent (z_t) + Timestep (t) + Class (c) ↓ Patchify (latent → patches) ↓ + Position Embedding ↓ ┌─────────────────────────────────────┐ │ DiT Blocks × N │ │ ┌─────────────────────────────┐ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Multi-Head Self-Attn │ │ │ │ ↓ │ │ │ │ Layer Norm │ │ │ │ ↓ │ │ │ │ Pointwise FFN │ │ │ │ ↓ │ │ │ │ + AdaLN (t, c embed) │ │ │ └─────────────────────────────┘ │ └─────────────────────────────────────┘ ↓ Final Layer Norm ↓ Linear (predict noise \u0026amp; variance) ↓ Unpatchify ↓ Output: Predicted Noise (ε) + Variance (Σ)AdaLN (Adaptive Layer Norm)# 조건 정보(timestep, class)를 Layer Norm에 주입:\n$$\\text{AdaLN}(h, y) = y_s \\odot \\text{LayerNorm}(h) + y_b$$\n여기서 $y_s$, $y_b$는 timestep과 class embedding에서 예측된 scale, shift 값\n조건 주입 방식# 세 가지 변형# 방식 설명 성능 In-context 조건을 추가 토큰으로 중간 Cross-attention 조건에 cross-attention 중간 AdaLN-Zero Adaptive LN + zero 초기화 최고 AdaLN-Zero# 각 블록의 출력을 0으로 초기화:\n학습 초기에 identity mapping 안정적인 학습 모델 변형# 모델 Layers Hidden Heads Params FID-50K DiT-S/2 12 384 6 33M 68.4 DiT-B/2 12 768 12 130M 43.5 DiT-L/2 24 1024 16 458M 23.3 DiT-XL/2 28 1152 16 675M 2.27 /2는 patch size 2 의미 (latent space에서) 모델 크기 증가에 따른 일관된 성능 향상 Scaling 특성# Gflops vs FID# DiT는 compute가 증가할수록 일관되게 FID가 개선:\nGflops: 10 → 100 → 1000 FID: 68 → 43 → 2.27이는 LLM에서 관찰되는 scaling law와 유사합니다.\n왜 Transformer가 잘 scaling 되는가?# Attention의 유연성: 전역적 패턴 학습 표준화된 구조: 최적화 기법 적용 용이 병렬화: 효율적인 학습 구현 예시# import torch import torch.nn as nn class DiTBlock(nn.Module): def __init__(self, hidden_size, num_heads, mlp_ratio=4.0): super().__init__() self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False) self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True) self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False) self.mlp = nn.Sequential( nn.Linear(hidden_size, int(hidden_size * mlp_ratio)), nn.GELU(), nn.Linear(int(hidden_size * mlp_ratio), hidden_size), ) # AdaLN parameters self.adaLN_modulation = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size) # scale, shift for both norms + gate ) def forward(self, x, c): # c: condition embedding (timestep + class) shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \\ self.adaLN_modulation(c).chunk(6, dim=-1) # Self-attention with AdaLN h = self.norm1(x) h = h * (1 + scale_msa.unsqueeze(1)) + shift_msa.unsqueeze(1) h, _ = self.attn(h, h, h) x = x + gate_msa.unsqueeze(1) * h # FFN with AdaLN h = self.norm2(x) h = h * (1 + scale_mlp.unsqueeze(1)) + shift_mlp.unsqueeze(1) h = self.mlp(h) x = x + gate_mlp.unsqueeze(1) * h return x class DiT(nn.Module): def __init__(self, input_size=32, patch_size=2, hidden_size=1152, depth=28, num_heads=16, num_classes=1000): super().__init__() self.num_patches = (input_size // patch_size) ** 2 # Patch embedding self.x_embedder = nn.Conv2d(4, hidden_size, patch_size, patch_size) self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, hidden_size)) # Condition embedding self.t_embedder = TimestepEmbedder(hidden_size) self.y_embedder = nn.Embedding(num_classes, hidden_size) # DiT blocks self.blocks = nn.ModuleList([ DiTBlock(hidden_size, num_heads) for _ in range(depth) ]) # Output self.final_layer = FinalLayer(hidden_size, patch_size, 4) def forward(self, x, t, y): # Patchify x = self.x_embedder(x).flatten(2).transpose(1, 2) x = x + self.pos_embed # Condition c = self.t_embedder(t) + self.y_embedder(y) # DiT blocks for block in self.blocks: x = block(x, c) # Unpatchify x = self.final_layer(x, c) return x DiT의 영향# 후속 모델들# Sora (OpenAI): DiT 기반 비디오 생성 Stable Diffusion 3: DiT 아키텍처 채택 PixArt-α: 효율적인 DiT 학습 왜 중요한가?# Diffusion 모델의 새로운 방향 제시 Scaling law 확인 Transformer의 범용성 증명 U-Net vs DiT# 측면 U-Net DiT 구조 CNN + Skip Transformer Scaling 제한적 일관적 향상 계산량 상대적 효율 더 많은 compute 구현 복잡도 복잡 단순 관련 콘텐츠# Transformer - 기반 아키텍처 ViT - 이미지 패치화 아이디어 Stable Diffusion - U-Net 기반 Diffusion Diffusion Process - Diffusion 수식 "},{"id":17,"href":"/ko/docs/architecture/generative/gan/","title":"GAN","section":"Generative","content":"GAN (Generative Adversarial Network)# 개요# 논문: Generative Adversarial Networks (2014) 저자: Ian Goodfellow et al. 핵심 기여: 적대적 학습으로 선명한 이미지 생성 핵심 아이디어# \u0026ldquo;Generator와 Discriminator가 경쟁하며 학습\u0026rdquo;\n위조지폐범(Generator)과 경찰(Discriminator)의 게임:\nGenerator: 진짜처럼 보이는 가짜 생성 Discriminator: 진짜와 가짜 구분 구조# 전체 아키텍처# Random Noise z ↓ ┌─────────────────┐ │ Generator │ │ z → fake image │ └─────────────────┘ ↓ ┌─────────────────┐ ┌─────────────────┐ │ Fake Image │ │ Real Image │ └────────┬────────┘ └────────┬────────┘ │ │ └──────────┬─────────────┘ ↓ ┌─────────────────────┐ │ Discriminator │ │ image → real/fake │ └─────────────────────┘ ↓ 0 (fake) or 1 (real) 목적 함수# Minimax Game# $$\\min_G \\max_D V(D, G) = \\mathbb{E}{x \\sim p{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n각각의 목표# Discriminator: $V$를 최대화\n$D(x) \\to 1$ (진짜를 진짜로) $D(G(z)) \\to 0$ (가짜를 가짜로) Generator: $V$를 최소화\n$D(G(z)) \\to 1$ (가짜를 진짜로 속이기) 실제 학습# Generator의 gradient 문제 해결을 위해:\n$$\\max_G \\mathbb{E}_{z \\sim p_z}[\\log D(G(z))]$$\n학습 알고리즘# for epoch in range(epochs): for real_images in dataloader: # 1. Discriminator 학습 z = torch.randn(batch_size, latent_dim) fake_images = G(z) d_real = D(real_images) d_fake = D(fake_images.detach()) d_loss = -torch.mean(torch.log(d_real) + torch.log(1 - d_fake)) d_optimizer.zero_grad() d_loss.backward() d_optimizer.step() # 2. Generator 학습 z = torch.randn(batch_size, latent_dim) fake_images = G(z) d_fake = D(fake_images) g_loss = -torch.mean(torch.log(d_fake)) g_optimizer.zero_grad() g_loss.backward() g_optimizer.step() 구현 예시# import torch import torch.nn as nn class Generator(nn.Module): def __init__(self, latent_dim=100, img_channels=3, features=64): super().__init__() self.gen = nn.Sequential( # z: latent_dim → 4×4×(features*16) nn.ConvTranspose2d(latent_dim, features*16, 4, 1, 0, bias=False), nn.BatchNorm2d(features*16), nn.ReLU(True), # 4×4 → 8×8 nn.ConvTranspose2d(features*16, features*8, 4, 2, 1, bias=False), nn.BatchNorm2d(features*8), nn.ReLU(True), # 8×8 → 16×16 nn.ConvTranspose2d(features*8, features*4, 4, 2, 1, bias=False), nn.BatchNorm2d(features*4), nn.ReLU(True), # 16×16 → 32×32 nn.ConvTranspose2d(features*4, features*2, 4, 2, 1, bias=False), nn.BatchNorm2d(features*2), nn.ReLU(True), # 32×32 → 64×64 nn.ConvTranspose2d(features*2, img_channels, 4, 2, 1, bias=False), nn.Tanh() ) def forward(self, z): return self.gen(z.view(-1, z.size(1), 1, 1)) class Discriminator(nn.Module): def __init__(self, img_channels=3, features=64): super().__init__() self.disc = nn.Sequential( # 64×64 → 32×32 nn.Conv2d(img_channels, features, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # 32×32 → 16×16 nn.Conv2d(features, features*2, 4, 2, 1, bias=False), nn.BatchNorm2d(features*2), nn.LeakyReLU(0.2, inplace=True), # 16×16 → 8×8 nn.Conv2d(features*2, features*4, 4, 2, 1, bias=False), nn.BatchNorm2d(features*4), nn.LeakyReLU(0.2, inplace=True), # 8×8 → 4×4 nn.Conv2d(features*4, features*8, 4, 2, 1, bias=False), nn.BatchNorm2d(features*8), nn.LeakyReLU(0.2, inplace=True), # 4×4 → 1×1 nn.Conv2d(features*8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, img): return self.disc(img).view(-1) GAN의 문제점# 1. Mode Collapse# Generator가 다양성 없이 몇 가지 샘플만 생성\n2. Training Instability# D와 G의 균형 맞추기 어려움\n3. Vanishing Gradient# D가 너무 잘 학습되면 G의 gradient가 사라짐\nGAN 변형# 모델 핵심 개선 DCGAN Conv 구조, 안정적 학습 WGAN Wasserstein distance, 안정성 StyleGAN Style-based, 고품질 얼굴 BigGAN Large-scale, ImageNet 생성 Pix2Pix Image-to-image translation CycleGAN Unpaired translation StyleGAN# 고품질 얼굴 생성의 대표 모델:\nMapping Network: z → w (learned latent space) ↓ Synthesis Network: w → image (with style injection)핵심 기술:\nStyle injection: 각 해상도에서 스타일 적용 Progressive growing: 저해상도 → 고해상도 Noise injection: 세부 디테일 현재 상태# GAN은 2014-2021년 이미지 생성의 주류였지만, 현재는 Diffusion 모델에 대부분 대체되었습니다.\nGAN의 장점이 남아있는 분야:\n실시간 생성 (빠른 샘플링) Image-to-image translation 비디오 생성 (일부) 관련 콘텐츠# VAE - 또 다른 생성 모델 Stable Diffusion - GAN을 대체한 모델 Generation 태스크 - FID 등 평가 지표 "},{"id":18,"href":"/ko/docs/architecture/segmentation/mask-rcnn/","title":"Mask R-CNN","section":"Segmentation","content":"Mask R-CNN# 개요# 논문: Mask R-CNN (2017) 저자: Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick (FAIR) 핵심 기여: Faster R-CNN에 mask branch를 추가하여 instance segmentation 핵심 아이디어# \u0026ldquo;Faster R-CNN + Mask Branch = Instance Segmentation\u0026rdquo;\nDetection과 segmentation을 동시에 수행하는 multi-task 모델입니다.\n구조# 전체 아키텍처# Input Image ↓ Backbone (ResNet + FPN) ↓ Region Proposal Network (RPN) ↓ RoI Align (not RoI Pooling!) ↓ ┌─────────────────────────────────────────┐ │ Three Branches │ ├─────────────┬─────────────┬─────────────┤ │ Classification │ Box Regression │ Mask Prediction │ │ (class) │ (x,y,w,h) │ (28×28 binary) │ └─────────────┴─────────────┴─────────────┘Faster R-CNN과의 차이# 구성 요소 Faster R-CNN Mask R-CNN RoI 처리 RoI Pooling RoI Align 출력 class, box class, box, mask Backbone ResNet ResNet + FPN 핵심 컴포넌트# 1. RoI Align# RoI Pooling의 양자화 문제를 해결:\nRoI Pooling 문제:\nRoI: 10.6 → 10 (반올림) 실제 위치와 오차 발생 → segmentation 품질 저하RoI Align 해결:\n양자화 없이 bilinear interpolation 정확한 위치에서 특징 추출2. Mask Branch# 각 RoI에 대해 28×28 크기의 마스크 예측:\n# Mask Head 구조 FCN (4 Conv layers) → Deconv → 28×28×num_classes특징:\n클래스별로 독립적인 마스크 예측 Binary mask (sigmoid) 최종 마스크는 box 크기로 resize 3. FPN (Feature Pyramid Network)# 다양한 스케일의 객체 검출을 위한 feature pyramid:\nP5 (작은 해상도, 큰 객체) ↑ P4 ↑ P3 ↑ P2 (큰 해상도, 작은 객체) 학습# Multi-task Loss# $$L = L_{cls} + L_{box} + L_{mask}$$\n$L_{cls}$: Classification loss (cross-entropy) $L_{box}$: Bounding box regression (smooth L1) $L_{mask}$: Binary cross-entropy (픽셀별) Mask Loss# Ground truth 클래스에 해당하는 마스크에서만 loss 계산:\n$$L_{mask} = -\\frac{1}{m^2} \\sum_{i,j} [y_{ij} \\log \\hat{y}{ij} + (1-y{ij}) \\log(1-\\hat{y}_{ij})]$$\n구현 예시# import torch import torchvision from torchvision.models.detection import maskrcnn_resnet50_fpn # 사전 학습된 모델 로드 model = maskrcnn_resnet50_fpn(pretrained=True) model.eval() # 추론 with torch.no_grad(): predictions = model([image]) # 결과 형태 pred = predictions[0] print(pred[\u0026#39;boxes\u0026#39;].shape) # (N, 4) - bounding boxes print(pred[\u0026#39;labels\u0026#39;].shape) # (N,) - class labels print(pred[\u0026#39;scores\u0026#39;].shape) # (N,) - confidence scores print(pred[\u0026#39;masks\u0026#39;].shape) # (N, 1, H, W) - instance masks커스텀 학습# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor from torchvision.models.detection.faster_rcnn import FastRCNNPredictor model = maskrcnn_resnet50_fpn(pretrained=True) num_classes = 10 # Box predictor 교체 in_features = model.roi_heads.box_predictor.cls_score.in_features model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # Mask predictor 교체 in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 model.roi_heads.mask_predictor = MaskRCNNPredictor( in_features_mask, hidden_layer, num_classes ) 성능# COCO Dataset# Backbone box AP mask AP ResNet-50-FPN 38.2 34.7 ResNet-101-FPN 40.0 36.1 ResNeXt-101-FPN 41.3 37.1 Mask R-CNN의 확장# Keypoint Detection# Mask branch 대신 keypoint branch 추가:\nfrom torchvision.models.detection import keypointrcnn_resnet50_fpn model = keypointrcnn_resnet50_fpn(pretrained=True) # 17개 keypoint (COCO person keypoints)Panoptic Segmentation# Instance + semantic segmentation 통합\n한계점# Two-stage라 속도 제한 (~5 FPS) 겹치는 객체 처리 어려움 작은 객체 segmentation 품질 저하 관련 콘텐츠# Faster R-CNN - 기반 모델 U-Net - Semantic segmentation SAM - Promptable segmentation ResNet - Backbone Segmentation 태스크 - 평가 지표 "},{"id":19,"href":"/ko/docs/engineering/deployment/tensorrt/","title":"TensorRT","section":"배포","content":"TensorRT# 개요# TensorRT는 NVIDIA GPU에서 딥러닝 추론을 최적화하는 라이브러리입니다.\nONNX 모델 → TensorRT 최적화 → 최대 10x 빠른 추론 주요 최적화# 기법 설명 Layer Fusion 여러 레이어를 하나로 병합 Precision Calibration FP32 → FP16/INT8 변환 Kernel Auto-tuning 최적의 CUDA 커널 선택 Dynamic Tensor Memory 메모리 재사용 Multi-stream Execution 병렬 처리 ONNX → TensorRT 변환# trtexec (CLI)# # FP16 엔진 생성 trtexec --onnx=model.onnx \\ --saveEngine=model.engine \\ --fp16 # INT8 엔진 생성 (캘리브레이션 필요) trtexec --onnx=model.onnx \\ --saveEngine=model.engine \\ --int8 \\ --calib=calibration_cache.bin # 동적 배치 trtexec --onnx=model.onnx \\ --saveEngine=model.engine \\ --minShapes=input:1x3x224x224 \\ --optShapes=input:8x3x224x224 \\ --maxShapes=input:16x3x224x224Python API# import tensorrt as trt logger = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(logger) network = builder.create_network(1 \u0026lt;\u0026lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, logger) # ONNX 파싱 with open(\u0026#34;model.onnx\u0026#34;, \u0026#34;rb\u0026#34;) as f: parser.parse(f.read()) # 빌더 설정 config = builder.create_builder_config() config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 \u0026lt;\u0026lt; 30) # 1GB config.set_flag(trt.BuilderFlag.FP16) # FP16 활성화 # 엔진 빌드 serialized_engine = builder.build_serialized_network(network, config) # 저장 with open(\u0026#34;model.engine\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(serialized_engine) TensorRT 추론# import tensorrt as trt import pycuda.driver as cuda import pycuda.autoinit import numpy as np # 엔진 로드 with open(\u0026#34;model.engine\u0026#34;, \u0026#34;rb\u0026#34;) as f: runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) engine = runtime.deserialize_cuda_engine(f.read()) context = engine.create_execution_context() # 메모리 할당 input_shape = (1, 3, 224, 224) output_shape = (1, 1000) d_input = cuda.mem_alloc(np.prod(input_shape) * 4) # float32 d_output = cuda.mem_alloc(np.prod(output_shape) * 4) # 추론 def infer(input_data): cuda.memcpy_htod(d_input, input_data) context.execute_v2([int(d_input), int(d_output)]) output = np.empty(output_shape, dtype=np.float32) cuda.memcpy_dtoh(output, d_output) return output # 사용 input_data = np.random.randn(*input_shape).astype(np.float32) result = infer(input_data) INT8 양자화# 캘리브레이션# import tensorrt as trt import numpy as np class Calibrator(trt.IInt8EntropyCalibrator2): def __init__(self, data_loader, cache_file): super().__init__() self.data_loader = data_loader self.cache_file = cache_file self.batch_idx = 0 # 디바이스 메모리 할당 self.d_input = cuda.mem_alloc(self.data_loader.batch_size * 3 * 224 * 224 * 4) def get_batch_size(self): return self.data_loader.batch_size def get_batch(self, names): try: batch = next(self.data_loader) cuda.memcpy_htod(self.d_input, batch) return [int(self.d_input)] except StopIteration: return None def read_calibration_cache(self): if os.path.exists(self.cache_file): with open(self.cache_file, \u0026#34;rb\u0026#34;) as f: return f.read() def write_calibration_cache(self, cache): with open(self.cache_file, \u0026#34;wb\u0026#34;) as f: f.write(cache) # 빌더 설정 config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = Calibrator(calibration_loader, \u0026#34;calibration.cache\u0026#34;) 동적 Shape# # 프로파일 설정 profile = builder.create_optimization_profile() profile.set_shape( \u0026#34;input\u0026#34;, min=(1, 3, 224, 224), # 최소 opt=(8, 3, 224, 224), # 최적 max=(32, 3, 224, 224) # 최대 ) config.add_optimization_profile(profile) # 추론 시 shape 설정 context.set_input_shape(\u0026#34;input\u0026#34;, (batch_size, 3, 224, 224)) torch2trt (간편한 변환)# from torch2trt import torch2trt import torch model = MyModel().cuda().eval() x = torch.randn(1, 3, 224, 224).cuda() # TensorRT 변환 model_trt = torch2trt( model, [x], fp16_mode=True, max_batch_size=16, ) # 추론 y_trt = model_trt(x) # 저장/로드 torch.save(model_trt.state_dict(), \u0026#34;model_trt.pth\u0026#34;) Triton과 통합# 모델 저장소 구조# model_repository/ └── my_model/ ├── config.pbtxt └── 1/ └── model.plan # TensorRT 엔진config.pbtxt# name: \u0026#34;my_model\u0026#34; platform: \u0026#34;tensorrt_plan\u0026#34; max_batch_size: 16 input [ { name: \u0026#34;input\u0026#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: \u0026#34;output\u0026#34; data_type: TYPE_FP32 dims: [1000] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [0] } ] 성능 비교# 벤치마크 예시 (ResNet-50, RTX 3090)# Precision Latency Throughput PyTorch FP32 5.2 ms 192 FPS TensorRT FP32 2.1 ms 476 FPS TensorRT FP16 1.0 ms 1000 FPS TensorRT INT8 0.6 ms 1667 FPS 문제 해결# 지원되지 않는 연산# # 플러그인 사용 # 또는 ONNX 레벨에서 수정 # 예: 지원되지 않는 레이어를 여러 지원 레이어로 분해정확도 저하 (INT8)# # 1. 캘리브레이션 데이터 확인 (대표성 있는 데이터 사용) # 2. 민감한 레이어는 FP16 유지 config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)메모리 부족# # 워크스페이스 크기 조정 config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 \u0026lt;\u0026lt; 28) # 256MB 관련 콘텐츠# ONNX - 모델 변환 모델 서빙 - Triton 서버 최적화 - 양자화 이론 "},{"id":20,"href":"/ko/docs/topdown/","title":"Top-Down","section":"Docs","content":"Top-Down 학습# 문제를 정하고, 필요한 지식을 찾아갑니다.\nImage Classification# Cross-entropy Loss CNN 기초 ResNet Classification Object Detection# IoU \u0026amp; NMS Anchor Box Faster R-CNN YOLO Detection Segmentation# Transposed Convolution U-Net Mask R-CNN Segmentation Image Generation# 확률분포 VAE GAN Diffusion Stable Diffusion ControlNet DiT Generation Vision-Language (VLM)# Contrastive Learning CLIP VLM Vision-Language 3D Vision# 카메라 모델 NeRF 3D Vision 모델 배포# Quantization Deployment "},{"id":21,"href":"/ko/docs/architecture/cnn/vgg/","title":"VGG","section":"CNN","content":"VGG# 개요# 논문: Very Deep Convolutional Networks for Large-Scale Image Recognition (2014) 저자: Karen Simonyan, Andrew Zisserman (Oxford) 핵심 기여: 작은 필터(3×3)를 깊게 쌓는 것이 효과적임을 증명 핵심 아이디어# \u0026ldquo;3×3 필터를 여러 번 쌓으면 큰 필터와 같은 receptive field를 가지면서 파라미터는 더 적다\u0026rdquo;\n예시: 3×3 두 번 = 5×5 한 번의 receptive field\n방식 Receptive Field 파라미터 5×5 한 번 5×5 25C² 3×3 두 번 5×5 18C² 구조# VGG-16 아키텍처# Input (224×224×3) ↓ Block 1: [Conv3-64] × 2 → MaxPool ↓ Block 2: [Conv3-128] × 2 → MaxPool ↓ Block 3: [Conv3-256] × 3 → MaxPool ↓ Block 4: [Conv3-512] × 3 → MaxPool ↓ Block 5: [Conv3-512] × 3 → MaxPool ↓ FC: 4096 → 4096 → 1000VGG 변형# 모델 레이어 수 파라미터 VGG-11 11 133M VGG-13 13 133M VGG-16 16 138M VGG-19 19 144M 설계 원칙# 1. 작은 필터 (3×3)# 모든 Conv 레이어에서 3×3 필터 사용:\n더 많은 비선형성 (ReLU 더 많이 적용) 파라미터 효율성 더 깊은 네트워크 가능 2. 일관된 구조# 모든 Conv: 3×3, stride 1, padding 1 모든 Pool: 2×2, stride 2 채널 수: 64 → 128 → 256 → 512 → 512 3. 깊이의 중요성# 레이어를 깊게 쌓을수록 성능 향상:\nVGG-11: 10.4% top-5 error VGG-16: 7.4% top-5 error VGG-19: 7.3% top-5 error 구현 예시# import torch.nn as nn def make_layers(cfg): layers = [] in_channels = 3 for v in cfg: if v == \u0026#39;M\u0026#39;: layers.append(nn.MaxPool2d(kernel_size=2, stride=2)) else: layers.append(nn.Conv2d(in_channels, v, kernel_size=3, padding=1)) layers.append(nn.ReLU(inplace=True)) in_channels = v return nn.Sequential(*layers) # VGG-16 config cfg_vgg16 = [64, 64, \u0026#39;M\u0026#39;, 128, 128, \u0026#39;M\u0026#39;, 256, 256, 256, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;, 512, 512, 512, \u0026#39;M\u0026#39;] class VGG16(nn.Module): def __init__(self, num_classes=1000): super().__init__() self.features = make_layers(cfg_vgg16) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x 특징 추출기로서의 VGG# VGG의 중간 레이어는 범용적인 특징 추출기로 활용됩니다:\n# Perceptual Loss에서 VGG 특징 사용 vgg = models.vgg16(pretrained=True).features[:16] # relu3_3까지 def perceptual_loss(pred, target): pred_features = vgg(pred) target_features = vgg(target) return F.mse_loss(pred_features, target_features)활용 분야:\nStyle Transfer Perceptual Loss Feature Matching 한계점# 파라미터 과다: 138M 파라미터 (대부분 FC 레이어) 메모리 사용량: 학습 시 많은 GPU 메모리 필요 학습 어려움: 깊은 네트워크에서 gradient vanishing 이러한 한계는 ResNet의 Skip Connection으로 해결되었습니다.\n관련 콘텐츠# AlexNet - 이전 모델 ResNet - Skip Connection으로 깊이 문제 해결 CNN 기초 - Convolution의 원리 "},{"id":22,"href":"/ko/docs/architecture/multimodal/vlm/","title":"VLM","section":"Multimodal","content":"VLM (Vision-Language Models)# 개요# Vision-Language Model은 이미지와 텍스트를 함께 이해하고 생성하는 모델입니다.\n발전 과정# CLIP (2021) → BLIP (2022) → BLIP-2 (2023) → LLaVA (2023) → GPT-4V (2023) 주요 모델들# 1. BLIP (Bootstrapping Language-Image Pre-training)# 개요# 논문: BLIP: Bootstrapping Language-Image Pre-training (2022) 저자: Salesforce Research 핵심: 캡션 생성과 필터링을 통한 데이터 품질 향상 구조# Image ─────→ Image Encoder ─────┐ ├──→ Multi-task Learning Text ─────→ Text Encoder ─────┘ Tasks: ITC (Image-Text Contrastive) ITM (Image-Text Matching) LM (Language Modeling) 2. BLIP-2# 개요# 논문: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and LLMs (2023) 저자: Salesforce Research 핵심: 사전학습된 Vision/Language 모델을 효율적으로 연결 구조# ┌─────────────────────────────────────────────────────────┐ │ BLIP-2 │ │ │ │ Image ──→ [Frozen Image Encoder] ──→ Image Features │ │ ↓ │ │ ┌─────────────────┐ │ │ │ Q-Former │ │ │ │ (Learnable) │ │ │ │ 32 Query Tokens │ │ │ └────────┬────────┘ │ │ ↓ │ │ Text ───────────────────────→ [Frozen LLM] ──→ Output │ │ │ └─────────────────────────────────────────────────────────┘Q-Former# Vision과 Language 모델을 연결하는 경량 모듈:\n# Q-Former: 32개의 학습 가능한 쿼리 query_tokens = nn.Parameter(torch.zeros(1, 32, hidden_dim)) # Image features에서 정보 추출 image_features = frozen_vit(image) query_output = q_former(query_tokens, image_features) # LLM에 입력 llm_input = [query_output, text_tokens] output = frozen_llm(llm_input) 3. LLaVA (Large Language and Vision Assistant)# 개요# 논문: Visual Instruction Tuning (2023) 저자: Microsoft Research 핵심: Visual instruction tuning으로 GPT-4 수준의 대화 능력 구조# ┌─────────────────────────────────────────────────────────┐ │ LLaVA │ │ │ │ Image ──→ CLIP ViT ──→ Linear Projection ─┐ │ │ ↓ │ │ \u0026#34;Describe\u0026#34; ──→ Tokenizer ──→ LLM (Vicuna) ──→ Caption │ │ │ └─────────────────────────────────────────────────────────┘학습 단계# 단계 데이터 목적 Stage 1 CC3M (필터링) Vision-Language 정렬 Stage 2 LLaVA-Instruct-150K Instruction following 구현 예시# from transformers import LlavaForConditionalGeneration, AutoProcessor model = LlavaForConditionalGeneration.from_pretrained(\u0026#34;llava-hf/llava-1.5-7b-hf\u0026#34;) processor = AutoProcessor.from_pretrained(\u0026#34;llava-hf/llava-1.5-7b-hf\u0026#34;) # 이미지 + 질문 inputs = processor( text=\u0026#34;\u0026lt;image\u0026gt;\\nWhat is shown in this image?\u0026#34;, images=image, return_tensors=\u0026#34;pt\u0026#34; ) output = model.generate(**inputs, max_new_tokens=200) print(processor.decode(output[0], skip_special_tokens=True)) 4. GPT-4V (GPT-4 with Vision)# 특징# OpenAI의 멀티모달 모델 이미지 이해 + 텍스트 생성 복잡한 추론 능력 사용 예시# from openai import OpenAI client = OpenAI() response = client.chat.completions.create( model=\u0026#34;gpt-4-vision-preview\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;이 이미지에서 무엇이 보이나요?\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: {\u0026#34;url\u0026#34;: image_url}} ] } ] ) VLM 아키텍처 비교# 모델 Vision Encoder Language Model 연결 방식 CLIP ViT Transformer Contrastive BLIP ViT BERT/GPT Multi-task BLIP-2 Frozen ViT Frozen LLM Q-Former LLaVA CLIP ViT Vicuna/LLaMA Linear GPT-4V 비공개 GPT-4 비공개 학습 방법# 1. Contrastive Learning# 이미지-텍스트 쌍의 유사도 학습:\n$$L_{ITC} = -\\log \\frac{\\exp(sim(I, T^+)/\\tau)}{\\sum_j \\exp(sim(I, T_j)/\\tau)}$$\n2. Image-Text Matching# 이미지와 텍스트가 매칭되는지 분류:\n$$L_{ITM} = \\text{CrossEntropy}(p_{match}, y)$$\n3. Language Modeling# 이미지 조건부 텍스트 생성:\n$$L_{LM} = -\\sum_t \\log P(w_t | w_{\u0026lt;t}, I)$$\nVLM의 능력# 1. Visual Question Answering (VQA)# Image: [고양이 사진] Q: \u0026#34;이 동물의 색깔은?\u0026#34; A: \u0026#34;주황색과 흰색 줄무늬입니다.\u0026#34;2. Image Captioning# Image: [해변 사진] Caption: \u0026#34;맑은 날 파란 하늘 아래 모래사장에서 사람들이 휴식을 취하고 있습니다.\u0026#34;3. Visual Reasoning# Image: [수학 문제 사진] Q: \u0026#34;이 문제를 풀어주세요.\u0026#34; A: \u0026#34;주어진 방정식을 정리하면... 답은 x = 5입니다.\u0026#34;4. OCR + 이해# Image: [메뉴판 사진] Q: \u0026#34;가장 비싼 메뉴는?\u0026#34; A: \u0026#34;스테이크 35,000원이 가장 비쌉니다.\u0026#34; 최신 동향# 오픈소스 모델들# 모델 파라미터 특징 LLaVA-1.5 7B/13B 효율적인 학습 InternVL 6B-25B 강력한 성능 Qwen-VL 7B 다국어 지원 CogVLM 17B 세밀한 이해 발전 방향# 고해상도 처리: 더 세밀한 이미지 이해 비디오 이해: 시간적 정보 처리 다중 이미지: 여러 이미지 동시 처리 Grounding: 객체 위치 지정 구현 팁# 효율적인 추론# # 4-bit 양자화로 메모리 절약 from transformers import BitsAndBytesConfig bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16 ) model = LlavaForConditionalGeneration.from_pretrained( \u0026#34;llava-hf/llava-1.5-7b-hf\u0026#34;, quantization_config=bnb_config )배치 처리# # 여러 이미지 동시 처리 inputs = processor( text=[\u0026#34;\u0026lt;image\u0026gt;\\nDescribe this.\u0026#34;] * len(images), images=images, return_tensors=\u0026#34;pt\u0026#34;, padding=True ) 관련 콘텐츠# CLIP - 기반 모델 ViT - Vision encoder Transformer - 기본 아키텍처 "},{"id":23,"href":"/ko/docs/architecture/detection/yolo/","title":"YOLO","section":"Detection","content":"YOLO (You Only Look Once)# 개요# 논문: You Only Look Once: Unified, Real-Time Object Detection (2016) 저자: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi 핵심 기여: Detection을 단일 회귀 문제로 재정의, 실시간 처리 가능 핵심 아이디어# \u0026ldquo;Detection을 한 번의 forward pass로 해결\u0026rdquo;\nTwo-stage detector와 달리, 이미지를 그리드로 나누고 각 그리드에서 직접 박스와 클래스를 예측합니다.\nYOLO 버전 변화# 버전 연도 핵심 개선 mAP (COCO) FPS YOLOv1 2016 첫 one-stage detector 63.4 (VOC) 45 YOLOv2 2016 Batch Norm, Anchor 78.6 (VOC) 67 YOLOv3 2018 Multi-scale, FPN 33.0 30 YOLOv4 2020 CSPNet, Mish 43.5 65 YOLOv5 2020 PyTorch, 편의성 50.7 140 YOLOv8 2023 Anchor-free, SOTA 53.9 100+ YOLOv1 구조# 동작 원리# 1. 이미지를 S×S 그리드로 분할 (S=7) 2. 각 그리드 셀이 B개의 bounding box 예측 (B=2) 3. 각 박스: (x, y, w, h, confidence) 4. 각 그리드 셀이 C개 클래스 확률 예측 (C=20) Output: S × S × (B×5 + C) = 7 × 7 × 30네트워크 구조# Input (448×448×3) ↓ 24 Conv Layers (GoogLeNet inspired) ↓ 2 FC Layers ↓ Output (7×7×30)Confidence Score# $$\\text{Confidence} = P(\\text{Object}) \\times \\text{IoU}_{pred}^{truth}$$\nLoss Function# $$L = \\lambda_{coord} L_{coord} + L_{obj} + \\lambda_{noobj} L_{noobj} + L_{class}$$\n$\\lambda_{coord} = 5$: 좌표 loss 가중치 $\\lambda_{noobj} = 0.5$: 객체 없는 셀의 loss 가중치 YOLOv3 구조# 개선점# Multi-scale Prediction: 3가지 스케일에서 예측 FPN-like Structure: 다양한 크기의 객체 검출 Anchor Boxes: 9개의 anchor (3 scales × 3 ratios) Darknet-53: ResNet-like backbone 아키텍처# Input (416×416) ↓ Darknet-53 Backbone ↓ ┌─────────────────────────────────────┐ │ FPN-like Neck │ │ 52×52 (small) + 26×26 + 13×13 (large) │ └─────────────────────────────────────┘ ↓ 3 Detection Heads ↓ Output: - 13×13×255 (대형 객체) - 26×26×255 (중형 객체) - 52×52×255 (소형 객체) 255 = 3 anchors × (4 coords + 1 obj + 80 classes) YOLOv8 구조 (최신)# 주요 특징# Anchor-free: anchor box 없이 직접 예측 Decoupled Head: classification과 regression 분리 C2f Module: CSP + 더 효율적인 특징 융합 Task-specific Heads: detection, segmentation, pose 등 모델 변형# 모델 파라미터 mAP FPS (V100) YOLOv8n 3.2M 37.3 430 YOLOv8s 11.2M 44.9 350 YOLOv8m 25.9M 50.2 220 YOLOv8l 43.7M 52.9 160 YOLOv8x 68.2M 53.9 100 구현 예시# YOLOv8 (Ultralytics)# from ultralytics import YOLO # 모델 로드 model = YOLO(\u0026#39;yolov8n.pt\u0026#39;) # nano 모델 # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 확인 for result in results: boxes = result.boxes # Bounding boxes print(boxes.xyxy) # x1, y1, x2, y2 형식 print(boxes.conf) # confidence scores print(boxes.cls) # class indices # 학습 model.train(data=\u0026#39;coco128.yaml\u0026#39;, epochs=100) # Export model.export(format=\u0026#39;onnx\u0026#39;)YOLOv5 (torch hub)# import torch # 모델 로드 model = torch.hub.load(\u0026#39;ultralytics/yolov5\u0026#39;, \u0026#39;yolov5s\u0026#39;, pretrained=True) # 추론 results = model(\u0026#39;image.jpg\u0026#39;) # 결과 출력 results.print() results.show() # pandas DataFrame으로 결과 df = results.pandas().xyxy[0] YOLO vs Faster R-CNN# 측면 YOLO Faster R-CNN 방식 One-stage Two-stage 속도 빠름 (30-100+ FPS) 느림 (~7 FPS) 정확도 상대적으로 낮음 높음 작은 객체 어려움 상대적으로 좋음 실시간 가능 어려움 구현 단순 복잡 실제 활용# 자율주행: 실시간 객체 검출 CCTV 분석: 사람/차량 검출 산업 자동화: 제품 불량 검출 모바일: 경량화 모델 (YOLOv8n) 관련 콘텐츠# Faster R-CNN - Two-stage detector IoU \u0026amp; NMS - Detection 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표 "},{"id":24,"href":"/ko/docs/engineering/data/formats/","title":"데이터 포맷","section":"데이터","content":"데이터 포맷# 개요# Vision 태스크별로 다양한 데이터 포맷이 사용됩니다.\nClassification# ImageFolder 구조# dataset/ ├── train/ │ ├── cat/ │ │ ├── cat_001.jpg │ │ └── cat_002.jpg │ └── dog/ │ ├── dog_001.jpg │ └── dog_002.jpg └── val/ ├── cat/ └── dog/from torchvision.datasets import ImageFolder dataset = ImageFolder(root=\u0026#39;dataset/train\u0026#39;, transform=transform)CSV 형식# image_path,label train/cat_001.jpg,cat train/dog_001.jpg,dog Object Detection# COCO Format# { \u0026#34;images\u0026#34;: [ {\u0026#34;id\u0026#34;: 1, \u0026#34;file_name\u0026#34;: \u0026#34;image1.jpg\u0026#34;, \u0026#34;width\u0026#34;: 640, \u0026#34;height\u0026#34;: 480} ], \u0026#34;annotations\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;image_id\u0026#34;: 1, \u0026#34;category_id\u0026#34;: 1, \u0026#34;bbox\u0026#34;: [100, 100, 200, 150], // [x, y, width, height] \u0026#34;area\u0026#34;: 30000, \u0026#34;iscrowd\u0026#34;: 0 } ], \u0026#34;categories\u0026#34;: [ {\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;cat\u0026#34;} ] }Pascal VOC Format (XML)# \u0026lt;annotation\u0026gt; \u0026lt;filename\u0026gt;image1.jpg\u0026lt;/filename\u0026gt; \u0026lt;size\u0026gt; \u0026lt;width\u0026gt;640\u0026lt;/width\u0026gt; \u0026lt;height\u0026gt;480\u0026lt;/height\u0026gt; \u0026lt;/size\u0026gt; \u0026lt;object\u0026gt; \u0026lt;name\u0026gt;cat\u0026lt;/name\u0026gt; \u0026lt;bndbox\u0026gt; \u0026lt;xmin\u0026gt;100\u0026lt;/xmin\u0026gt; \u0026lt;ymin\u0026gt;100\u0026lt;/ymin\u0026gt; \u0026lt;xmax\u0026gt;300\u0026lt;/xmax\u0026gt; \u0026lt;ymax\u0026gt;250\u0026lt;/ymax\u0026gt; \u0026lt;/bndbox\u0026gt; \u0026lt;/object\u0026gt; \u0026lt;/annotation\u0026gt;YOLO Format (TXT)# # class_id x_center y_center width height (정규화된 값) 0 0.5 0.4 0.3 0.2 1 0.2 0.6 0.1 0.15디렉토리 구조:\ndataset/ ├── images/ │ ├── train/ │ └── val/ └── labels/ ├── train/ └── val/ Segmentation# COCO Segmentation# { \u0026#34;annotations\u0026#34;: [ { \u0026#34;segmentation\u0026#34;: [[x1,y1,x2,y2,...]], // polygon \u0026#34;area\u0026#34;: 1000, \u0026#34;iscrowd\u0026#34;: 0, \u0026#34;image_id\u0026#34;: 1, \u0026#34;bbox\u0026#34;: [100, 100, 200, 150], \u0026#34;category_id\u0026#34;: 1 } ] }Mask 이미지# dataset/ ├── images/ │ └── image1.jpg └── masks/ └── image1.png # 클래스별 픽셀값 (0: background, 1: class1, ...)RLE (Run-Length Encoding)# 압축된 마스크 표현:\nfrom pycocotools import mask as mask_util # Mask → RLE rle = mask_util.encode(np.asfortranarray(binary_mask)) # RLE → Mask mask = mask_util.decode(rle) 포맷 변환# COCO ↔ YOLO# def coco_to_yolo(bbox, img_width, img_height): \u0026#34;\u0026#34;\u0026#34;COCO [x, y, w, h] → YOLO [x_center, y_center, w, h] (normalized)\u0026#34;\u0026#34;\u0026#34; x, y, w, h = bbox x_center = (x + w / 2) / img_width y_center = (y + h / 2) / img_height w_norm = w / img_width h_norm = h / img_height return [x_center, y_center, w_norm, h_norm] def yolo_to_coco(bbox, img_width, img_height): \u0026#34;\u0026#34;\u0026#34;YOLO → COCO\u0026#34;\u0026#34;\u0026#34; x_center, y_center, w_norm, h_norm = bbox w = w_norm * img_width h = h_norm * img_height x = x_center * img_width - w / 2 y = y_center * img_height - h / 2 return [x, y, w, h]VOC ↔ COCO# def voc_to_coco(bbox): \u0026#34;\u0026#34;\u0026#34;VOC [xmin, ymin, xmax, ymax] → COCO [x, y, w, h]\u0026#34;\u0026#34;\u0026#34; xmin, ymin, xmax, ymax = bbox return [xmin, ymin, xmax - xmin, ymax - ymin] def coco_to_voc(bbox): \u0026#34;\u0026#34;\u0026#34;COCO → VOC\u0026#34;\u0026#34;\u0026#34; x, y, w, h = bbox return [x, y, x + w, y + h] 변환 도구# FiftyOne# import fiftyone as fo import fiftyone.utils.coco as fouc # COCO → FiftyOne dataset = fo.Dataset.from_dir( dataset_type=fo.types.COCODetectionDataset, data_path=\u0026#34;images\u0026#34;, labels_path=\u0026#34;annotations.json\u0026#34; ) # FiftyOne → YOLO dataset.export( export_dir=\u0026#34;yolo_dataset\u0026#34;, dataset_type=fo.types.YOLOv5Dataset, )Roboflow# 온라인 도구로 다양한 포맷 간 변환 지원\n데이터셋 검증# 기본 검증# import json from pathlib import Path def validate_coco(json_path, image_dir): with open(json_path) as f: coco = json.load(f) errors = [] # 이미지 파일 존재 확인 for img in coco[\u0026#39;images\u0026#39;]: if not (Path(image_dir) / img[\u0026#39;file_name\u0026#39;]).exists(): errors.append(f\u0026#34;Missing image: {img[\u0026#39;file_name\u0026#39;]}\u0026#34;) # bbox 유효성 확인 for ann in coco[\u0026#39;annotations\u0026#39;]: x, y, w, h = ann[\u0026#39;bbox\u0026#39;] if w \u0026lt;= 0 or h \u0026lt;= 0: errors.append(f\u0026#34;Invalid bbox: {ann[\u0026#39;id\u0026#39;]}\u0026#34;) return errorsFiftyOne으로 시각화# import fiftyone as fo dataset = fo.Dataset.from_dir(...) session = fo.launch_app(dataset) 관련 콘텐츠# 데이터 파이프라인 - 데이터 로딩 최적화 레이블링 - 어노테이션 도구 Detection - Detection 태스크 "},{"id":25,"href":"/ko/docs/engineering/deployment/","title":"배포","section":"실무 기술","content":"모델 배포 (Deployment)# 학습된 모델을 프로덕션 환경에 배포하는 기술입니다.\n주요 내용# 주제 설명 ONNX 모델 포맷 변환 TensorRT NVIDIA GPU 최적화 모델 서빙 API 서버 구축 최적화 양자화, 프루닝 배포 파이프라인# 학습 (PyTorch) → 변환 (ONNX) → 최적화 (TensorRT) → 서빙 (Triton) ↓ ↓ ↓ ↓ .pt 파일 .onnx 파일 .engine 파일 REST/gRPC API 고려사항# 요소 질문 성능 필요한 처리량(TPS)? 지연시간(latency)? 하드웨어 GPU? CPU? 엣지? 비용 클라우드 비용? 하드웨어 투자? 확장성 트래픽 증가에 대응? 운영 모니터링? 업데이트? A/B 테스트? "},{"id":26,"href":"/ko/docs/engineering/hardware/lighting/","title":"조명","section":"하드웨어","content":"조명# 개요# 조명은 Vision 시스템의 성공을 좌우하는 핵심 요소입니다.\n\u0026ldquo;좋은 조명이 좋은 이미지를 만들고, 좋은 이미지가 좋은 결과를 만든다\u0026rdquo;\n조명의 중요성# 조명 vs 알고리즘# 나쁜 조명 + 좋은 알고리즘 = 불안정한 결과 좋은 조명 + 간단한 알고리즘 = 안정적인 결과조명 설계 순서# 1. 검사 대상 분석 (재질, 형상, 결함 유형) ↓ 2. 조명 방식 선택 ↓ 3. 조명 배치 결정 ↓ 4. 밝기/색상 조정 ↓ 5. 테스트 및 최적화 조명 방식# 확산 (Diffuse)# ┌─────────────┐ │ ▓▓▓▓▓▓▓▓▓▓▓ │ ← 확산판 └─────────────┘ ↓ ~~~~~~~~~~~~~ ← 균일한 빛 ↓ ┌───────────┐ │ 피사체 │ └───────────┘ 균일한 조명 그림자 최소화 용도: 표면 검사, 문자 인식 직접광 (Direct)# ○ ← LED ╲ ╲ ╲ ┌───────────┐ │ 피사체 │ └───────────┘ 높은 대비 그림자 발생 용도: 엣지 검출, 형상 검사 동축 (Coaxial)# 카메라 ↓ ┌─────────────┐ │ Half Mirror │ └──────┬──────┘ ↙ ↓ ↘ LED → ↓ ← LED ↓ ┌───────────┐ │ 피사체 │ ← 반사광 직접 검출 └───────────┘ 정반사 표면 검사 스크래치, 먼지 검출 용도: 금속 표면, 유리 검사 역광 (Backlight)# 카메라 ↓ ┌───────────┐ │ 피사체 │ ← 실루엣 └───────────┘ ───────────── │▓▓▓▓▓▓▓▓▓▓▓│ ← 조명 └───────────┘ 실루엣/윤곽 검출 구멍, 형상 측정 용도: 치수 측정, 결손 검출 저각 (Low Angle / Dark Field)# 카메라 ↓ ───←───────────────────→─── LED ───→ ←─── LED ┌───────────┐ │ 피사체 │ └───────────┘ 표면 결함 강조 스크래치, 기포 검출 용도: 표면 결함 검사 돔 조명 (Dome)# 카메라 ↓ ╭─────────────╮ │ ▓▓▓▓▓▓▓▓▓▓▓ │ ← 내부 LED │ ○ │ ← 구멍 │ ▓▓▓▓▓▓▓▓▓▓▓ │ ╰─────────────╯ ↓ ┌───────────┐ │ 피사체 │ └───────────┘ 완전 균일 조명 반사 최소화 용도: 곡면 검사, 광택 표면 LED 조명# 색상 선택# 색상 파장 용도 빨강 620-750nm 녹색 물체 대비, 표면 검사 녹색 495-570nm 붉은 물체 대비 파랑 450-495nm 얕은 표면 결함 백색 전체 일반 촬영 적외선 750-1000nm 투과 검사, 흑백 촬영 자외선 315-400nm 형광 검사 색상 활용 예시# 빨간 배경 위의 녹색 결함 검출: → 빨간 LED + 빨간 필터 = 결함만 어둡게 녹색 PCB의 회로 검사: → 빨간 LED = 녹색 부분 어둡게, 회로 밝게 조명 컨트롤러# 기능# ┌─────────────────────────────────────┐ │ 조명 컨트롤러 │ │ │ │ 밝기 조절 ← 아날로그/디지털 제어 │ │ 스트로브 ← 짧은 순간 강한 빛 │ │ 동기화 ← 카메라 트리거 연동 │ │ 복수 채널 ← 여러 조명 개별 제어 │ └─────────────────────────────────────┘스트로브 조명# 연속 조명: ───────────────────── 밝기 제한, 발열 스트로브: ▃▃▃ ▃▃▃ ▃▃▃ 고밝기, 블러 감소 ↑ ↑ ↑ 카메라 트리거 문제 해결# 반사 (Reflection)# 문제: 광택 표면에서 핫스팟 해결: - 동축 조명 사용 - 확산광/돔 조명 - 편광 필터그림자 (Shadow)# 문제: 요철에 의한 그림자 해결: - 다방향 조명 - 돔 조명 - 확산광불균일 (Uneven)# 문제: 이미지 가장자리가 어두움 해결: - 조명 거리 조정 - 복수 조명 배치 - 소프트웨어 보정 (Flat-field correction) 조명 선택 가이드# 검사 유형 권장 조명 표면 결함 (스크래치) 저각/다크필드 치수 측정 역광 문자 인식 확산광/돔 금속 표면 동축 곡면 검사 돔 투과 검사 역광 실용적 팁# 1. 먼저 실험# 여러 조명 방식을 실제로 테스트 → 이론과 다를 수 있음2. 안정성 확보# - 외부 광원 차단 (암실, 커버) - 조명 노화 모니터링 - 주기적 보정3. 비용 고려# 고가 카메라 + 저가 조명 ❌ 적정 카메라 + 최적 조명 ✓ 관련 콘텐츠# 카메라 - 이미지 센서 엣지 디바이스 - 처리 장치 데이터 증강 - 소프트웨어 보완 "},{"id":27,"href":"/ko/docs/bottomup/","title":"Bottom-Up","section":"Docs","content":"Bottom-Up 학습# 기초 개념부터 차근차근 쌓아올립니다.\nLevel 1: 수학 기초# 선형대수 미적분 \u0026amp; Chain Rule 확률/통계 Level 2: 딥러닝 기초# Convolution Backpropagation Loss Functions Optimization Level 3: 기본 아키텍처# CNN 기초 AlexNet VGG ResNet Level 4: 기본 태스크# Classification Detection Segmentation Level 5: 고급 개념# Attention Transformer ViT Level 6: 생성 모델# Diffusion Process Stable Diffusion ControlNet DiT Generation Level 7: Multimodal \u0026amp; 3D# Contrastive Learning CLIP VLM Vision-Language NeRF 3D Vision "},{"id":28,"href":"/ko/docs/architecture/detection/","title":"Detection","section":"아키텍처","content":"Object Detection 모델# 이미지에서 객체의 위치(bounding box)와 클래스를 동시에 예측하는 모델들입니다.\n두 가지 접근 방식# Two-Stage Detector# Region Proposal: 객체가 있을 법한 영역 제안 Classification: 각 영역을 분류 특징: 정확도 높음, 속도 느림\nR-CNN → Fast R-CNN → Faster R-CNN One-Stage Detector# 한 번에 위치와 클래스를 예측\n특징: 속도 빠름, 실시간 가능\nYOLO SSD RetinaNet 모델 비교# 모델 유형 FPS mAP (COCO) 특징 Faster R-CNN Two-stage ~7 42.0 높은 정확도 YOLOv3 One-stage ~30 33.0 실시간 YOLOv8 One-stage ~100 53.9 SOTA DETR Transformer ~28 42.0 End-to-end 관련 콘텐츠# IoU \u0026amp; NMS - Detection의 핵심 개념 Anchor Box - 박스 예측 방식 Detection 태스크 - 평가 지표, 데이터셋 "},{"id":29,"href":"/ko/docs/architecture/cnn/resnet/","title":"ResNet","section":"CNN","content":"ResNet (Residual Network)# 개요# 논문: Deep Residual Learning for Image Recognition (2015) 저자: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research) 핵심 기여: Skip Connection으로 100+ 레이어 학습 가능하게 함 해결한 문제: Degradation Problem# 네트워크가 깊어지면 성능이 오히려 떨어지는 현상:\n20-layer: 7.4% error 56-layer: 8.5% error ← 더 깊은데 성능이 나빠짐이는 과적합이 아니라 최적화의 어려움 때문입니다.\n핵심 아이디어: Residual Learning# Skip Connection (Shortcut)# 기존 학습: $$H(x) = F(x)$$\nResidual 학습: $$H(x) = F(x) + x$$\n여기서 $F(x)$는 **잔차(residual)**를 학습합니다.\n직관# 입력을 그대로 출력에 더함 네트워크는 \u0026ldquo;변화량\u0026quot;만 학습 Identity mapping이 쉬워짐 → 깊은 네트워크 학습 가능 구조# Basic Block (ResNet-18/34)# x → Conv3×3 → BN → ReLU → Conv3×3 → BN → (+x) → ReLU └─────────────────────────────────────┘ skip connectionBottleneck Block (ResNet-50/101/152)# x → Conv1×1 → BN → ReLU → Conv3×3 → BN → ReLU → Conv1×1 → BN → (+x) → ReLU └────────────────────────────────────────────────────────────┘ skip connection1×1 Conv로 채널을 줄였다가 늘려 계산량 감소.\nResNet 변형# 모델 레이어 파라미터 Top-5 Error ResNet-18 18 11.7M 10.92% ResNet-34 34 21.8M 9.46% ResNet-50 50 25.6M 7.13% ResNet-101 101 44.5M 6.44% ResNet-152 152 60.2M 6.16% 전체 아키텍처 (ResNet-50)# Input (224×224×3) ↓ Conv 7×7, 64, stride 2 ↓ MaxPool 3×3, stride 2 ↓ Stage 1: [Bottleneck(64, 256)] × 3 ↓ Stage 2: [Bottleneck(128, 512)] × 4 ↓ Stage 3: [Bottleneck(256, 1024)] × 6 ↓ Stage 4: [Bottleneck(512, 2048)] × 3 ↓ Global Average Pool ↓ FC → 1000 구현 예시# import torch.nn as nn class BasicBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) # Skip connection (차원 맞추기) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) out += identity # Skip connection out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, in_channels, mid_channels, stride=1): super().__init__() out_channels = mid_channels * self.expansion self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False) self.bn1 = nn.BatchNorm2d(mid_channels) self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, stride, 1, bias=False) self.bn2 = nn.BatchNorm2d(mid_channels) self.conv3 = nn.Conv2d(mid_channels, out_channels, 1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.shortcut = nn.Sequential() if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm2d(out_channels) ) def forward(self, x): identity = self.shortcut(x) out = self.relu(self.bn1(self.conv1(x))) out = self.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += identity out = self.relu(out) return out ResNet의 영향# Backbone으로 널리 사용# Object Detection: Faster R-CNN Segmentation: Mask R-CNN 대부분의 Vision 태스크에서 기본 backbone 후속 연구# ResNeXt: 그룹 컨볼루션 추가 DenseNet: 모든 레이어를 연결 SE-Net: 채널 attention 추가 EfficientNet: 최적 스케일링 왜 Skip Connection이 효과적인가?# 1. Gradient Flow 개선# $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\left(1 + \\frac{\\partial F}{\\partial x}\\right)$$\n항상 1이 더해지므로 gradient가 소실되지 않습니다.\n2. Ensemble 효과# Skip connection은 다양한 깊이의 경로를 만들어 앙상블과 유사한 효과를 냅니다.\n관련 콘텐츠# VGG - 깊이의 한계를 보여준 모델 CNN 기초 - Convolution의 원리 Detection - ResNet을 backbone으로 사용 ViT - ResNet을 대체하는 새로운 패러다임 "},{"id":30,"href":"/ko/docs/architecture/segmentation/sam/","title":"SAM","section":"Segmentation","content":"SAM (Segment Anything Model)# 개요# 논문: Segment Anything (2023) 저자: Alexander Kirillov et al. (Meta AI) 핵심 기여: 프롬프트 기반의 범용 segmentation foundation model 핵심 아이디어# \u0026ldquo;한 번 학습으로 모든 객체를 segmentation\u0026rdquo;\nPoint, box, text 등 다양한 프롬프트로 원하는 객체를 분할합니다.\n프로젝트 구성# 세 가지 요소# Task: Promptable segmentation Model: SAM 아키텍처 Data: SA-1B 데이터셋 (11M 이미지, 1.1B 마스크) 구조# 전체 아키텍처# ┌─────────────────────────────────────────┐ │ Image Encoder │ │ (ViT-H, 실행 1회) │ │ Image → Image Embeddings │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Prompt Encoder │ │ Points/Boxes/Text → Prompt Tokens │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Mask Decoder │ │ Image Emb + Prompt → Masks │ └─────────────────────────────────────────┘ ↓ Output Masks1. Image Encoder# 모델: ViT-H (Vision Transformer Huge) 입력: 1024×1024 이미지 출력: 64×64 feature embedding 특징: 이미지당 한 번만 실행 (0.15초) 2. Prompt Encoder# 다양한 프롬프트 처리:\n프롬프트 인코딩 방식 Point Positional encoding + learned embedding Box Corner points의 positional encoding Mask Convolution으로 downsampling Text CLIP text encoder (연구 중) 3. Mask Decoder# Transformer 기반 경량 디코더 양방향 attention (prompt ↔ image) 다중 마스크 출력 (모호성 해결) 프롬프트 유형# Point Prompt# # 클릭 한 번으로 객체 선택 point_coords = [[500, 375]] # x, y point_labels = [1] # 1: foreground, 0: backgroundBox Prompt# # Bounding box로 객체 범위 지정 box = [100, 100, 400, 400] # x1, y1, x2, y2Mask Prompt# # 이전 마스크를 프롬프트로 사용 (iterative refinement) mask_input = previous_mask 구현 예시# from segment_anything import sam_model_registry, SamPredictor # 모델 로드 sam = sam_model_registry[\u0026#34;vit_h\u0026#34;](checkpoint=\u0026#34;sam_vit_h.pth\u0026#34;) predictor = SamPredictor(sam) # 이미지 설정 (한 번만 실행) predictor.set_image(image) # Point prompt로 예측 masks, scores, logits = predictor.predict( point_coords=np.array([[500, 375]]), point_labels=np.array([1]), multimask_output=True, # 3개 마스크 출력 ) # Box prompt로 예측 masks, scores, logits = predictor.predict( box=np.array([100, 100, 400, 400]), multimask_output=False, )Automatic Mask Generation# from segment_anything import SamAutomaticMaskGenerator mask_generator = SamAutomaticMaskGenerator(sam) # 이미지의 모든 객체 자동 분할 masks = mask_generator.generate(image) # 각 마스크 정보 for mask in masks: print(mask[\u0026#39;segmentation\u0026#39;].shape) # binary mask print(mask[\u0026#39;area\u0026#39;]) # 마스크 면적 print(mask[\u0026#39;bbox\u0026#39;]) # bounding box print(mask[\u0026#39;predicted_iou\u0026#39;]) # 예측 IoU print(mask[\u0026#39;stability_score\u0026#39;]) # 안정성 점수 모델 변형# 모델 Encoder 파라미터 속도 SAM ViT-H ViT-Huge 636M 0.15s/image SAM ViT-L ViT-Large 308M 0.1s/image SAM ViT-B ViT-Base 91M 0.06s/image 경량화 버전# MobileSAM: 모바일용 경량 모델 FastSAM: YOLO 기반 실시간 버전 EfficientSAM: 효율적인 학습 방법 SA-1B 데이터셋# 데이터 엔진# 1. Manual Phase: 전문가가 마스크 생성 2. Semi-Automatic: SAM 제안 + 사람 수정 3. Fully Automatic: SAM 자동 생성 + 검증규모# 11M 이미지 1.1B 마스크 평균 이미지당 100개 마스크 활용# Zero-shot Transfer# 학습하지 않은 도메인에서도 동작:\n의료 영상 위성 영상 현미경 이미지 응용 분야# 이미지 편집: 객체 선택 및 제거 Annotation 도구: 라벨링 자동화 AR/VR: 실시간 객체 분할 비디오 분석: 프레임별 segmentation SAM 2 (2024)# SAM의 비디오 확장:\n비디오 전체에서 객체 추적 시간적 일관성 유지 메모리 효율적 처리 한계점# 큰 모델 크기 (ViT-H) 세밀한 경계에서 부정확 Text prompt 지원 제한적 Real-time 어려움 관련 콘텐츠# ViT - Image encoder 기반 Mask R-CNN - Instance segmentation CLIP - Text-image 연결 Segmentation 태스크 - 평가 지표 "},{"id":31,"href":"/ko/docs/architecture/generative/stable-diffusion/","title":"Stable Diffusion","section":"Generative","content":"Stable Diffusion# 개요# 논문: High-Resolution Image Synthesis with Latent Diffusion Models (2022) 저자: Robin Rombach et al. (CompVis, Stability AI) 핵심 기여: Latent space에서 Diffusion을 수행하여 효율적인 고해상도 생성 핵심 아이디어# \u0026ldquo;픽셀 공간이 아닌 압축된 잠재 공간에서 Diffusion\u0026rdquo;\n기존 DDPM은 픽셀 공간에서 직접 작동해 계산량이 많았습니다. Stable Diffusion은 VAE로 이미지를 압축한 후 latent space에서 diffusion을 수행합니다.\nDiffusion 기본 개념# Forward Process (노이즈 추가)# $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$$\n점진적으로 노이즈를 추가하여 순수 노이즈로 변환\nReverse Process (노이즈 제거)# $$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n학습된 모델로 노이즈를 제거하며 이미지 복원\n구조# 전체 아키텍처# Text Prompt ↓ ┌─────────────────────────────┐ │ Text Encoder (CLIP) │ └─────────────────────────────┘ ↓ Text Embedding ↓ Random Noise ↓ ↓ ┌─────────────────────────────────────────────┐ │ U-Net │ │ (Latent Diffusion in compressed space) │ │ │ │ Input: z_t + timestep + text_embedding │ │ Output: predicted noise ε │ └─────────────────────────────────────────────┘ ↓ Denoised Latent z_0 ↓ ┌─────────────────────────────┐ │ VAE Decoder │ └─────────────────────────────┘ ↓ Generated Image (512×512 or higher)핵심 컴포넌트# 컴포넌트 역할 VAE 이미지 ↔ Latent 변환 (8× 압축) U-Net 조건부 노이즈 예측 Text Encoder 텍스트 → 임베딩 (CLIP) Scheduler 샘플링 전략 (DDPM, DDIM 등) VAE (Variational Autoencoder)# 512×512 이미지를 64×64×4 latent로 압축:\nImage (512×512×3) → VAE Encoder → Latent (64×64×4) Latent (64×64×4) → VAE Decoder → Image (512×512×3)장점:\n계산량 64배 감소 (512² vs 64²) 메모리 효율적 의미있는 latent space U-Net# 조건부 노이즈 예측 네트워크:\n# U-Net 입력 z_t: 노이즈가 추가된 latent (64×64×4) t: timestep embedding c: text embedding (cross-attention으로 주입) # U-Net 출력 epsilon: 예측된 노이즈 (64×64×4)Cross-Attention# 텍스트 조건을 U-Net에 주입:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$\nQ: 이미지 특징에서 K, V: 텍스트 임베딩에서 학습# 목적 함수# $$L = \\mathbb{E}{z, \\epsilon, t, c} \\left[ | \\epsilon - \\epsilon\\theta(z_t, t, c) |^2 \\right]$$\n$z$: 원본 이미지의 latent $\\epsilon$: 추가된 노이즈 $t$: timestep $c$: 텍스트 조건 $\\epsilon_\\theta$: 예측된 노이즈 학습 데이터# LAION-5B: 50억 이미지-텍스트 쌍 대규모 데이터로 일반화 능력 확보 샘플링# DDPM Sampling# # T=1000 스텝에서 시작 z_T = torch.randn_like(latent) for t in reversed(range(T)): # 노이즈 예측 eps = unet(z_t, t, text_embedding) # 한 스텝 denoise z_{t-1} = denoise_step(z_t, eps, t) # VAE Decoder로 이미지 변환 image = vae.decode(z_0)DDIM Sampling (빠른 버전)# 50 스텝으로도 좋은 품질:\nDeterministic sampling Skip steps 가능 구현 예시# Diffusers 라이브러리# from diffusers import StableDiffusionPipeline import torch # 모델 로드 pipe = StableDiffusionPipeline.from_pretrained( \u0026#34;runwayml/stable-diffusion-v1-5\u0026#34;, torch_dtype=torch.float16 ).to(\u0026#34;cuda\u0026#34;) # 이미지 생성 prompt = \u0026#34;a photo of an astronaut riding a horse\u0026#34; image = pipe(prompt).images[0] image.save(\u0026#34;output.png\u0026#34;)세부 제어# image = pipe( prompt=\u0026#34;a beautiful landscape\u0026#34;, negative_prompt=\u0026#34;blurry, low quality\u0026#34;, num_inference_steps=50, guidance_scale=7.5, # CFG 강도 height=512, width=512, generator=torch.Generator().manual_seed(42) ).images[0] Classifier-Free Guidance (CFG)# 조건부/무조건부 예측을 결합:\n$$\\epsilon = \\epsilon_\\theta(z_t, t, \\emptyset) + s \\cdot (\\epsilon_\\theta(z_t, t, c) - \\epsilon_\\theta(z_t, t, \\emptyset))$$\n$s = 1$: 순수 조건부 $s \u0026gt; 1$: 조건 강화 (보통 7.5 사용) 높을수록 프롬프트에 충실하지만 다양성 감소 Stable Diffusion 버전# 버전 특징 SD 1.x CLIP ViT-L/14, 512×512 SD 2.x OpenCLIP, 768×768, depth SDXL 1024×1024, 2개 text encoder SD 3 DiT 아키텍처, Flow Matching 활용# Image-to-Image# from diffusers import StableDiffusionImg2ImgPipeline pipe = StableDiffusionImg2ImgPipeline.from_pretrained(...) new_image = pipe( prompt=\u0026#34;oil painting style\u0026#34;, image=init_image, strength=0.75 # 0: 원본 유지, 1: 완전 재생성 ).images[0]Inpainting# from diffusers import StableDiffusionInpaintPipeline pipe = StableDiffusionInpaintPipeline.from_pretrained(...) result = pipe( prompt=\u0026#34;a cat sitting\u0026#34;, image=image, mask_image=mask # 수정할 영역 ).images[0] 관련 콘텐츠# Diffusion Process - Diffusion 수학 VAE - Latent 압축 ControlNet - 추가 조건 제어 DiT - Transformer 기반 Diffusion Generation 태스크 - 평가 지표 "},{"id":32,"href":"/ko/docs/engineering/data/pipeline/","title":"데이터 파이프라인","section":"데이터","content":"데이터 파이프라인# 개요# 효율적인 데이터 로딩은 학습 속도에 큰 영향을 미칩니다.\nPyTorch DataLoader# 기본 구조# from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, image_paths, labels, transform=None): self.image_paths = image_paths self.labels = labels self.transform = transform def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image = Image.open(self.image_paths[idx]).convert(\u0026#39;RGB\u0026#39;) label = self.labels[idx] if self.transform: image = self.transform(image) return image, label # DataLoader loader = DataLoader( dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, )주요 파라미터# 파라미터 설명 권장값 batch_size 배치 크기 GPU 메모리에 맞게 num_workers 데이터 로딩 프로세스 CPU 코어 수 * 0.5~1 pin_memory GPU 전송 최적화 True (CUDA 사용 시) prefetch_factor 미리 로딩할 배치 수 2 (기본값) persistent_workers 워커 재사용 True (epoch 간) 병목 지점 파악# 데이터 로딩 vs 학습# import time data_time = 0 train_time = 0 for batch in loader: start = time.time() images, labels = batch images = images.cuda() data_time += time.time() - start start = time.time() output = model(images) loss = criterion(output, labels) loss.backward() optimizer.step() train_time += time.time() - start print(f\u0026#34;Data loading: {data_time:.2f}s\u0026#34;) print(f\u0026#34;Training: {train_time:.2f}s\u0026#34;)해결 방법# 병목 해결책 데이터 로딩 느림 num_workers 증가, SSD 사용 전처리 느림 사전 처리, GPU 전처리 GPU 대기 prefetch_factor 증가 메모리 부족 batch_size 감소, gradient accumulation 최적화 기법# 1. 멀티프로세싱 최적화# # num_workers 최적화 import multiprocessing num_workers = min(8, multiprocessing.cpu_count()) loader = DataLoader( dataset, batch_size=32, num_workers=num_workers, persistent_workers=True, # 워커 재사용 prefetch_factor=2, )2. 메모리 매핑# 큰 데이터셋을 메모리에 올리지 않고 처리:\nimport numpy as np # 사전에 저장 np.save(\u0026#39;images.npy\u0026#39;, all_images) # 메모리 매핑으로 로딩 data = np.load(\u0026#39;images.npy\u0026#39;, mmap_mode=\u0026#39;r\u0026#39;)3. 사전 처리 (Preprocessing)# # 1. 이미지를 미리 리사이즈해서 저장 from PIL import Image from pathlib import Path def preprocess_images(src_dir, dst_dir, size=(224, 224)): for path in Path(src_dir).glob(\u0026#39;**/*.jpg\u0026#39;): img = Image.open(path) img = img.resize(size) out_path = Path(dst_dir) / path.relative_to(src_dir) out_path.parent.mkdir(parents=True, exist_ok=True) img.save(out_path) # 2. 또는 PyTorch 텐서로 저장 import torch torch.save(preprocessed_tensors, \u0026#39;processed_data.pt\u0026#39;)4. LMDB / HDF5# 대용량 데이터 효율적 저장:\nimport lmdb import pickle # 저장 env = lmdb.open(\u0026#39;dataset.lmdb\u0026#39;, map_size=1e12) with env.begin(write=True) as txn: for i, (img, label) in enumerate(data): txn.put(str(i).encode(), pickle.dumps((img, label))) # 로딩 class LMDBDataset(Dataset): def __init__(self, lmdb_path): self.env = lmdb.open(lmdb_path, readonly=True) with self.env.begin() as txn: self.length = txn.stat()[\u0026#39;entries\u0026#39;] def __getitem__(self, idx): with self.env.begin() as txn: data = pickle.loads(txn.get(str(idx).encode())) return data5. WebDataset (대규모 분산 학습)# import webdataset as wds # tar 파일로 저장된 데이터 스트리밍 dataset = wds.WebDataset(\u0026#34;dataset-{000000..000099}.tar\u0026#34;) .shuffle(1000) .decode(\u0026#34;pil\u0026#34;) .to_tuple(\u0026#34;jpg\u0026#34;, \u0026#34;cls\u0026#34;) loader = DataLoader(dataset, batch_size=32) GPU 전처리# NVIDIA DALI# CPU 대신 GPU에서 전처리:\nfrom nvidia.dali import pipeline_def import nvidia.dali.fn as fn import nvidia.dali.types as types @pipeline_def def image_pipeline(): jpegs, labels = fn.readers.file(file_root=\u0026#39;images/\u0026#39;) images = fn.decoders.image(jpegs, device=\u0026#39;mixed\u0026#39;) # GPU 디코딩 images = fn.resize(images, resize_x=224, resize_y=224) images = fn.crop_mirror_normalize( images, mean=[0.485 * 255, 0.456 * 255, 0.406 * 255], std=[0.229 * 255, 0.224 * 255, 0.225 * 255], mirror=fn.random.coin_flip() ) return images, labels pipe = image_pipeline(batch_size=32, num_threads=4, device_id=0)Kornia (PyTorch GPU 증강)# import kornia.augmentation as K import torch.nn as nn class GPUAugmentation(nn.Module): def __init__(self): super().__init__() self.transform = nn.Sequential( K.RandomHorizontalFlip(p=0.5), K.ColorJitter(0.1, 0.1, 0.1, 0.1), K.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) def forward(self, x): return self.transform(x) # 학습 루프 aug = GPUAugmentation().cuda() for images, labels in loader: images = images.cuda() images = aug(images) # GPU에서 증강 output = model(images) 분산 학습용 DataLoader# from torch.utils.data.distributed import DistributedSampler sampler = DistributedSampler( dataset, num_replicas=world_size, rank=rank, shuffle=True ) loader = DataLoader( dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True, ) # 각 epoch마다 sampler 셔플 for epoch in range(epochs): sampler.set_epoch(epoch) for batch in loader: ... 체크리스트# num_workers 최적화 (보통 4-8) pin_memory=True 설정 persistent_workers=True 설정 이미지를 미리 리사이즈 SSD에 데이터 저장 데이터 로딩 시간 측정 GPU 사용률 모니터링 (nvidia-smi) 관련 콘텐츠# 데이터 증강 - 증강 기법 데이터 포맷 - 포맷 변환 배포 - 추론 최적화 "},{"id":33,"href":"/ko/docs/engineering/deployment/serving/","title":"모델 서빙","section":"배포","content":"모델 서빙# 개요# 학습된 모델을 API로 제공하는 인프라 구축입니다.\n서빙 옵션# 도구 특징 적합한 경우 FastAPI 간단, Python 친화적 프로토타입, 소규모 Triton 고성능, 다중 모델 대규모 프로덕션 TorchServe PyTorch 통합 PyTorch 중심 TensorFlow Serving TF 최적화 TensorFlow 중심 BentoML ML 특화 빠른 배포 FastAPI 서빙# 기본 구조# from fastapi import FastAPI, File, UploadFile from PIL import Image import torch import io app = FastAPI() # 모델 로드 (시작 시 한 번) model = torch.load(\u0026#34;model.pt\u0026#34;) model.eval() @app.post(\u0026#34;/predict\u0026#34;) async def predict(file: UploadFile = File(...)): # 이미지 로드 image_bytes = await file.read() image = Image.open(io.BytesIO(image_bytes)) # 전처리 input_tensor = preprocess(image) # 추론 with torch.no_grad(): output = model(input_tensor) # 후처리 result = postprocess(output) return {\u0026#34;prediction\u0026#34;: result} # 실행: uvicorn main:app --host 0.0.0.0 --port 8000배치 처리# from fastapi import BackgroundTasks import asyncio from collections import deque batch_queue = deque() batch_size = 32 batch_timeout = 0.1 # 100ms async def batch_processor(): while True: if len(batch_queue) \u0026gt;= batch_size or \\ (len(batch_queue) \u0026gt; 0 and time_since_first \u0026gt; batch_timeout): # 배치 처리 batch = [batch_queue.popleft() for _ in range(min(len(batch_queue), batch_size))] inputs = torch.stack([item[\u0026#39;input\u0026#39;] for item in batch]) outputs = model(inputs) for item, output in zip(batch, outputs): item[\u0026#39;future\u0026#39;].set_result(output) await asyncio.sleep(0.01) @app.on_event(\u0026#34;startup\u0026#34;) async def startup(): asyncio.create_task(batch_processor()) Triton Inference Server# 모델 저장소# model_repository/ ├── resnet/ │ ├── config.pbtxt │ └── 1/ │ └── model.onnx └── yolo/ ├── config.pbtxt └── 1/ └── model.planconfig.pbtxt# name: \u0026#34;resnet\u0026#34; platform: \u0026#34;onnxruntime_onnx\u0026#34; max_batch_size: 32 input [ { name: \u0026#34;input\u0026#34; data_type: TYPE_FP32 dims: [3, 224, 224] } ] output [ { name: \u0026#34;output\u0026#34; data_type: TYPE_FP32 dims: [1000] } ] dynamic_batching { preferred_batch_size: [8, 16, 32] max_queue_delay_microseconds: 100000 } instance_group [ { count: 2, kind: KIND_GPU } ]서버 실행# docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\ -v $(pwd)/model_repository:/models \\ nvcr.io/nvidia/tritonserver:23.10-py3 \\ tritonserver --model-repository=/models클라이언트# import tritonclient.http as httpclient import numpy as np client = httpclient.InferenceServerClient(url=\u0026#34;localhost:8000\u0026#34;) # 입력 준비 input_data = np.random.randn(1, 3, 224, 224).astype(np.float32) inputs = [httpclient.InferInput(\u0026#34;input\u0026#34;, input_data.shape, \u0026#34;FP32\u0026#34;)] inputs[0].set_data_from_numpy(input_data) # 추론 outputs = [httpclient.InferRequestedOutput(\u0026#34;output\u0026#34;)] result = client.infer(\u0026#34;resnet\u0026#34;, inputs, outputs=outputs) output_data = result.as_numpy(\u0026#34;output\u0026#34;) TorchServe# 모델 아카이브 생성# # handler.py 작성 torch-model-archiver --model-name resnet \\ --version 1.0 \\ --model-file model.py \\ --serialized-file model.pt \\ --handler image_classifier \\ --export-path model_storehandler.py# from ts.torch_handler.vision_handler import VisionHandler class MyHandler(VisionHandler): def preprocess(self, data): images = [] for row in data: image = row.get(\u0026#34;data\u0026#34;) or row.get(\u0026#34;body\u0026#34;) image = Image.open(io.BytesIO(image)) image = self.transform(image) images.append(image) return torch.stack(images) def postprocess(self, output): probs = torch.softmax(output, dim=1) return probs.tolist()서버 실행# torchserve --start --model-store model_store --models resnet=resnet.mar BentoML# 모델 저장# import bentoml # 모델 저장 bentoml.pytorch.save_model(\u0026#34;resnet\u0026#34;, model)서비스 정의# # service.py import bentoml from bentoml.io import Image, JSON runner = bentoml.pytorch.get(\u0026#34;resnet:latest\u0026#34;).to_runner() svc = bentoml.Service(\u0026#34;image_classifier\u0026#34;, runners=[runner]) @svc.api(input=Image(), output=JSON()) async def predict(image): input_tensor = preprocess(image) result = await runner.async_run(input_tensor) return postprocess(result)빌드 및 배포# bentoml build bentoml serve service:svc 성능 최적화# 동시성# # FastAPI with uvicorn workers uvicorn main:app --workers 4 --host 0.0.0.0 --port 8000GPU 메모리 관리# # 모델별 GPU 할당 import os os.environ[\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;] = \u0026#34;0\u0026#34; # 또는 Triton에서 instance_group 설정모니터링# from prometheus_client import Counter, Histogram, start_http_server REQUEST_COUNT = Counter(\u0026#39;requests_total\u0026#39;, \u0026#39;Total requests\u0026#39;) REQUEST_LATENCY = Histogram(\u0026#39;request_latency_seconds\u0026#39;, \u0026#39;Request latency\u0026#39;) @app.middleware(\u0026#34;http\u0026#34;) async def add_metrics(request, call_next): REQUEST_COUNT.inc() with REQUEST_LATENCY.time(): response = await call_next(request) return response Docker 배포# Dockerfile# FROM python:3.10-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;]docker-compose.yml# version: \u0026#39;3.8\u0026#39; services: api: build: . ports: - \u0026#34;8000:8000\u0026#34; deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] Kubernetes 배포# apiVersion: apps/v1 kind: Deployment metadata: name: model-server spec: replicas: 3 selector: matchLabels: app: model-server template: spec: containers: - name: model-server image: my-model-server:latest ports: - containerPort: 8000 resources: limits: nvidia.com/gpu: 1 --- apiVersion: v1 kind: Service metadata: name: model-server spec: selector: app: model-server ports: - port: 80 targetPort: 8000 type: LoadBalancer 관련 콘텐츠# TensorRT - GPU 최적화 ONNX - 모델 변환 최적화 - 모델 경량화 "},{"id":34,"href":"/ko/docs/engineering/hardware/edge/","title":"엣지 디바이스","section":"하드웨어","content":"엣지 디바이스# 개요# 엣지에서 AI 추론을 수행하는 하드웨어입니다.\n왜 엣지인가?# 클라우드 엣지 높은 지연시간 낮은 지연시간 네트워크 의존 오프라인 가능 대역폭 비용 로컬 처리 보안/프라이버시 우려 데이터 로컬 유지 무제한 컴퓨팅 제한된 리소스 NVIDIA Jetson 시리즈# 라인업# 모델 GPU 메모리 AI 성능 용도 Orin NX 16GB Ampere 1024 CUDA 16GB 100 TOPS 고성능 엣지 Orin NX 8GB Ampere 918 CUDA 8GB 70 TOPS 상용 Orin Nano 8GB Ampere 1024 CUDA 8GB 40 TOPS 입문/개발 Orin Nano 4GB Ampere 512 CUDA 4GB 20 TOPS 저비용 Xavier NX Volta 384 CUDA 8GB 21 TOPS 이전 세대 Nano (구형) Maxwell 128 CUDA 4GB 0.5 TFLOPS 교육용 JetPack 설치# # NVIDIA SDK Manager 사용 (호스트 PC에서) # 또는 SD 카드 이미지 다운로드 후 플래싱환경 설정# # 전력 모드 설정 sudo nvpmodel -m 0 # MAXN (최대 성능) sudo nvpmodel -m 1 # 저전력 # 팬 속도 sudo jetson_clocks --fan # 상태 모니터링 jtop # tegrastats의 GUI 버전 Jetson에서 모델 실행# TensorRT 변환# # ONNX → TensorRT import tensorrt as trt # trtexec 사용 # trtexec --onnx=model.onnx --saveEngine=model.engine --fp16PyTorch 직접 실행# import torch import torchvision.models as models model = models.resnet50(pretrained=True) model.eval().cuda() # FP16 추론 model.half() input = torch.randn(1, 3, 224, 224).half().cuda() with torch.no_grad(): output = model(input)DeepStream# NVIDIA의 스트리밍 분석 SDK:\nCamera → Decode → Inference → Tracking → Display/Save (NVDEC) (TensorRT) (NvTracker)# 파이프라인 실행 deepstream-app -c config.txt 기타 엣지 디바이스# Google Coral# 모델 AI 성능 인터페이스 USB Accelerator 4 TOPS USB Dev Board 4 TOPS 독립형 Dev Board Mini 4 TOPS 소형 from pycoral.utils.edgetpu import make_interpreter from pycoral.adapters import common interpreter = make_interpreter(\u0026#39;model_edgetpu.tflite\u0026#39;) interpreter.allocate_tensors() common.set_input(interpreter, image) interpreter.invoke() output = common.output_tensor(interpreter, 0)Intel Neural Compute Stick# from openvino.runtime import Core core = Core() model = core.read_model(\u0026#34;model.xml\u0026#34;) compiled = core.compile_model(model, \u0026#34;MYRIAD\u0026#34;) result = compiled([input_data])Raspberry Pi + Hailo# Raspberry Pi 5 + Hailo-8L M.2 → 13 TOPS AI 성능 성능 최적화# 배치 처리# # 단일 추론보다 배치 처리가 효율적 batch_size = 4 inputs = torch.stack([preprocess(img) for img in images]) outputs = model(inputs)파이프라인 병렬화# from concurrent.futures import ThreadPoolExecutor def pipeline(): with ThreadPoolExecutor(max_workers=3) as executor: # 병렬: 이미지 획득, 전처리, 추론 futures = { \u0026#39;capture\u0026#39;: executor.submit(capture_image), \u0026#39;preprocess\u0026#39;: executor.submit(preprocess, prev_image), \u0026#39;infer\u0026#39;: executor.submit(model, preprocessed), }메모리 관리# import gc import torch # 명시적 메모리 해제 del model gc.collect() torch.cuda.empty_cache() # 메모리 모니터링 print(torch.cuda.memory_allocated() / 1024**2, \u0026#34;MB\u0026#34;) 전력 관리# Jetson 전력 모드# # 사용 가능한 모드 확인 sudo nvpmodel -q --verbose # 예: Orin NX # Mode 0: MAXN (최대 성능) # Mode 1: 15W (저전력) # Mode 2: 10W (초저전력)동적 조절# import subprocess def set_power_mode(mode): subprocess.run([\u0026#39;sudo\u0026#39;, \u0026#39;nvpmodel\u0026#39;, \u0026#39;-m\u0026#39;, str(mode)]) # 작업량에 따라 전력 모드 조절 if workload_high: set_power_mode(0) # MAXN else: set_power_mode(1) # 저전력 실제 배포# Docker 사용# # Jetson용 Dockerfile FROM nvcr.io/nvidia/l4t-pytorch:r35.2.1-pth2.0-py3 COPY model.engine /app/ COPY main.py /app/ WORKDIR /app CMD [\u0026#34;python3\u0026#34;, \u0026#34;main.py\u0026#34;]docker run --runtime nvidia -it my-app시스템 서비스# # /etc/systemd/system/vision-app.service [Unit] Description=Vision Application After=network.target [Service] ExecStart=/usr/bin/python3 /app/main.py Restart=always User=root [Install] WantedBy=multi-user.targetsudo systemctl enable vision-app sudo systemctl start vision-app 벤치마크# Jetson Orin NX (FP16)# 모델 입력 크기 지연시간 FPS ResNet-50 224×224 2.1 ms 476 YOLOv8n 640×640 8.5 ms 118 YOLOv8s 640×640 15.2 ms 66 YOLOv8m 640×640 32.1 ms 31 디바이스 선택 가이드# 요구사항 추천 최고 성능 Jetson AGX Orin 비용 효율 Jetson Orin NX/Nano 초저전력 Coral, Hailo 개발/프로토타입 Jetson Orin Nano 대량 생산 가격/지원 협상 필요 관련 콘텐츠# TensorRT - GPU 최적화 최적화 - 모델 경량화 카메라 - 이미지 획득 "},{"id":35,"href":"/ko/docs/engineering/hardware/","title":"하드웨어","section":"실무 기술","content":"하드웨어# Vision 시스템 구축에 필요한 하드웨어 지식입니다.\n주요 내용# 주제 설명 카메라 센서, 렌즈, 인터페이스 조명 조명 설계 기초 엣지 디바이스 Jetson, 엣지 AI Vision 시스템 구성# ┌─────────────────────────────────────────────────────────┐ │ Vision System │ │ │ │ 조명 ──→ 피사체 ──→ 렌즈 ──→ 센서 ──→ 프로세서 │ │ │ │ Light Object Lens Camera Edge/Server │ └─────────────────────────────────────────────────────────┘ 하드웨어 선택 가이드# 요소 고려사항 해상도 검출할 결함/객체 크기 프레임레이트 이동 속도, 처리량 조명 반사, 그림자, 균일성 인터페이스 USB, GigE, Camera Link 비용 예산 대비 성능 "},{"id":36,"href":"/ko/docs/architecture/generative/controlnet/","title":"ControlNet","section":"Generative","content":"ControlNet# 개요# 논문: Adding Conditional Control to Text-to-Image Diffusion Models (2023) 저자: Lvmin Zhang, Maneesh Agrawala (Stanford) 핵심 기여: 사전학습된 Diffusion 모델에 공간적 조건 추가 핵심 아이디어# \u0026ldquo;원본 모델을 고정하고, 복사본에서 조건을 학습\u0026rdquo;\nStable Diffusion의 가중치를 건드리지 않고, 에지/포즈/깊이 등의 조건을 추가할 수 있습니다.\n왜 필요한가?# 텍스트만으로는 부족한 제어# Prompt: \u0026#34;a person raising their hand\u0026#34; → 손 위치, 포즈를 정확히 지정 불가ControlNet의 해결# Prompt: \u0026#34;a person\u0026#34; + Pose Image (조건) → 원하는 포즈 그대로 생성 구조# 전체 아키텍처# Input Image ↓ ┌──────────────────┐ │ Condition Extractor │ (Canny, Pose, Depth 등) └──────────────────┘ ↓ Condition (edge/pose/depth map) ↓ ┌───────────────────────────────────────────────┐ │ ControlNet │ │ │ │ ┌─────────────────┐ ┌─────────────────┐ │ │ │ Trainable Copy │───→│ Zero Conv │ │ │ │ of SD Encoder │ │ (initialized 0)│ │ │ └─────────────────┘ └────────┬────────┘ │ │ ↓ │ │ ┌─────────────────────────────────────────┐ │ │ │ Locked Stable Diffusion │ │ │ │ (original weights frozen) │ │ │ └─────────────────────────────────────────┘ │ └───────────────────────────────────────────────┘ ↓ Generated Image (following the condition)핵심 컴포넌트# 컴포넌트 설명 Locked Copy 원본 SD 가중치 고정 Trainable Copy SD encoder의 학습 가능한 복사본 Zero Convolution 0으로 초기화된 1×1 Conv Zero Convolution# 학습 시작 시 ControlNet의 영향이 0:\n$$y = x + \\text{ZeroConv}(\\text{ControlNet}(x, c))$$\n초기 상태:\nZeroConv 가중치 = 0 ControlNet 출력이 0 원본 SD와 동일하게 동작 학습 후:\n조건에 맞게 출력 조정 이렇게 하면 학습 초기에 원본 SD의 품질을 해치지 않습니다.\n지원하는 조건들# 조건 설명 용도 Canny Edge 윤곽선 구조 유지 OpenPose 인체 키포인트 포즈 제어 Depth 깊이 맵 3D 구조 Normal 노멀 맵 표면 방향 Scribble 낙서/스케치 대략적 형태 Segmentation 세그멘테이션 영역별 제어 M-LSD 직선 검출 건축물 등 SoftEdge 부드러운 에지 HED, PiDi 학습# 데이터셋 구성# { \u0026#34;source\u0026#34;: \u0026#34;condition_image.png\u0026#34;, # 조건 이미지 \u0026#34;target\u0026#34;: \u0026#34;ground_truth.png\u0026#34;, # 원본 이미지 \u0026#34;prompt\u0026#34;: \u0026#34;text description\u0026#34; }손실 함수# 표준 Diffusion loss와 동일:\n$$L = \\mathbb{E}{z, \\epsilon, t, c, c{control}} \\left[ | \\epsilon - \\epsilon_\\theta(z_t, t, c, c_{control}) |^2 \\right]$$\n학습 전략# 원본 SD 가중치 고정 ControlNet 부분만 학습 GPU 1개로도 학습 가능 (효율적) 구현 예시# 기본 사용# from diffusers import StableDiffusionControlNetPipeline, ControlNetModel import torch from PIL import Image import cv2 import numpy as np # ControlNet 로드 (Canny edge) controlnet = ControlNetModel.from_pretrained( \u0026#34;lllyasviel/sd-controlnet-canny\u0026#34;, torch_dtype=torch.float16 ) # 파이프라인 생성 pipe = StableDiffusionControlNetPipeline.from_pretrained( \u0026#34;runwayml/stable-diffusion-v1-5\u0026#34;, controlnet=controlnet, torch_dtype=torch.float16 ).to(\u0026#34;cuda\u0026#34;) # 조건 이미지 생성 (Canny edge) image = Image.open(\u0026#34;input.png\u0026#34;) image_np = np.array(image) canny = cv2.Canny(image_np, 100, 200) canny_image = Image.fromarray(canny) # 생성 output = pipe( prompt=\u0026#34;a beautiful garden\u0026#34;, image=canny_image, num_inference_steps=30 ).images[0]OpenPose 사용# from controlnet_aux import OpenposeDetector # 포즈 추출 openpose = OpenposeDetector.from_pretrained(\u0026#34;lllyasviel/ControlNet\u0026#34;) pose = openpose(image) # ControlNet으로 생성 controlnet = ControlNetModel.from_pretrained(\u0026#34;lllyasviel/sd-controlnet-openpose\u0026#34;) # ... 이후 동일다중 ControlNet# from diffusers import StableDiffusionControlNetPipeline, ControlNetModel # 여러 ControlNet 로드 controlnets = [ ControlNetModel.from_pretrained(\u0026#34;lllyasviel/sd-controlnet-canny\u0026#34;), ControlNetModel.from_pretrained(\u0026#34;lllyasviel/sd-controlnet-depth\u0026#34;), ] pipe = StableDiffusionControlNetPipeline.from_pretrained( \u0026#34;runwayml/stable-diffusion-v1-5\u0026#34;, controlnet=controlnets, # 리스트로 전달 torch_dtype=torch.float16 ) # 다중 조건으로 생성 output = pipe( prompt=\u0026#34;a house\u0026#34;, image=[canny_image, depth_image], # 각 조건 이미지 controlnet_conditioning_scale=[1.0, 0.5] # 각 조건의 강도 ).images[0] 조건 강도 조절# output = pipe( prompt=\u0026#34;a landscape\u0026#34;, image=canny_image, controlnet_conditioning_scale=0.5 # 0: 무시, 1: 완전 준수 ).images[0]낮은 값: 더 자유로운 생성 높은 값: 조건에 더 충실\nIP-Adapter와 결합# 이미지 스타일 + 구조 제어:\n# IP-Adapter: 참조 이미지의 스타일 # ControlNet: 구조 조건 pipe.load_ip_adapter(\u0026#34;h94/IP-Adapter\u0026#34;, ...) output = pipe( prompt=\u0026#34;a portrait\u0026#34;, image=canny_image, # 구조 ip_adapter_image=style_image # 스타일 ).images[0] ControlNet 변형# 변형 특징 T2I-Adapter 더 가벼운 구조 ControlNet-XS 경량화 버전 ControlNet 1.1 개선된 학습 전략 ControlNet++ SDXL 지원 관련 콘텐츠# Stable Diffusion - 기반 모델 Diffusion Process - Diffusion 수학 U-Net - ControlNet 구조와 유사 Generation 태스크 - 이미지 생성 평가 "},{"id":37,"href":"/ko/docs/architecture/segmentation/","title":"Segmentation","section":"아키텍처","content":"Segmentation 모델# 이미지의 각 픽셀에 레이블을 할당하는 모델들입니다.\nSegmentation 유형# Semantic Segmentation# 각 픽셀을 클래스로 분류 (객체 구분 X)\nInput: 고양이 2마리 이미지 Output: 모든 고양이 픽셀 → \u0026#34;고양이\u0026#34; 클래스Instance Segmentation# 각 객체 인스턴스를 구분\nInput: 고양이 2마리 이미지 Output: 고양이1 픽셀, 고양이2 픽셀 (분리)Panoptic Segmentation# Semantic + Instance 통합\nOutput: 배경(semantic) + 각 객체(instance) 주요 모델# 모델 유형 연도 특징 U-Net Semantic 2015 Encoder-Decoder, 의료영상 Mask R-CNN Instance 2017 Faster R-CNN + Mask DeepLab Semantic 2017 Atrous Convolution SAM Promptable 2023 Foundation Model 관련 콘텐츠# Transposed Convolution - Upsampling 기법 Segmentation 태스크 - 평가 지표, 데이터셋 "},{"id":38,"href":"/ko/docs/engineering/data/labeling/","title":"레이블링","section":"데이터","content":"레이블링 (Annotation)# 개요# 데이터 레이블링은 모델 성능의 상한선을 결정하는 중요한 작업입니다.\n레이블링 도구# 오픈소스# 도구 태스크 특징 LabelImg Detection 간단, VOC/YOLO 포맷 CVAT Detection, Segmentation 웹 기반, 협업 Label Studio 멀티태스크 유연한 설정 Labelme Segmentation Polygon 지원 VoTT Detection MS 개발, 간편 상용 서비스# 서비스 특징 Roboflow 자동 증강, 포맷 변환 Scale AI 대규모 레이블링 Supervisely 협업, 자동화 V7 (Darwin) AI 지원 레이블링 CVAT 사용법# 설치# # Docker로 설치 git clone https://github.com/opencv/cvat cd cvat docker compose up -d프로젝트 설정# 프로젝트 생성 레이블 정의 (클래스명, 속성) Task 생성 및 이미지 업로드 레이블링 작업 데이터 내보내기 (COCO, YOLO 등) 품질 관리# 일관성 유지# 가이드라인 문서 필수: - 클래스 정의 (예시 이미지 포함) - 경계선 기준 (타이트하게? 여유있게?) - 모호한 케이스 처리 방법 - 품질 기준 (IoU \u0026gt; 0.8 등)교차 검증# # 동일 이미지를 여러 작업자가 레이블링 # IoU로 일관성 측정 def calculate_agreement(boxes1, boxes2, threshold=0.5): \u0026#34;\u0026#34;\u0026#34;두 작업자 간 일치도 계산\u0026#34;\u0026#34;\u0026#34; matched = 0 for b1 in boxes1: for b2 in boxes2: if iou(b1, b2) \u0026gt; threshold: matched += 1 break return matched / max(len(boxes1), len(boxes2))리뷰 프로세스# 1차 레이블링 → 리뷰 → 수정 → 최종 승인 ↓ ↓ 80% 20% 작업자 리뷰어 자동 레이블링# 사전학습 모델 활용# from ultralytics import YOLO # 사전학습된 모델로 초기 레이블 생성 model = YOLO(\u0026#39;yolov8x.pt\u0026#39;) results = model.predict(\u0026#39;unlabeled_images/\u0026#39;) # 결과를 COCO 포맷으로 저장 for result in results: boxes = result.boxes.xyxy.cpu().numpy() classes = result.boxes.cls.cpu().numpy() # ... annotation 파일 생성SAM으로 Segmentation# from segment_anything import sam_model_registry, SamPredictor sam = sam_model_registry[\u0026#34;vit_h\u0026#34;](checkpoint=\u0026#34;sam_vit_h.pth\u0026#34;) predictor = SamPredictor(sam) # Detection 결과 → SAM으로 마스크 생성 predictor.set_image(image) masks, _, _ = predictor.predict(box=detection_box)Human-in-the-loop# 자동 레이블링 → 사람 검토/수정 → 모델 재학습 → 반복 ↓ ↓ ↓ 빠름 정확도 보장 품질 향상 Active Learning# 효율적으로 레이블링할 샘플 선택:\nimport numpy as np def uncertainty_sampling(model, unlabeled_data, n_samples): \u0026#34;\u0026#34;\u0026#34;불확실성이 높은 샘플 선택\u0026#34;\u0026#34;\u0026#34; predictions = model.predict(unlabeled_data) # 엔트로피 계산 entropy = -np.sum(predictions * np.log(predictions + 1e-10), axis=1) # 불확실성 높은 순으로 선택 indices = np.argsort(entropy)[-n_samples:] return indices전략# 전략 설명 Uncertainty 모델이 불확실한 샘플 Diversity 다양한 샘플 Query-by-Committee 여러 모델이 다르게 예측하는 샘플 비용 효율화# 계층적 레이블링# 1단계: 쉬운 샘플 (비전문가, 저비용) ↓ 2단계: 어려운 샘플 (전문가, 고비용)약한 레이블링 (Weak Supervision)# # 이미지 레벨 레이블 → 객체 레벨 레이블 # CAM (Class Activation Map) 활용 from pytorch_grad_cam import GradCAM cam = GradCAM(model=model, target_layers=[model.layer4]) grayscale_cam = cam(input_tensor=image) # CAM에서 객체 위치 추정 threshold = 0.5 mask = grayscale_cam \u0026gt; threshold bbox = mask_to_bbox(mask) 레이블링 체크리스트# 시작 전# 명확한 레이블링 가이드라인 작성 예시 이미지 준비 도구 선택 및 설정 작업자 교육 진행 중# 정기적인 샘플 리뷰 작업자 간 일관성 체크 어려운 케이스 논의 및 가이드라인 업데이트 완료 후# 전체 데이터 품질 검증 포맷 변환 및 검증 백업 관련 콘텐츠# 데이터 포맷 - 어노테이션 포맷 데이터 증강 - 데이터 확장 SAM - 자동 세그멘테이션 "},{"id":39,"href":"/ko/docs/engineering/deployment/optimization/","title":"모델 최적화","section":"배포","content":"모델 최적화# 개요# 모델 크기와 추론 속도를 개선하는 기법들입니다.\n최적화 기법 분류# 기법 설명 정확도 영향 양자화 비트 수 줄임 (FP32→INT8) 약간 감소 프루닝 불필요한 가중치 제거 약간 감소 Knowledge Distillation 작은 모델에 지식 전달 가능한 한 유지 아키텍처 최적화 효율적 구조 설계 설계에 따라 다름 양자화 (Quantization)# 종류# 방식 설명 특징 PTQ Post-Training Quantization 빠름, 약간의 정확도 손실 QAT Quantization-Aware Training 느림, 정확도 유지 Dynamic 추론 시 동적 양자화 간단, 제한적 성능 향상 PyTorch 동적 양자화# import torch.quantization # 동적 양자화 (주로 Linear, LSTM에 효과적) model_quantized = torch.quantization.quantize_dynamic( model, {torch.nn.Linear}, dtype=torch.qint8 )PyTorch 정적 양자화 (PTQ)# import torch.quantization model.eval() # 양자화 설정 model.qconfig = torch.quantization.get_default_qconfig(\u0026#39;fbgemm\u0026#39;) # 퓨전 (Conv + BN + ReLU) model_fused = torch.quantization.fuse_modules(model, [[\u0026#39;conv\u0026#39;, \u0026#39;bn\u0026#39;, \u0026#39;relu\u0026#39;]]) # 준비 model_prepared = torch.quantization.prepare(model_fused) # 캘리브레이션 (대표 데이터로) with torch.no_grad(): for data in calibration_loader: model_prepared(data) # 변환 model_quantized = torch.quantization.convert(model_prepared)QAT (Quantization-Aware Training)# # 준비 model.train() model.qconfig = torch.quantization.get_default_qat_qconfig(\u0026#39;fbgemm\u0026#39;) model_prepared = torch.quantization.prepare_qat(model) # 학습 (fake quantization 포함) for epoch in range(epochs): for data, target in train_loader: output = model_prepared(data) loss = criterion(output, target) loss.backward() optimizer.step() # 변환 model_prepared.eval() model_quantized = torch.quantization.convert(model_prepared) 프루닝 (Pruning)# 비구조적 프루닝# 개별 가중치 제거:\nimport torch.nn.utils.prune as prune # L1 프루닝 (30% 제거) prune.l1_unstructured(model.conv1, name=\u0026#39;weight\u0026#39;, amount=0.3) # 영구 적용 prune.remove(model.conv1, \u0026#39;weight\u0026#39;)구조적 프루닝# 전체 채널/필터 제거:\n# 채널 프루닝 prune.ln_structured(model.conv1, name=\u0026#39;weight\u0026#39;, amount=0.3, n=2, dim=0)글로벌 프루닝# parameters_to_prune = [ (model.conv1, \u0026#39;weight\u0026#39;), (model.conv2, \u0026#39;weight\u0026#39;), (model.fc, \u0026#39;weight\u0026#39;), ] prune.global_unstructured( parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.3, ) Knowledge Distillation# 기본 구조# Teacher (큰 모델) ──→ Soft Labels ↓ Student (작은 모델) ↑ Ground Truth ────────→ Hard Labels구현# import torch.nn.functional as F def distillation_loss(student_logits, teacher_logits, labels, T=4.0, alpha=0.7): \u0026#34;\u0026#34;\u0026#34; T: temperature (높을수록 soft한 분포) alpha: soft loss 비중 \u0026#34;\u0026#34;\u0026#34; # Soft loss (teacher의 지식) soft_loss = F.kl_div( F.log_softmax(student_logits / T, dim=1), F.softmax(teacher_logits / T, dim=1), reduction=\u0026#39;batchmean\u0026#39; ) * (T * T) # Hard loss (실제 레이블) hard_loss = F.cross_entropy(student_logits, labels) return alpha * soft_loss + (1 - alpha) * hard_loss # 학습 teacher.eval() student.train() for data, labels in train_loader: with torch.no_grad(): teacher_logits = teacher(data) student_logits = student(data) loss = distillation_loss(student_logits, teacher_logits, labels) optimizer.zero_grad() loss.backward() optimizer.step() 효율적인 아키텍처# MobileNet# Depthwise Separable Convolution:\nclass DepthwiseSeparable(nn.Module): def __init__(self, in_ch, out_ch, stride=1): super().__init__() # Depthwise: 채널별 독립 연산 self.depthwise = nn.Conv2d(in_ch, in_ch, 3, stride, 1, groups=in_ch) # Pointwise: 1x1 conv self.pointwise = nn.Conv2d(in_ch, out_ch, 1) def forward(self, x): x = self.depthwise(x) x = self.pointwise(x) return xEfficientNet# Compound Scaling:\ndepth = α^φ width = β^φ resolution = γ^φ 여기서 α · β² · γ² ≈ 2 혼합 정밀도 학습# 학습 시 FP16 사용으로 메모리와 속도 개선:\nfrom torch.cuda.amp import autocast, GradScaler scaler = GradScaler() for data, target in train_loader: optimizer.zero_grad() with autocast(): # FP16 연산 output = model(data) loss = criterion(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() 최적화 파이프라인# 1. 베이스라인 측정 ↓ 2. 프루닝 (30-50%) ↓ 3. 재학습 (정확도 복구) ↓ 4. 양자화 (INT8) ↓ 5. ONNX 변환 ↓ 6. TensorRT 최적화 ↓ 7. 최종 벤치마크 성능 측정# import torch import time def benchmark(model, input_shape, n_runs=100, warmup=10): device = next(model.parameters()).device x = torch.randn(input_shape).to(device) # 워밍업 for _ in range(warmup): with torch.no_grad(): model(x) # 동기화 (GPU) if device.type == \u0026#39;cuda\u0026#39;: torch.cuda.synchronize() # 측정 start = time.time() for _ in range(n_runs): with torch.no_grad(): model(x) if device.type == \u0026#39;cuda\u0026#39;: torch.cuda.synchronize() elapsed = time.time() - start latency = elapsed / n_runs * 1000 throughput = n_runs / elapsed print(f\u0026#34;Latency: {latency:.2f} ms\u0026#34;) print(f\u0026#34;Throughput: {throughput:.2f} FPS\u0026#34;) return latency, throughput모델 크기 측정# def get_model_size(model): param_size = 0 for param in model.parameters(): param_size += param.nelement() * param.element_size() buffer_size = 0 for buffer in model.buffers(): buffer_size += buffer.nelement() * buffer.element_size() size_mb = (param_size + buffer_size) / 1024 / 1024 return size_mb 관련 콘텐츠# TensorRT - GPU 최적화 ONNX - 모델 변환 모델 서빙 - 배포 "},{"id":40,"href":"/ko/docs/architecture/transformer/","title":"Transformer","section":"아키텍처","content":"Transformer# 개요# 논문: Attention Is All You Need (2017) 저자: Vaswani et al. (Google) 핵심 기여: Self-attention으로 RNN 없이 시퀀스 처리 원래 NLP를 위해 설계되었으나, Vision 분야에서도 혁신을 가져왔습니다.\n핵심 아이디어: Self-Attention# 모든 위치 간의 관계를 직접 모델링:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nQ (Query): 현재 위치에서 찾고 싶은 것 K (Key): 다른 위치들의 특성 V (Value): 실제 정보 CNN과 비교# 측면 CNN Transformer 수용 영역 지역적 (kernel size) 전역적 (모든 위치) 관계 모델링 암시적 명시적 (attention) 위치 정보 구조에 내재 Positional encoding 필요 계산량 O(n) O(n²) 구조# Encoder-Decoder 구조# ┌─────────────────────┐ ┌─────────────────────┐ │ Encoder │ │ Decoder │ ├─────────────────────┤ ├─────────────────────┤ │ Multi-Head Attn │ │ Masked Multi-Head │ │ ↓ │ │ ↓ │ │ Add \u0026amp; Norm │────→│ Cross Attention │ │ ↓ │ │ ↓ │ │ Feed Forward │ │ Add \u0026amp; Norm │ │ ↓ │ │ ↓ │ │ Add \u0026amp; Norm │ │ Feed Forward │ └─────────────────────┘ │ ↓ │ × N │ Add \u0026amp; Norm │ └─────────────────────┘ × NMulti-Head Attention# 여러 관점에서 attention 수행:\n# 8개 head, 각 head는 64차원 MultiHead(Q, K, V) = Concat(head_1, ..., head_8) @ W_O where head_i = Attention(Q @ W_Q_i, K @ W_K_i, V @ W_V_i)Positional Encoding# 위치 정보 주입:\n$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$ $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\nVision에서의 Transformer# 핵심 모델들# 모델 연도 입력 처리 특징 ViT 2020 Image → Patches 순수 Transformer DeiT 2021 Distillation 효율적 학습 Swin 2021 Shifted Windows 계층적 구조 DiT 2023 Diffusion + Transformer 이미지 생성 구현 예시# import torch import torch.nn as nn import math class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super().__init__() self.num_heads = num_heads self.d_k = d_model // num_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, Q, K, V, mask=None): batch_size = Q.size(0) # Linear projections Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) attn = torch.softmax(scores, dim=-1) context = torch.matmul(attn, V) # Concat and project context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) return self.W_o(context) class TransformerBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout=0.1): super().__init__() self.attention = MultiHeadAttention(d_model, num_heads) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.ffn = nn.Sequential( nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model), ) self.dropout = nn.Dropout(dropout) def forward(self, x): # Self-attention with residual attn_out = self.attention(x, x, x) x = self.norm1(x + self.dropout(attn_out)) # FFN with residual ffn_out = self.ffn(x) x = self.norm2(x + self.dropout(ffn_out)) return x Transformer의 영향# Vision 분야# Image Classification: ViT, DeiT, Swin Object Detection: DETR, Deformable DETR Segmentation: SAM, SegFormer Generation: DiT 장점# 전역적 context 이해 확장성 (scaling law) 사전학습 효과적 단점# O(n²) 계산량 많은 데이터 필요 위치 정보 취약 관련 콘텐츠# Attention - Attention 수식 상세 ViT - Vision Transformer DiT - Diffusion Transformer CNN 기초 - 대조되는 접근 방식 "},{"id":41,"href":"/ko/docs/architecture/generative/","title":"Generative","section":"아키텍처","content":"생성 모델 (Generative Models)# 데이터의 분포를 학습하여 새로운 샘플을 생성하는 모델들입니다.\n발전 과정# 2013: VAE - 확률적 잠재 공간 ↓ 2014: GAN - 적대적 학습 ↓ 2020: DDPM - Diffusion 시작 ↓ 2022: Stable Diffusion - Latent Diffusion ↓ 2023: ControlNet - 제어 가능한 생성 모델 비교# 모델 방식 장점 단점 VAE 변분 추론 안정적 학습 흐릿한 결과 GAN 적대적 학습 선명한 결과 불안정, mode collapse Stable Diffusion Diffusion 고품질, 다양성 느린 샘플링 ControlNet 조건부 Diffusion 정밀 제어 추가 학습 필요 관련 콘텐츠# Diffusion Process - Diffusion 수학 확률분포 - 생성 모델의 기초 Generation 태스크 - 평가 지표 "},{"id":42,"href":"/ko/docs/architecture/multimodal/","title":"Multimodal","section":"아키텍처","content":"Multimodal 모델# 이미지와 텍스트 등 여러 모달리티를 함께 이해하는 모델들입니다.\n발전 과정# 2021: CLIP - 이미지-텍스트 contrastive learning ↓ 2022: Flamingo - Few-shot visual reasoning ↓ 2023: LLaVA, GPT-4V - Vision Language Model ↓ 2024: 더 강력한 VLM들 (Claude 3, Gemini) 주요 모델# 모델 유형 특징 CLIP Encoder 이미지-텍스트 임베딩 정렬 VLM LLM 기반 이미지 이해 + 텍스트 생성 두 가지 접근# Dual Encoder (CLIP 방식)# Image → Image Encoder → Image Embedding ↓ (cosine similarity) Text → Text Encoder → Text Embedding 빠른 검색/매칭 Zero-shot classification LLM 기반 (VLM 방식)# Image → Vision Encoder → Visual Tokens ↓ Text ───────────────────────→ LLM → Response 복잡한 추론 가능 자연어로 답변 관련 콘텐츠# Contrastive Learning - CLIP의 수학적 기초 ViT - 이미지 인코더 Vision-Language 태스크 - VQA, Captioning "},{"id":43,"href":"/ko/docs/architecture/3d/","title":"3D Vision","section":"아키텍처","content":"3D Vision# 3차원 공간을 이해하고 재구성하는 모델들입니다.\n모델 목록# 모델 연도 핵심 기여 NeRF 2020 Neural Radiance Fields 3D Gaussian Splatting 2023 실시간 렌더링 발전 과정# NeRF (2020) → Instant-NGP (2022) → 3D Gaussian Splatting (2023)공통 개념# Volume Rendering# 3D 공간을 2D 이미지로 렌더링:\n$$C(r) = \\int_{t_n}^{t_f} T(t) \\sigma(r(t)) c(r(t), d) dt$$\nNovel View Synthesis# 학습하지 않은 시점에서 이미지 생성\n"},{"id":44,"href":"/ko/docs/math/","title":"수학","section":"Docs","content":"수학 기초# Vision 엔지니어에게 필요한 수학 지식입니다.\n"},{"id":45,"href":"/ko/docs/engineering/","title":"실무 기술","section":"Docs","content":"실무 기술 (Engineering)# Vision 엔지니어가 프로덕션 환경에서 필요한 실무 기술들입니다.\n카테고리# 분류 내용 데이터 데이터 증강, 포맷, 파이프라인 배포 TensorRT, ONNX, 모델 서빙 하드웨어 카메라, 조명, 엣지 디바이스 왜 중요한가?# 연구 vs 실무# 연구 (Research) 실무 (Production) ───────────────── ───────────────── 정확도 최대화 정확도 + 속도 + 비용 밸런스 깨끗한 데이터 노이즈, 결측, 불균형 데이터 고성능 GPU 제한된 하드웨어 단일 모델 파이프라인 통합 논문 출판 제품 출시/운영실무 엔지니어의 역할# 데이터: 수집, 정제, 증강, 레이블링 파이프라인 학습: 효율적인 학습 환경 구축, 실험 관리 배포: 모델 최적화, 서빙 인프라 구축 운영: 모니터링, A/B 테스트, 모델 업데이트 "},{"id":46,"href":"/ko/docs/architecture/","title":"아키텍처","section":"Docs","content":"아키텍처# Computer Vision 모델 및 네트워크 구조입니다.\n카테고리# Classical CV - SIFT, HOG 등 전통적 특징 추출 CNN - Convolutional Neural Network 계열 Detection - 객체 탐지 모델 Segmentation - 세그멘테이션 모델 Transformer - Transformer 기반 모델 Generative - 생성 모델 Multimodal - 멀티모달 모델 3D - 3D 비전 모델 "},{"id":47,"href":"/ko/docs/task/","title":"태스크","section":"Docs","content":"태스크# 문제 정의, 평가지표, 데이터셋입니다.\n"},{"id":48,"href":"/ko/docs/etc/","title":"기타 자료","section":"Docs","content":"논문 \u0026amp; 자료# 필수 논문과 학습 자료 모음입니다.\n"}]